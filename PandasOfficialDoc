PandasOfficialDoc
-----------------
pandas: powerful Python data analysis toolkit Release 1.2.3
------------------------------------------------------------

In pandas a data table is called a DataFrame.

pandas supports the integration with many file formats or data sources out of the box (csv, excel, sql, json, parquet,. . . ). Importing data from each of these data sources is provided by function with the prefix read_*. Similarly, the to_* methods are used to store data.

There is no need to loop over all rows of your data table to do calculations. Data manipulations on a column work elementwise. Adding a column to a DataFrame based on existing data in other columns is straightforward.

Split-apply-combine approach:
Basic statistics (mean, median, min, max, counts...) are easily calculable. These or custom aggregations can be applied on the entire data set, a sliding window of the data or grouped by categories. The latter is also known as the split-apply-combine approach.

Change the structure of your data table in multiple ways. You can melt() your data table from wide to long/tidy form or pivot() from long to wide format. With aggregations built-in, a pivot table is created with a single command.

Multiple tables can be concatenated both column wise as row wise and database-like join/merge operations are provided to combine multiple tables of data.

pandas has great support for time series and has an extensive set of tools for working with dates, times, and time-indexed data.

Data sets do not only contain numerical data. pandas provides a wide range of functions to clean textual data and extract useful information from it.

Handling Import Errors:
If you encounter an ImportError, it usually means that Python couldn‚Äôt find pandas in the list of available libraries. Python internally has a list of directories it searches through, to find packages. You can obtain these directories with:
import sys 
sys.path

Dependencies 
Package             Minimum supported version
setuptools              24.2.0
NumPy                   1.16.5
python-dateutil         2.7.3
pytz                    2017.3

Package Overview:
-----------------
pandas is a Python package providing fast, flexible, and expressive data structures designed to make working with ‚Äúrelational‚Äù or ‚Äúlabeled‚Äù data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real-world data analysis in Python. Additionally, it has the broader goal of becoming the most powerful and flexible open source data analysis/manipulation tool available in any language. It is already well on its way towards this goal.
pandas is well suited for many different kinds of data:
‚Ä¢ Tabular data with heterogeneously-typed columns, as in a SQL table or Excel spreadsheet
‚Ä¢ Ordered and unordered (not necessarily fixed-frequency) time series data.
‚Ä¢ Arbitrary matrix data (homogeneously typed or heterogeneous) with row and column labels
‚Ä¢ Any other form of observational / statistical data sets. The data need not be labeled at all to be placed into a pandas data structure.

The two primary data structures of pandas, Series (1-dimensional) and DataFrame (2-dimensional), handle the vast majority of typical use cases in finance, statistics, social science, and many areas of engineering. pandas is built on top of NumPy and is intended to integrate well within a scientific computing environment with many other 3rd party libraries.

Here are just a few of the things that pandas does well:
Easy handling of missing data (represented as NaN) in floating point as well as non-floating point data 
Size mutability: columns can be inserted and deleted from DataFrame and higher dimensional objects
Automatic and explicit data alignment: objects can be explicitly aligned to a set of labels,or the user can simply ignore the labels and let Series, DataFrame, etc. automatically align the data for you in computations.
Powerful, flexible group by functionality to perform split-apply-combine operations on data sets, for both aggregating and transforming data
Make it easy to convert ragged, differently-indexed data in other Python and NumPy data structures into DataFrame objects
Intelligent label-based slicing, fancy indexing, and subsetting of large data sets Intuitive merging and joining data sets
Flexible reshaping and pivoting of data sets
Hierarchical labeling of axes (possible to have multiple labels per tick)
Robust IO tools for loading data from flat files (CSV and delimited), Excel files, databases, and saving / loading data from the ultrafast HDF5 format
Time series-specific functionality: date range generation and frequency conversion, moving window statistics, date shifting, and lagging.

Many of these principles are here to address the shortcomings frequently experienced using other languages / scientific research environments. For data scientists, working with data is typically divided into multiple stages: munging and cleaning data, analyzing / modeling it, then organizing the results of the analysis into a form suitable for plotting or tabular display. pandas is the ideal tool for all of these tasks.
Some other notes
‚Ä¢ pandas is fast. Many of the low-level algorithmic bits have been extensively tweaked in Cython code. However, as with anything else generalization usually sacrifices performance. So if you focus on one feature for your application you may be able to create a faster specialized tool.
‚Ä¢ pandas is a dependency of statsmodels, making it an important part of the statistical computing ecosystem in Python.
‚Ä¢ pandas has been used extensively in production in financial applications.

DataStructures
df = DataFrame({
    'Dimensions':[1,2],
    'Name':['Series', 'DataFrame'],
    'Description':['1D labeled homogeneously-typed array',
                  'General 2D labeled, size-mutable tabular structure with potentially heterogeneously-typed column']
})
df


Dimensions	Name	    Description
	1	    Series	    1D labeled homogeneously-typed array
	2	    DataFrame	General 2D labeled, size-mutable tabular structure  with
                         potentially heterogeneously-typed column

Why more than one data structure?
The best way to think about the pandas data structures is as flexible containers for lower dimensional data. For example, DataFrame is a container for Series, and Series is a container for scalars. We would like to be able to insert and remove objects from these containers in a dictionary-like fashion.
Also, we would like sensible default behaviors for the common API functions which take into account the typical orientation of time series and cross-sectional data sets. When using the N-dimensional array (ndarrays) to store 2- and 3-dimensional data, a burden is placed on the user to consider the orientation of the data set when writing functions; axes are considered more or less equivalent (except when C- or Fortran-contiguousness matters for performance). In pandas, the axes are intended to lend more semantic meaning to the data; i.e., for a particular data set, there is likely to be a ‚Äúright‚Äù way to orient the data. The goal, then, is to reduce the amount of mental effort required to code up data transformations in downstream functions.
For example, with tabular data (DataFrame) it is more semantically helpful to think of the index (the rows) and the columns rather than axis 0 and axis 1. Iterating through the columns of the DataFrame thus results in more readable code:

for col in df.columns:
    series = df[col]
    # do something with series

Mutability and copying of data
All pandas data structures are value-mutable (the values they contain can be altered) but not always size-mutable. The length of a Series cannot be changed, but, for example, columns can be inserted into a DataFrame. However, the vast majority of methods produce new objects and leave the input data untouched. In general we like to favor immutability where sensible.

1.4.3 Getting started tutorials
WHAT KIND OF DATA DOES PANDAS HANDLE?
---------------------------------------
I want to start using pandas
import pandas as pd

To load the pandas package and start working with it, import the package. The community agreed alias for pandas is pd, so loading pandas as pd is assumed standard practice for all of the pandas documentation.

pandas data table representation
I want to store passenger data of the Titanic. For a number of passengers, I know the name (characters), age (integers) and sex (male/female) data.

df = pd.DataFrame({
    'Names' : ['Ramesh Kumar', 'Suresh Kumar', 'Rupesh Kumar'],
    'Age' : [22, 35, 58],
    'Sex' : ['male', 'male', 'female']
})
df

To manually store data in a table, create a DataFrame. When using a Python dictionary of lists, the dictionary keys will be used as column headers and the values in each list as columns of the DataFrame.

A DataFrame is a 2-dimensional data structure that can store data of different types (including characters, integers, floating point values, categorical data and more) in columns. It is similar to a spreadsheet, a SQL table 

‚Ä¢ The table has 3 columns, each of them with a column label. The column labels are respectively Name, Age and Sex.
‚Ä¢ The columnName consists of textual data with each value a string,the column Age are numbers and the column Sex is textual data.

EACH COLUMN IN A DATAFRAME IS A SERIES
I‚Äôm just interested in working with the data in the column Age
df['Age]

When selecting a single column of a pandas DataFrame, the result is a pandas Series. To select the column, use the column label in between square brackets [].

Note: If you are familiar to Python dictionaries, the selection of a single column is very similar to selection of dictionary values based on the key.

You can create a Series from scratch as well:

ages = pd.Series([22, 35, 58])
ages

0    22
1    35
2    58
dtype: int64

A pandas Series has no column labels, as it is just a single column of a DataFrame. A Series does have row labels.

Do something with a DataFrame or Series
I want to know the maximum Age of the passengers
We can do this on the DataFrame by selecting the Age column and applying max():
df['Age'].max() # DataFrame
ages.max() # Series

As illustrated by the max() method, you can do things with a DataFrame or Series. pandas provides a lot of functionalities, each of them a method you can apply to a DataFrame or Series. As methods are functions, do not forget to use parentheses ().
I‚Äôm interested in some basic statistics of the numerical data of my data table
df.describe()

The describe() method provides a quick overview of the numerical data in a DataFrame. As the Name and Sex columns are textual data, these are by default not taken into account by the describe() method.
Many pandas operations return a DataFrame or a Series. The describe() method is an example of a pandas operation returning a pandas Series.

Bullet Points:
--------------
‚Ä¢ Import the package, aka import pandas as pd
‚Ä¢ A table of data is stored as a pandas DataFrame
‚Ä¢ Each column in a DataFrame is a Series
‚Ä¢ You can do things by applying a method to a DataFrame or Series

This tutorial uses the Titanic data set, stored as CSV. The data consists of the following data columns: 
‚Ä¢ PassengerId: Id of every passenger.
‚Ä¢ Survived: This feature have value 0 and 1. 0 for not survived and 1 for survived.
‚Ä¢ Pclass: There are 3 classes: Class 1, Class 2 and Class 3.
‚Ä¢ Name: Name of passenger.
‚Ä¢ Sex: Gender of passenger.
‚Ä¢ Age: Age of passenger.
‚Ä¢ SibSp: Indication that passenger have siblings and spouse. 
‚Ä¢ Parch: Whether a passenger is alone or have family.
‚Ä¢ Ticket: Ticket number of passenger. 
‚Ä¢ Fare: Indicating the fare.
‚Ä¢ Cabin: The cabin of passenger.
‚Ä¢ Embarked: The embarked category.

HOW DO I READ AND WRITE TABULAR DATA?
--------------------------------------
I want to analyze the Titanic passenger data, available as a CSV file.
titanic = pd.read_csv('data/titanic.csv')
titanic

pandas provides the read_csv() function to read data stored as a csv file into a pandas DataFrame. pandas supports many different file formats or data sources out of the box (csv, excel, sql, json, parquet, . . . ), each of them with the prefix read_*.
Make sure to always have a check on the data after reading in the data. When displaying a DataFrame, the first and last 5 rows will be shown by default:

I want to see the first 8 rows of a pandas DataFrame.
titanic.head(8)
To see the first N rows of a DataFrame, use the head() method with the required number of rows (in this case 8) as argument.

Interested in the last N rows instead? pandas also provides a tail() method. For example, titanic.tail(10) will return the last 10 rows of the DataFrame.

A check on how pandas interpreted each of the column data types can be done by requesting the pandas dtypes attribute:
titanic.dtypes
PassengerId      int64
Pclass           int64
Name            object
Sex             object
Age            float64

For each of the columns, the used data type is enlisted. The data types in this DataFrame are integers (int64), floats (float64) and strings (object).

NOTE: 
When asking for the dtypes, no brackets are used! dtypes is an attribute of a DataFrame and Series. Attributes of DataFrame or Series do not need brackets. Attributes represent a characteristic of a DataFrame/Series, whereas a method (which requires brackets) do something with the DataFrame/Series.

My colleague requested the Titanic data as a spreadsheet.
titanic.to_excel("titanic.xlsx", sheet_name="passengers", index=False)

Whereas read_* functions are used to read data to pandas, the to_* methods are used to store data. The to_excel() method stores the data as an excel file. In the example here, the sheet_name is named passengers instead of the default Sheet1. By setting index=False the row index labels are not saved in the spreadsheet.
The equivalent read function read_excel() will reload the data to a DataFrame:
titanic = pd.read_excel("titanic.xlsx", sheet_name="passengers")

I‚Äôm interested in a technical summary of a DataFrame

titanic.info()

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   PassengerId  891 non-null    int64  
 1   Survived     891 non-null    int64  
 2   Pclass       891 non-null    int64  
 3   Name         891 non-null    object

dtypes: float64(2), int64(4), object(5)
memory usage: 36.0+ KB

The method info() provides technical information about a DataFrame, so let‚Äôs explain the output in more detail:
In [1]: import pandas as pd

This tutorial uses the Titanic data set, stored as CSV. The data consists of the following data columns:
‚Ä¢ PassengerId: Id of every passenger.
It is indeed a DataFrame.
There are 891 entries, i.e. 891 rows.
Each row has a row label (aka the index) with values ranging from 0 to 890.
The table has 12 columns. Most columns have a value for each of the rows (all 891 values are non-null). Some columns do have missing values and less than 891 non-null values.
The columns Name, Sex, Cabin and Embarked consists of textual data (strings, aka object). The other columns are numerical data with some of them whole numbers (aka integer) and others are real numbers (aka float).
* The kind of data (characters, integers,. . . ) in the different columns are summarized by listing the dtypes. The approximate amount of RAM used to hold the DataFrame is provided as well.
* Getting data in to pandas from many different file formats or data sources is supported by read_* functions. Exporting data out of pandas is provided by different to_* methods.
* The head/tail/info methods and the dtypes attribute are convenient for a first check.

HOW DO I SELECT A SUBSET OF A TABLE?
HOW DO I SELECT A SUBSET OF A DATAFRAME?
HOW DO I SELECT SPECIFIC COLUMNS FROM A DATAFRAME?
I‚Äôm interested in the age of the Titanic passengers.
ages = titanic['Age']
ages.head()

To select a single column, use square brackets [] with the column name of the column of interest.
Each column in a DataFrame is a Series. As a single column is selected, the returned object is a pandas Series.
We can verify this by checking the type of the output:
type(titanic['Age']) # pandas.core.series.Series

And have a look at the shape of the output:
titanic['Age'].shape # (891,)
titanic.shape # (891, 12)

DataFrame.shape is an attribute of a pandas Series and DataFrame containing the number of rows and columns: (nrows, ncolumns). A pandas Series is 1-dimensional and only the number of rows is returned.

I‚Äôm interested in the age and sex of the Titanic passengers.
age_sex = titanic[['Age', 'Sex']]
age_sex.head()
To select multiple columns, use a list of column names within the selection brackets [].
Note: The inner square brackets define a Python list with columnnames,whereas the outer brackets are used to select the data from a pandas DataFrame as seen in the previous example.

The returned data type is a pandas DataFrame:
age_sex = titanic[['Age', 'Sex']]
age_sex.head()
type(age_sex) # pandas.core.frame.DataFrame
age_sex.shape # (891, 2)
The selection returned a DataFrame with 891 rows and 2 columns. Remember, a DataFrame is 2-dimensional with both a row and column dimension.

How do I filter specific rows from a DataFrame?
I‚Äôm interested in the passengers older than 35 years.
above_35 = titanic[titanic['Age'] > 35]
above_35.head()

To select rows based on a conditional expression, use a condition inside the selection brackets []. The condition inside the selection brackets 
titanic["Age"] > 35
checks for which rows the Age column has a value greater than 35
(titanic['Age']>35).head() 

0    False
1     True
2     True
3    False
4    False
Name: Age, dtype: bool

The output of the conditional expression (>, but also ==, !=, <, <=,. . . would work) is actually a pandas Series of boolean values (either True or False) with the same number of rows as the original DataFrame. Such a Series of boolean values can be used to filter the DataFrame by putting it in between the selection brackets []. Only rows for which the value is True will be selected.

We know from before that the original Titanic DataFrame consists of 418 rows. Let‚Äôs have a look at the number of rows which satisfy the condition by checking the shape attribute of the resulting DataFrame above_35:
above_35.shape # (105, 11)

I‚Äôm interested in the Titanic passengers from cabin class 2 and 3.
class_23 = titanic[titanic['Pclass'].isin([2,3])]
class_23.head()

Similar to the conditional expression, the isin() conditional function returns a True for each row the values are in the provided list. To filter the rows based on such a function, use the conditional function inside the selection brackets []. In this case, the condition inside the selection brackets titanic["Pclass"].isin([2, 3]) checks for which rows the Pclass column is either 2 or 3.
The above is equivalent to filtering by rows for which the class is either 2 or 3 and combining the two statements with an | (or) operator:

class_32 = titanic[(titanic['Pclass'] == 2 ) | (titanic['Pclass'] == 3 )]

Note: When combining multiple conditional statements, each condition must be surrounded by parentheses (). Moreover, you can not use or/and but need to use the or operator | and the and operator &.

I want to work with passenger data for which the age is known.
age_not_na = titanic[titanic['Age'].notna()]
age_not_na

The notna() conditional function returns a True for each row the values are not an Null value. As such, this can be combined with the selection brackets [] to filter the data table.
You might wonder what actually changed, as the first 5 lines are still the same values. One way to verify is to check if the shape has changed:
age_not_na = titanic[titanic['Age'].notna()]
age_not_na.shape # (714, 12)

How do I select specific rows and columns from a DataFrame?
I‚Äôm interested in the names of the passengers older than 35 years.
adult_names = titanic.loc[titanic['Age']>35, 'Name']
adult_names.head()

In this case, a subset of both rows and columns is made in one go and just using selection brackets [] is not sufficient anymore. The loc/iloc operators are required in front of the selection brackets []. WHEN USING LOC/ILOC, THE PART BEFORE THE COMMA IS THE ROWS YOU WANT, AND THE PART AFTER THE COMMA IS THE COLUMNS YOU WANT TO SELECT.
When using the column names, row labels or a condition expression, use the loc operator in front of the selection brackets []. For both the part before and after the comma, you can use a single label, a list of labels, a slice of labels, a conditional expression or a colon. USING A COLON SPECIFIES YOU WANT TO SELECT ALL ROWS OR COLUMNS.

I‚Äôm interested in rows 10 till 25 and columns 3 to 5.
titanic.iloc[9:25, 2:5]

Again, a subset of both rows and columns is made in one go and just using selection brackets [] is not sufficient anymore. When specifically interested in certain rows and/or columns based on their position in the table, use the iloc operator in front of the selection brackets [].
When selecting specific rows and/or columns with loc or iloc, new values can be assigned to the selected data. For example, to assign the name anonymous to the first 3 elements of the third column:
titanic.iloc[:3,2] = 'anonymous'
titanic

Bullet Points for Indexing:
‚Ä¢ When selecting subsets of data, square brackets [] are used.
‚Ä¢ Inside these brackets, you can use a single column/row label, a list of column/row labels, a slice of labels, a conditional expression or a colon.
‚Ä¢ Select specific rows and/or columns using loc when using the row and column names
‚Ä¢ Select specific rows and/or columns using iloc when using the positions in the table
‚Ä¢ You can assign new values to a selection based on loc/iloc.

For this tutorial, air quality data about ùëÅùëÇ2 is used, made available by openaq and using the py-openaq package. The air_quality_no2.csv data set provides ùëÅùëÇ2 values for the measurement stations FR04014, BETR801 and London Westminster in respectively Paris, Antwerp and London.
air_quality = pd.read_csv("data/air_quality_no2.csv", index_col=0, parse_dates=True)
air_quality.head()

	                station_antwerp	    station_paris	station_london
datetime			
2019-05-07 02:00:00	    NaN	                NaN	            23.0
2019-05-07 03:00:00	    50.5	            25.0	        19.0
2019-05-07 04:00:00	    45.0	            27.7	        19.0
2019-05-07 05:00:00	    NaN	                50.4	        16.0
2019-05-07 06:00:00	    NaN	                61.9        	NaN

air_quality.info()
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 1035 entries, 2019-05-07 02:00:00 to 2019-06-21 02:00:00
Data columns (total 3 columns):
 #   Column           Non-Null Count  Dtype  
---  ------           --------------  -----  
 0   station_antwerp  95 non-null     float64
 1   station_paris    1004 non-null   float64
 2   station_london   969 non-null    float64
dtypes: float64(3)
memory usage: 32.3 KB

Note: The usage of the index_col and parse_dates parameters of the read_csv function to define the first (0th) column as index of the resulting DataFrame and convert the dates in the column to Timestamp objects, respectively.

HOW TO CREATE PLOTS IN PANDAS?
------------------------------
I want a quick visual check of the data.
air_quality.plot()

With a DataFrame, pandas creates by default one 'line plot' for each of the columns with numeric data.

I want to plot only the columns of the data table with the data from Paris.
air_quality['station_paris'].plot()

To plot a specific column, use the selection method of the subset data in combination with the plot() method. Hence, the plot() method works on both Series and DataFrame.

air_quality.plot.scatter(x='station_london', y='station_paris', alpha=0.7)
# alpha make scatters solid as it reaches 1.

Apart from the default line plot when using the plot function, a number of alternatives are available to plot data. Let‚Äôs use some standard Python to get an overview of the available plot methods:
code:
[
    method_name
    for method_name in dir(air_quality.plot)
    if not method_name.startswith("_")
]
['area',
 'bar',
 'barh',
 'box',
 'density',
 'hexbin',
 'hist',
 'kde',
 'line',
 'pie',
 'scatter']

Note: In many development environments as well as IPython and Jupyter Notebook, use the TAB button to get an overview of the available methods, for example air_quality.plot. + TAB.

One of the options is DataFrame.plot.box(), which refers to a boxplot. The box method is applicable on the air quality example data:
air_quality.plot.box()
air_quality.describe() # to understand the boxplot.

I want each of the columns in a separate subplot.
axs = air_quality.plot.area(figsize=(12,6) , subplots=True)

Separate subplots for each of the data columns are supported by the subplots argument of the plot functions. The builtin options available in each of the pandas plot functions that are worthwhile to have a look.

I want to further customize, extend or save the resulting plot.
fig, axs = plt.subplots(figsize = (12, 4))
air_quality.plot.area(ax=axs)
axs.set_ylabel('NO$_2$ Concentration')
fig.savefig("no2_concentration.png")

Each of the plot objects created by pandas is a matplotlib object. As Matplotlib provides plenty of options to customize plots, making the link between pandas and Matplotlib explicit enables all the power of matplotlib to the plot. This strategy is applied in the previous example:

fig, axs = plt.subplots(figsize=(12, 4)) ## Create an empty matplotlib Figure and Axes
air_quality.plot.area(ax=axs ## Use pandas to put the area plot on the prepared Figure/Axes
axs.set_ylabel("NO$_2$ concentration") ## Do any matplotlib customization you like
fig.savefig("no2_concentrations.png") ## Save the Figure/Axes using the existing matplotlib method.

Bullet Points on plots:
‚Ä¢ The .plot.* methods are applicable on both Series and DataFrames
‚Ä¢ By default, each of the columns is plotted as a different element (line, boxplot,. .) ‚Ä¢ Any plot created by pandas is a Matplotlib object.

HOW TO CREATE NEW COLUMNS DERIVED FROM EXISTING COLUMNS?
--------------------------------------------------------
I WANT TO EXPRESS THE ùëÅùëÇ2 CONCENTRATION OF THE STATION IN LONDON IN mg/m3

(If we assume temperature of 25 degrees Celsius and pressure of 1013 hPa, the conversion factor is 1.882)

air_quality['london_mg_per_cubic'] = air_quality['station_london']*1.882
air_quality.head()

To create a new column, use the [] brackets with the new column name at the left side of the assignment.
Note: The calculation of the values is done element_wise.This means all values in the given column are multiplied by the value 1.882 at once. You do not need to use a loop to iterate each of the rows!

I WANT TO CHECK THE RATIO OF THE VALUES IN PARIS VERSUS ANTWERP AND SAVE THE RESULT IN A NEW COLUMN
air_quality['ratio_paris_antwerp'] = (
    air_quality['station_paris'] / air_quality['station_antwerp']
)
air_quality.head()

The calculation is again element-wise, so the / is applied for the values in each row.
Also other mathematical operators (+, -, *, /) or logical operators (<, >, =,. . . ) work element wise.

I WANT TO RENAME THE DATA COLUMNS TO THE CORRESPONDING STATION IDENTIFIERS USED BY openAQ
air_quality_renamed = air_quality.rename(
    columns={
        'station_antwerp': 'BETR801',
        "station_paris": "FR04014",
        "station_london": "London Westminster",
    }
)
air_quality_renamed.head()

The rename() function can be used for both row labels and column labels. Provide a dictionary with the keys as the current names and the values as the new names to update the corresponding names.

The mapping should not be restricted to fixed names only, but can be a mapping function as well. For example, converting the column names to lowercase letters can be done using a function as well:

air_quality_renamed = air_quality_renamed.rename(columns=str.lower)
air_quality_renamed.head()

REMEMBER
* Create a new column by assigning the output to the DataFrame with a new column name in between the [].
* Operations are element-wise, no need to loop over rows.
* Use rename with a dictionary or function to rename row labels or column names.

HOW TO CALCULATE SUMMARY STATISTICS?
This tutorial uses the Titanic data set, stored as CSV. The data consists of the following data columns: ‚Ä¢ PassengerId: Id of every passenger.
‚Ä¢ Survived: This feature have value 0 and 1. 0 for not survived and 1 for survived.
‚Ä¢ Pclass: There are 3 classes: Class 1, Class 2 and Class 3.
‚Ä¢ Name: Name of passenger.
‚Ä¢ Sex: Gender of passenger.
‚Ä¢ Age: Age of passenger.
‚Ä¢ SibSp: Indication that passenger have siblings and spouse. ‚Ä¢ Parch: Whether a passenger is alone or have family.
‚Ä¢ Ticket: Ticket number of passenger. ‚Ä¢ Fare: Indicating the fare.
‚Ä¢ Cabin: The cabin of passenger.
‚Ä¢ Embarked: The embarked category.

titanic = pd.read_csv('data/titanic.csv')
titanic.head()

AGGREGATING STATISTICS
-----------------------
What is the average age of the Titanic passengers?
titanic['Age'].mean() # 29.699

Different statistics are available and can be applied to columns with numerical data. Operations in general exclude missing data and operate across rows by default.

What is the median age and ticket fare price of the Titanic passengers?
titanic[['Age', 'Fare']].median()
Age     28.0000
Fare    14.4542
dtype: float64

The statistic applied to multiple columns of a DataFrame is calculated for each numeric column.
The aggregating statistic can be calculated for multiple columns at the same time. Remember the describe function.
titanic[['Age', 'Fare']].describe()

Instead of the predefined statistics, specific combinations of aggregating statistics for given columns can be defined using the DataFrame.agg() method:
titanic.agg({
    'Age':['min', 'max', 'median', 'skew', 'mean'],
    'Fare':['min', 'max', 'median', 'skew', 'mean', 'std'],
})

AGGREGATING STATISTICS GROUPED BY CATEGORY
-------------------------------------------
What is the average age for male versus female Titanic passengers?
titanic[['Sex','Age']].groupby('Sex').mean()
	        Age
Sex	
female	27.915709
male	30.726645

As our interest is the average age for each gender, a subselection on these two columns is made first: titanic[[ "Sex", "Age"]]. Next, the groupby() method is applied on the Sex column to make a group per category. The average age for each gender is calculated and returned.
Calculating a given statistic (e.g. mean age) for each category in a column (e.g. male/female in the Sex column) is a common pattern. The groupby method is used to support this type of operations. More general, this fits in the more general split-apply-combine pattern:
‚Ä¢ Split the data into groups
‚Ä¢ Apply a function to each group independently 
‚Ä¢ Combine the results into a data structure

Split - Sex in Male and Female
Apply - Mean function on each group splitted
Combine - Combbine the result and present

The apply and combine steps are typically done together in pandas.

In the previous example, we explicitly selected the 2 columns first. If not, the mean method is applied to each column containing numerical columns:
titanic.groupby('Sex').mean()


It does not make much sense to get the average value of the Pclass. if we are only interested in the average age for each gender, the selection of columns (rectangular brackets [] as usual) is supported on the grouped data as well:
titanic.groupby('Sex')['Age'].mean()

Why option is Faster?
%timeit titanic.groupby('Sex')['Age'].mean() # 801 ¬µs ¬± 45.4 ¬µs
%timeit titanic[['Sex','Age']].groupby('Sex').mean() # 24.1 ms ¬± 282 ¬µs

Note: ThePclass column contains numerical data butactually represents 3 categories(orfactors) respectively the labels ‚Äò1‚Äô, ‚Äò2‚Äô and ‚Äò3‚Äô. Calculating statistics on these does not make much sense. Therefore, pandas provides a Categorical data type to handle this type of data.

What is the mean ticket fare price for each of the sex and cabin class combinations?
titanic.groupby(['Sex', 'Pclass'])['Fare'].mean()
Grouping can be done by multiple columns at the same time. Provide the column names as a list to the groupby() method.

COUNT NUMBER OF RECORDS BY CATEGORY
What is the number of passengers in each of the cabin classes?
titanic.Pclass.value_counts()
The function is a shortcut, as it is actually a groupby operation in combination with counting of the number of records within each group:
titanic.groupby('Pclass')['Pclass'].count()

Why one is faster?
%timeit titanic.Pclass.value_counts() # 389 ¬µs ¬± 44.5 ¬µs per loop
%timeit titanic.groupby('Pclass')['Pclass'].count() # 530 ¬µs ¬± 38.6 ¬µs per loop

Note: Both size and count can be used in combination with groupby.Where as size includes NaN values and just provides the number of rows (size of the table), count excludes the missing values. In the value_counts method, use the dropna argument to include or exclude the NaN value

REMEMBER
* Aggregation statistics can be calculated on entire columns or rows
* groupby provides the power of the split-apply-combine pattern
* value_counts is a convenient shortcut to count the number of entries in each category of a variable.  

HOW TO RESHAPE THE LAYOUT OF TABLES?
------------------------------------
This tutorial uses air quality data about ùëÅùëÇ2 and Particulate matter less than 2.5 micrometers, made available by openaq and using the py-openaq package. The air_quality_long.csv data set provides ùëÅ ùëÇ2 and ùëÉ ùëÄ25 values for the measurement stations FR04014, BETR801 and London Westminster in respectively Paris, Antwerp and London.
The air-quality data set has the following columns:
‚Ä¢ city: city where the sensor is used, either Paris, Antwerp or London
‚Ä¢ country: country where the sensor is used, either FR, BE or GB
‚Ä¢ location: the id of the sensor, either FR04014, BETR801 or London Westminster
‚Ä¢ parameter: the parameter measured by the sensor, either ùëÅ ùëÇ2 or Particulate matter ‚Ä¢ value: the measured value
‚Ä¢ unit: the unit of the measured parameter, in this case ‚ÄòŒºg/m3‚Äô
and the index of the DataFrame is datetime, the datetime of the measurement.

Note: The air-quality data is provided in a so-called long format data representation with each observation on a separate row and each variable a separate column of the data table. The long/narrow format is also known as the tidy data format.

tidy data - https://www.jstatsoft.org/article/view/v059i10
(A huge amount of effort is spent cleaning data to get it ready for analysis, but there has been little research on how to make data cleaning as easy and effective as possible. This paper tackles a small, but important, component of data cleaning: data tidying. Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets. This structure also makes it easier to develop tidy tools for data analysis, tools that both input and output tidy datasets. The advantages of a consistent data structure and matching tools are demonstrated with a case study free from mundane data manipulation chores.)

air_quality = pd.read_csv('data/air_quality_long.csv', index_col='date.utc', parse_dates=True) # date.utc is a column in dataset
air_quality.head()

How to reshape the layout of tables? 

SORT TABLE ROWS

I want to sort the Titanic data according to the age of the passengers.
titanic.sort_values(by='Age').head() ## KDB equivalent - `age xasc select from ('',(,)",")0:`titanic.csv

I want to sort the Titanic data according to the cabin class and age in descending order.
titanic.sort_values(by=['Pclass','Age'], ascending=False)

With Series.sort_values(), the rows in the table are sorted according to the defined column(s). The index will follow the row order.

LONG TO WIDE TABLE FORMAT
-------------------------
Let‚Äôs use a small subset of the air quality data set. We focus on ùëÅ ùëÇ2 data and only use the first two measurements of each location (i.e. the head of each group). The subset of data will be called no2_subset

 # filter for no2 data only
no2 = air_quality[air_quality['parameter'] == 'no2']
no2.head()

# use 2 measurements (head) for each location (groupby)
no2_subset = no2.sort_index().groupby(['location']).head(2)
no2_subset

I want the values for the three stations(location) as separate columns next to each other
no2_subset.pivot(columns='location', values='value') ## OR
no2_subset.pivot(columns='location')['value']

Which one is faster?
%timeit no2_subset.pivot(columns='location', values='value') # 2.34 ms ¬± 306 ¬µs per loop
%timeit no2_subset.pivot(columns='location')['value'] # 5.6 ms ¬± 204 ¬µs per loop

The pivot() function is purely reshaping of the data: a single value for each index/column combination is required.

As pandas support plotting of multiple columns out of the box, the conversion from long to wide table format enables the plotting of the different time series at the same time.
no2 = air_quality[air_quality['parameter'] == 'no2']
no2.head()
no2.pivot(columns='location', values='value').plot(figsize = (12, 6))

Note: When the index parameter is not defined, the existing index (rowlabels) is used.

PIVOT TABLE
I want the mean concentrations for ùëÅùëÇ2 and ùëÉùëÄ2.5 in each of the stations(locations) in table form.
air_quality = pd.read_csv('data/air_quality_long.csv', index_col='date.utc', parse_dates=True) # date.utc is a column in dataset

air_quality.pivot_table(columns='parameter', values='value', index='location', aggfunc='mean')

Difference between pivot() and pivot_table()?
In the case of pivot(), the data is only rearranged. When multiple values need to be aggregated (in this specific case, the values on different time steps) pivot_table() can be used, providing an aggregation function (e.g. mean) on how to combine these values.

Pivot table is a well known concept in spreadsheet software. When interested in summary columns for each variable separately as well, put the margin parameter to True:
air_quality.pivot_table(columns='parameter', values='value', 
                        index='location', aggfunc='mean', margins=True,)

Note: In case you are wondering, pivot_table() is indeed directly linked to groupby().The same result can be derived by grouping on both parameter and location:
air_quality.groupby(['parameter', 'location']).mean()

WIDE TO LONG FORMAT
no2_pivoted = no2.pivot(columns="location", values='value').reset_index()
no2_pivoted.head()

I want to collect all air quality ùëÅùëÇ2 measurements in a single column (long format)
no2_pivoted.melt(id_vars='date.utc')

The pandas.melt() method on a DataFrame converts the data table from wide format to long format. The column headers become the variable names in a newly created column.

The solution is the short version on how to apply pandas.melt(). The method will melt all columns NOT mentioned in id_vars together into two columns: A column with the column header names and a column with the values itself. The latter column gets by default the name value.

The pandas.melt() method can be defined in more detail
no2 = no2_pivoted.melt( id_vars='date.utc', value_vars=['BETR801','FR04014','London Westminster',], value_name='NO2', var_name='id_location', )
no2.head()

The result in the same, but in more detail defined:
‚Ä¢ value_vars  - defines explicitly which columns to melt together
‚Ä¢ value_name  - provides a custom column name for the values column instead of the default column name value
‚Ä¢ var_name  - provides a custom column name for the column collecting the column header names. Otherwise it takes the index name or a default variable

Hence, the arguments value_name and var_name are just user-defined names for the two generated columns. The columns to melt are defined by id_vars and value_vars.

REMEMBER
* Sorting by one or more columns is supported by sort_values
* The pivot function is purely restructuring of the data, pivot_table supports aggregations
* The reverse of pivot (long to wide format) is melt (wide to long format)

HOW TO COMBINE DATA FROM MULTIPLE TABLES?
------------------------------------------
DataSets:
air_quality_no2 = pd.read_csv("data/air_quality_no2_long.csv", parse_dates=True)
air_quality_no2.head()
air_quality_no2 = air_quality_no2[[ 'date.utc', 'location',
                                  'parameter', 'value', ]]
air_quality_no2.head()

The air_quality_pm25_long.csv data set provides ùëÉùëÄ25 values for the measurement stations FR04014, BETR801 and London Westminster in respectively Paris, Antwerp and London.
air_quality_pm25 = pd.read_csv("data/air_quality_pm25_long.csv",parse_dates=True)
air_quality_pm25.head()
air_quality_pm25 = air_quality_pm25[[ 'date.utc', 'location',
                                  'parameter', 'value', ]]
air_quality_pm25.head()

How to combine data from multiple tables? 
CONCATENATING OBJECTS
I want to combine the measurements of ùëÅùëÇ2 and ùëÉùëÄ25, two tables with a similar structure, in a single table

air_quality = pd.concat([air_quality_pm25, air_quality_no2], axis=0)
air_quality

The concat() function performs concatenation operations of multiple tables along one of the axis (row-wise or column-wise).
By default concatenation is along axis 0, so the resulting table combines the rows of the input tables. Let‚Äôs check the shape of the original and the concatenated tables to verify the operation:

print('Shape of the ``air_quality_pm25`` table', air_quality_pm25.shape)
print('Shape of the ``air_quality_no2`` table', air_quality_no2.shape)
print('Shape of resulting ``air_quality`` table', air_quality.shape)

Shape of the ``air_quality_pm25`` table (1110, 4)
Shape of the ``air_quality_no2`` table (2068, 4)
Shape of resulting ``air_quality`` table (3178, 4)

Hence, the resulting table has 3178 = 1110 + 2068 rows

Note: The axis argument will return in a number of pandas methods that can be applied along an axis. A DataFrame has two corresponding axes: the first running vertically downwards across rows (axis 0), and the second running horizontally across columns (axis 1). Most operations like concatenation or summary statistics are by default across rows (axis 0), but can be applied across columns as well.

Sorting the table on the datetime information illustrates also the combination of both tables, with the parameter column defining the origin of the table (either no2 from table air_quality_no2 or pm25 from table air_quality_pm25):

air_quality.sort_values('date.utc')

In this specific example, the parameter column provided by the data ensures that each of the original tables can be identified. This is not always the case. the concat function provides a convenient solution with the keys argument, adding an additional (hierarchical) row index. For example:

air_quality_ = pd.concat([air_quality_pm25, air_quality_no2], keys=["PM25", 'NO2',])
air_quality_

Note: The existence of multiple row/column indices at the same time has not been mentioned within these tutorials. Hierarchical indexing or MultiIndex is an advanced and powerful pandas feature to analyze higher dimensional data.
Multi-indexing is out of scope for this pandas introduction. For the moment, remember that the func- tion reset_index can be used to convert any level of an index to a column, e.g. air_quality. reset_index(level=0)

air_quality_ = pd.concat([air_quality_pm25, air_quality_no2], keys=["PM25", 'NO2',])
air_quality_
air_quality_.reset_index(level=0)

JOIN TABLES USING A COMMON IDENTIFIER

We have to Add the station coordinates, provided by the stations metadata table, to the corresponding rows in the measurements table.

Note: The stations used in this example (FR04014,BETR801 and LondonWestminster)are just three entries enlisted in the metadata table. We only want to add the coordinates of these three to the measurements table, each on the corresponding rows of the air_quality table.
stations_coord = pd.read_csv("data/air_quality_stations.csv")
stations_coord.head()

air_quality.head()

air_quality = pd.merge(air_quality, stations_coord, how='left', on='location')
air_quality.head()

Using the merge() function, for each of the rows in the air_quality table, the corresponding coordinates are added from the air_quality_stations_coord table. Both tables have the column location in common which is used as a key to combine the information. By choosing the left join, only the locations available in the air_quality (left) table, i.e. FR04014, BETR801 and London Westminster, end up in the resulting table. The merge function supports multiple join options similar to database-style operations.

I want to Add the parameter full description and name, provided by the parameters metadata table, to the measurements table

air_quality_parameters = pd.read_csv("data/air_quality_parameters.csv")
air_quality_parameters.head()

air_quality.head()

air_quality = pd.merge(air_quality, air_quality_parameters, how='left',
                      left_on='parameter', right_on='id')
air_quality.head()

Compared to the previous example, there is no common column name. However, the parameter column in the air_quality table and the id column in the air_quality_parameters_name both provide the measured variable in a common format. The left_on and right_on arguments are used here (instead of just on) to make the link between the two tables.
pandas supports also inner, outer, and right joins.

REMEMBER
* Multiple tables can be concatenated both column-wise and row-wise using the concat function.
* For database-like merging/joining of tables, use the merge function.

HOW TO HANDLE TIME SERIES DATA WITH EASE?
=========================================
For this tutorial, air quality data about ùëÅùëÇ2 and Particulate matter less than 2.5 micrometers is used, made available by openaq and downloaded using the py-openaq package. The air_quality_no2_long.csv" data set provides ùëÅ ùëÇ2 values for the measurement stations FR04014, BETR801 and London Westminster in respectively Paris, Antwerp and London.


Attributes:
index
columns
shape
dtypes
loc
iloc

Methods:
merge(,,how, left_on, right_on)
concat(, axis=, keys=)
melt(id_var=, value_var=, value_name=, var_name=)
reset_index()
pivot_table(colums=, values=, index=, aggfunc=, margins=)
pivot(columns=, values=)
size()
count()
sort_index()
sort_values()
value_counts()
mean(), median(), mode(), skew(), min(), max()
groupby().aggregateFunction()
agg({col=[fn1, fn2]})
rename(columns={'currentCol':'newCol'})
fig.savefig()
plt.subplot()
plot()
notna()
isin()
max()
head()
tail()
describe()
info()


PandasOfficialDoc
-----------------
pandas: powerful Python data analysis toolkit Release 1.2.3
------------------------------------------------------------

In pandas a data table is called a DataFrame.

pandas supports the integration with many file formats or data sources out of the box (csv, excel, sql, json, parquet,. . . ). Importing data from each of these data sources is provided by function with the prefix read_*. Similarly, the to_* methods are used to store data.

There is no need to loop over all rows of your data table to do calculations. Data manipulations on a column work elementwise. Adding a column to a DataFrame based on existing data in other columns is straightforward.

Split-apply-combine approach:
Basic statistics (mean, median, min, max, counts...) are easily calculable. These or custom aggregations can be applied on the entire data set, a sliding window of the data or grouped by categories. The latter is also known as the split-apply-combine approach.

Change the structure of your data table in multiple ways. You can melt() your data table from wide to long/tidy form or pivot() from long to wide format. With aggregations built-in, a pivot table is created with a single command.

Multiple tables can be concatenated both column wise as row wise and database-like join/merge operations are provided to combine multiple tables of data.

pandas has great support for time series and has an extensive set of tools for working with dates, times, and time-indexed data.

Data sets do not only contain numerical data. pandas provides a wide range of functions to clean textual data and extract useful information from it.

Handling Import Errors:
If you encounter an ImportError, it usually means that Python couldn’t find pandas in the list of available libraries. Python internally has a list of directories it searches through, to find packages. You can obtain these directories with:
import sys 
sys.path

Dependencies 
Package             Minimum supported version
setuptools              24.2.0
NumPy                   1.16.5
python-dateutil         2.7.3
pytz                    2017.3

Package Overview:
-----------------
pandas is a Python package providing fast, flexible, and expressive data structures designed to make working with “relational” or “labeled” data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real-world data analysis in Python. Additionally, it has the broader goal of becoming the most powerful and flexible open source data analysis/manipulation tool available in any language. It is already well on its way towards this goal.
pandas is well suited for many different kinds of data:
• Tabular data with heterogeneously-typed columns, as in a SQL table or Excel spreadsheet
• Ordered and unordered (not necessarily fixed-frequency) time series data.
• Arbitrary matrix data (homogeneously typed or heterogeneous) with row and column labels
• Any other form of observational / statistical data sets. The data need not be labeled at all to be placed into a pandas data structure.

The two primary data structures of pandas, Series (1-dimensional) and DataFrame (2-dimensional), handle the vast majority of typical use cases in finance, statistics, social science, and many areas of engineering. pandas is built on top of NumPy and is intended to integrate well within a scientific computing environment with many other 3rd party libraries.

Here are just a few of the things that pandas does well:
Easy handling of missing data (represented as NaN) in floating point as well as non-floating point data 
Size mutability: columns can be inserted and deleted from DataFrame and higher dimensional objects
Automatic and explicit data alignment: objects can be explicitly aligned to a set of labels,or the user can simply ignore the labels and let Series, DataFrame, etc. automatically align the data for you in computations.
Powerful, flexible group by functionality to perform split-apply-combine operations on data sets, for both aggregating and transforming data
Make it easy to convert ragged, differently-indexed data in other Python and NumPy data structures into DataFrame objects
Intelligent label-based slicing, fancy indexing, and subsetting of large data sets Intuitive merging and joining data sets
Flexible reshaping and pivoting of data sets
Hierarchical labeling of axes (possible to have multiple labels per tick)
Robust IO tools for loading data from flat files (CSV and delimited), Excel files, databases, and saving / loading data from the ultrafast HDF5 format
Time series-specific functionality: date range generation and frequency conversion, moving window statistics, date shifting, and lagging.

Many of these principles are here to address the shortcomings frequently experienced using other languages / scientific research environments. For data scientists, working with data is typically divided into multiple stages: munging and cleaning data, analyzing / modeling it, then organizing the results of the analysis into a form suitable for plotting or tabular display. pandas is the ideal tool for all of these tasks.
Some other notes
• pandas is fast. Many of the low-level algorithmic bits have been extensively tweaked in Cython code. However, as with anything else generalization usually sacrifices performance. So if you focus on one feature for your application you may be able to create a faster specialized tool.
• pandas is a dependency of statsmodels, making it an important part of the statistical computing ecosystem in Python.
• pandas has been used extensively in production in financial applications.

DataStructures
df = DataFrame({
    'Dimensions':[1,2],
    'Name':['Series', 'DataFrame'],
    'Description':['1D labeled homogeneously-typed array',
                  'General 2D labeled, size-mutable tabular structure with potentially heterogeneously-typed column']
})
df


Dimensions	Name	    Description
	1	    Series	    1D labeled homogeneously-typed array
	2	    DataFrame	General 2D labeled, size-mutable tabular structure  with
                         potentially heterogeneously-typed column

Why more than one data structure?
The best way to think about the pandas data structures is as flexible containers for lower dimensional data. For example, DataFrame is a container for Series, and Series is a container for scalars. We would like to be able to insert and remove objects from these containers in a dictionary-like fashion.
Also, we would like sensible default behaviors for the common API functions which take into account the typical orientation of time series and cross-sectional data sets. When using the N-dimensional array (ndarrays) to store 2- and 3-dimensional data, a burden is placed on the user to consider the orientation of the data set when writing functions; axes are considered more or less equivalent (except when C- or Fortran-contiguousness matters for performance). In pandas, the axes are intended to lend more semantic meaning to the data; i.e., for a particular data set, there is likely to be a “right” way to orient the data. The goal, then, is to reduce the amount of mental effort required to code up data transformations in downstream functions.
For example, with tabular data (DataFrame) it is more semantically helpful to think of the index (the rows) and the columns rather than axis 0 and axis 1. Iterating through the columns of the DataFrame thus results in more readable code:

for col in df.columns:
    series = df[col]
    # do something with series

Mutability and copying of data
All pandas data structures are value-mutable (the values they contain can be altered) but not always size-mutable. The length of a Series cannot be changed, but, for example, columns can be inserted into a DataFrame. However, the vast majority of methods produce new objects and leave the input data untouched. In general we like to favor immutability where sensible.

1.4.3 Getting started tutorials
WHAT KIND OF DATA DOES PANDAS HANDLE?
---------------------------------------
I want to start using pandas
import pandas as pd

To load the pandas package and start working with it, import the package. The community agreed alias for pandas is pd, so loading pandas as pd is assumed standard practice for all of the pandas documentation.

pandas data table representation
I want to store passenger data of the Titanic. For a number of passengers, I know the name (characters), age (integers) and sex (male/female) data.

df = pd.DataFrame({
    'Names' : ['Ramesh Kumar', 'Suresh Kumar', 'Rupesh Kumar'],
    'Age' : [22, 35, 58],
    'Sex' : ['male', 'male', 'female']
})
df

To manually store data in a table, create a DataFrame. When using a Python dictionary of lists, the dictionary keys will be used as column headers and the values in each list as columns of the DataFrame.

A DataFrame is a 2-dimensional data structure that can store data of different types (including characters, integers, floating point values, categorical data and more) in columns. It is similar to a spreadsheet, a SQL table 

• The table has 3 columns, each of them with a column label. The column labels are respectively Name, Age and Sex.
• The columnName consists of textual data with each value a string,the column Age are numbers and the column Sex is textual data.

EACH COLUMN IN A DATAFRAME IS A SERIES
I’m just interested in working with the data in the column Age
df['Age]

When selecting a single column of a pandas DataFrame, the result is a pandas Series. To select the column, use the column label in between square brackets [].

Note: If you are familiar to Python dictionaries, the selection of a single column is very similar to selection of dictionary values based on the key.

You can create a Series from scratch as well:

ages = pd.Series([22, 35, 58])
ages

0    22
1    35
2    58
dtype: int64

A pandas Series has no column labels, as it is just a single column of a DataFrame. A Series does have row labels.

Do something with a DataFrame or Series
I want to know the maximum Age of the passengers
We can do this on the DataFrame by selecting the Age column and applying max():
df['Age'].max() # DataFrame
ages.max() # Series

As illustrated by the max() method, you can do things with a DataFrame or Series. pandas provides a lot of functionalities, each of them a method you can apply to a DataFrame or Series. As methods are functions, do not forget to use parentheses ().
I’m interested in some basic statistics of the numerical data of my data table
df.describe()

The describe() method provides a quick overview of the numerical data in a DataFrame. As the Name and Sex columns are textual data, these are by default not taken into account by the describe() method.
Many pandas operations return a DataFrame or a Series. The describe() method is an example of a pandas operation returning a pandas Series.

Bullet Points:
--------------
• Import the package, aka import pandas as pd
• A table of data is stored as a pandas DataFrame
• Each column in a DataFrame is a Series
• You can do things by applying a method to a DataFrame or Series

This tutorial uses the Titanic data set, stored as CSV. The data consists of the following data columns: 
• PassengerId: Id of every passenger.
• Survived: This feature have value 0 and 1. 0 for not survived and 1 for survived.
• Pclass: There are 3 classes: Class 1, Class 2 and Class 3.
• Name: Name of passenger.
• Sex: Gender of passenger.
• Age: Age of passenger.
• SibSp: Indication that passenger have siblings and spouse. 
• Parch: Whether a passenger is alone or have family.
• Ticket: Ticket number of passenger. 
• Fare: Indicating the fare.
• Cabin: The cabin of passenger.
• Embarked: The embarked category.

HOW DO I READ AND WRITE TABULAR DATA?
--------------------------------------
I want to analyze the Titanic passenger data, available as a CSV file.
titanic = pd.read_csv('data/titanic.csv')
titanic

pandas provides the read_csv() function to read data stored as a csv file into a pandas DataFrame. pandas supports many different file formats or data sources out of the box (csv, excel, sql, json, parquet, . . . ), each of them with the prefix read_*.
Make sure to always have a check on the data after reading in the data. When displaying a DataFrame, the first and last 5 rows will be shown by default:

I want to see the first 8 rows of a pandas DataFrame.
titanic.head(8)
To see the first N rows of a DataFrame, use the head() method with the required number of rows (in this case 8) as argument.

Interested in the last N rows instead? pandas also provides a tail() method. For example, titanic.tail(10) will return the last 10 rows of the DataFrame.

A check on how pandas interpreted each of the column data types can be done by requesting the pandas dtypes attribute:
titanic.dtypes
PassengerId      int64
Pclass           int64
Name            object
Sex             object
Age            float64

For each of the columns, the used data type is enlisted. The data types in this DataFrame are integers (int64), floats (float64) and strings (object).

NOTE: 
When asking for the dtypes, no brackets are used! dtypes is an attribute of a DataFrame and Series. Attributes of DataFrame or Series do not need brackets. Attributes represent a characteristic of a DataFrame/Series, whereas a method (which requires brackets) do something with the DataFrame/Series.

My colleague requested the Titanic data as a spreadsheet.
titanic.to_excel("titanic.xlsx", sheet_name="passengers", index=False)

Whereas read_* functions are used to read data to pandas, the to_* methods are used to store data. The to_excel() method stores the data as an excel file. In the example here, the sheet_name is named passengers instead of the default Sheet1. By setting index=False the row index labels are not saved in the spreadsheet.
The equivalent read function read_excel() will reload the data to a DataFrame:
titanic = pd.read_excel("titanic.xlsx", sheet_name="passengers")

I’m interested in a technical summary of a DataFrame

titanic.info()

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   PassengerId  891 non-null    int64  
 1   Survived     891 non-null    int64  
 2   Pclass       891 non-null    int64  
 3   Name         891 non-null    object

dtypes: float64(2), int64(4), object(5)
memory usage: 36.0+ KB

The method info() provides technical information about a DataFrame, so let’s explain the output in more detail:
In [1]: import pandas as pd

This tutorial uses the Titanic data set, stored as CSV. The data consists of the following data columns:
• PassengerId: Id of every passenger.
It is indeed a DataFrame.
There are 891 entries, i.e. 891 rows.
Each row has a row label (aka the index) with values ranging from 0 to 890.
The table has 12 columns. Most columns have a value for each of the rows (all 891 values are non-null). Some columns do have missing values and less than 891 non-null values.
The columns Name, Sex, Cabin and Embarked consists of textual data (strings, aka object). The other columns are numerical data with some of them whole numbers (aka integer) and others are real numbers (aka float).
* The kind of data (characters, integers,. . . ) in the different columns are summarized by listing the dtypes. The approximate amount of RAM used to hold the DataFrame is provided as well.
* Getting data in to pandas from many different file formats or data sources is supported by read_* functions. Exporting data out of pandas is provided by different to_* methods.
* The head/tail/info methods and the dtypes attribute are convenient for a first check.

HOW DO I SELECT A SUBSET OF A TABLE?
HOW DO I SELECT A SUBSET OF A DATAFRAME?
HOW DO I SELECT SPECIFIC COLUMNS FROM A DATAFRAME?
I’m interested in the age of the Titanic passengers.
ages = titanic['Age']
ages.head()

To select a single column, use square brackets [] with the column name of the column of interest.
Each column in a DataFrame is a Series. As a single column is selected, the returned object is a pandas Series.
We can verify this by checking the type of the output:
type(titanic['Age']) # pandas.core.series.Series

And have a look at the shape of the output:
titanic['Age'].shape # (891,)
titanic.shape # (891, 12)

DataFrame.shape is an attribute of a pandas Series and DataFrame containing the number of rows and columns: (nrows, ncolumns). A pandas Series is 1-dimensional and only the number of rows is returned.

I’m interested in the age and sex of the Titanic passengers.
age_sex = titanic[['Age', 'Sex']]
age_sex.head()
To select multiple columns, use a list of column names within the selection brackets [].
Note: The inner square brackets define a Python list with columnnames,whereas the outer brackets are used to select the data from a pandas DataFrame as seen in the previous example.

The returned data type is a pandas DataFrame:
age_sex = titanic[['Age', 'Sex']]
age_sex.head()
type(age_sex) # pandas.core.frame.DataFrame
age_sex.shape # (891, 2)
The selection returned a DataFrame with 891 rows and 2 columns. Remember, a DataFrame is 2-dimensional with both a row and column dimension.

How do I filter specific rows from a DataFrame?
I’m interested in the passengers older than 35 years.
above_35 = titanic[titanic['Age'] > 35]
above_35.head()

To select rows based on a conditional expression, use a condition inside the selection brackets []. The condition inside the selection brackets 
titanic["Age"] > 35
checks for which rows the Age column has a value greater than 35
(titanic['Age']>35).head() 

0    False
1     True
2     True
3    False
4    False
Name: Age, dtype: bool

The output of the conditional expression (>, but also ==, !=, <, <=,. . . would work) is actually a pandas Series of boolean values (either True or False) with the same number of rows as the original DataFrame. Such a Series of boolean values can be used to filter the DataFrame by putting it in between the selection brackets []. Only rows for which the value is True will be selected.

We know from before that the original Titanic DataFrame consists of 418 rows. Let’s have a look at the number of rows which satisfy the condition by checking the shape attribute of the resulting DataFrame above_35:
above_35.shape # (105, 11)

I’m interested in the Titanic passengers from cabin class 2 and 3.
class_23 = titanic[titanic['Pclass'].isin([2,3])]
class_23.head()

Similar to the conditional expression, the isin() conditional function returns a True for each row the values are in the provided list. To filter the rows based on such a function, use the conditional function inside the selection brackets []. In this case, the condition inside the selection brackets titanic["Pclass"].isin([2, 3]) checks for which rows the Pclass column is either 2 or 3.
The above is equivalent to filtering by rows for which the class is either 2 or 3 and combining the two statements with an | (or) operator:

class_32 = titanic[(titanic['Pclass'] == 2 ) | (titanic['Pclass'] == 3 )]

Note: When combining multiple conditional statements, each condition must be surrounded by parentheses (). Moreover, you can not use or/and but need to use the or operator | and the and operator &.

I want to work with passenger data for which the age is known.
age_not_na = titanic[titanic['Age'].notna()]
age_not_na

The notna() conditional function returns a True for each row the values are not an Null value. As such, this can be combined with the selection brackets [] to filter the data table.
You might wonder what actually changed, as the first 5 lines are still the same values. One way to verify is to check if the shape has changed:
age_not_na = titanic[titanic['Age'].notna()]
age_not_na.shape # (714, 12)

How do I select specific rows and columns from a DataFrame?
I’m interested in the names of the passengers older than 35 years.
adult_names = titanic.loc[titanic['Age']>35, 'Name']
adult_names.head()

In this case, a subset of both rows and columns is made in one go and just using selection brackets [] is not sufficient anymore. The loc/iloc operators are required in front of the selection brackets []. WHEN USING LOC/ILOC, THE PART BEFORE THE COMMA IS THE ROWS YOU WANT, AND THE PART AFTER THE COMMA IS THE COLUMNS YOU WANT TO SELECT.
When using the column names, row labels or a condition expression, use the loc operator in front of the selection brackets []. For both the part before and after the comma, you can use a single label, a list of labels, a slice of labels, a conditional expression or a colon. USING A COLON SPECIFIES YOU WANT TO SELECT ALL ROWS OR COLUMNS.

I’m interested in rows 10 till 25 and columns 3 to 5.
titanic.iloc[9:25, 2:5]

Again, a subset of both rows and columns is made in one go and just using selection brackets [] is not sufficient anymore. When specifically interested in certain rows and/or columns based on their position in the table, use the iloc operator in front of the selection brackets [].
When selecting specific rows and/or columns with loc or iloc, new values can be assigned to the selected data. For example, to assign the name anonymous to the first 3 elements of the third column:
titanic.iloc[:3,2] = 'anonymous'
titanic

Bullet Points for Indexing:
• When selecting subsets of data, square brackets [] are used.
• Inside these brackets, you can use a single column/row label, a list of column/row labels, a slice of labels, a conditional expression or a colon.
• Select specific rows and/or columns using loc when using the row and column names
• Select specific rows and/or columns using iloc when using the positions in the table
• You can assign new values to a selection based on loc/iloc.

For this tutorial, air quality data about 𝑁𝑂2 is used, made available by openaq and using the py-openaq package. The air_quality_no2.csv data set provides 𝑁𝑂2 values for the measurement stations FR04014, BETR801 and London Westminster in respectively Paris, Antwerp and London.
air_quality = pd.read_csv("data/air_quality_no2.csv", index_col=0, parse_dates=True)
air_quality.head()

	                station_antwerp	    station_paris	station_london
datetime			
2019-05-07 02:00:00	    NaN	                NaN	            23.0
2019-05-07 03:00:00	    50.5	            25.0	        19.0
2019-05-07 04:00:00	    45.0	            27.7	        19.0
2019-05-07 05:00:00	    NaN	                50.4	        16.0
2019-05-07 06:00:00	    NaN	                61.9        	NaN

air_quality.info()
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 1035 entries, 2019-05-07 02:00:00 to 2019-06-21 02:00:00
Data columns (total 3 columns):
 #   Column           Non-Null Count  Dtype  
---  ------           --------------  -----  
 0   station_antwerp  95 non-null     float64
 1   station_paris    1004 non-null   float64
 2   station_london   969 non-null    float64
dtypes: float64(3)
memory usage: 32.3 KB

Note: The usage of the index_col and parse_dates parameters of the read_csv function to define the first (0th) column as index of the resulting DataFrame and convert the dates in the column to Timestamp objects, respectively.

HOW TO CREATE PLOTS IN PANDAS?
------------------------------
I want a quick visual check of the data.
air_quality.plot()

With a DataFrame, pandas creates by default one 'line plot' for each of the columns with numeric data.

I want to plot only the columns of the data table with the data from Paris.
air_quality['station_paris'].plot()

To plot a specific column, use the selection method of the subset data in combination with the plot() method. Hence, the plot() method works on both Series and DataFrame.

air_quality.plot.scatter(x='station_london', y='station_paris', alpha=0.7)
# alpha make scatters solid as it reaches 1.

Apart from the default line plot when using the plot function, a number of alternatives are available to plot data. Let’s use some standard Python to get an overview of the available plot methods:
code:
[
    method_name
    for method_name in dir(air_quality.plot)
    if not method_name.startswith("_")
]
['area',
 'bar',
 'barh',
 'box',
 'density',
 'hexbin',
 'hist',
 'kde',
 'line',
 'pie',
 'scatter']

Note: In many development environments as well as IPython and Jupyter Notebook, use the TAB button to get an overview of the available methods, for example air_quality.plot. + TAB.

One of the options is DataFrame.plot.box(), which refers to a boxplot. The box method is applicable on the air quality example data:
air_quality.plot.box()
air_quality.describe() # to understand the boxplot.

I want each of the columns in a separate subplot.
axs = air_quality.plot.area(figsize=(12,6) , subplots=True)

Separate subplots for each of the data columns are supported by the subplots argument of the plot functions. The builtin options available in each of the pandas plot functions that are worthwhile to have a look.

I want to further customize, extend or save the resulting plot.
fig, axs = plt.subplots(figsize = (12, 4))
air_quality.plot.area(ax=axs)
axs.set_ylabel('NO$_2$ Concentration')
fig.savefig("no2_concentration.png")

Each of the plot objects created by pandas is a matplotlib object. As Matplotlib provides plenty of options to customize plots, making the link between pandas and Matplotlib explicit enables all the power of matplotlib to the plot. This strategy is applied in the previous example:

fig, axs = plt.subplots(figsize=(12, 4)) ## Create an empty matplotlib Figure and Axes
air_quality.plot.area(ax=axs) ## Use pandas to put the area plot on the prepared Figure/Axes
axs.set_ylabel("NO$_2$ concentration") ## Do any matplotlib customization you like
fig.savefig("no2_concentrations.png") ## Save the Figure/Axes using the existing matplotlib method.

Bullet Points on plots:
• The .plot.* methods are applicable on both Series and DataFrames
• By default, each of the columns is plotted as a different element (line, boxplot,. .) • Any plot created by pandas is a Matplotlib object.

HOW TO CREATE NEW COLUMNS DERIVED FROM EXISTING COLUMNS?
--------------------------------------------------------
I WANT TO EXPRESS THE 𝑁𝑂2 CONCENTRATION OF THE STATION IN LONDON IN mg/m3

(If we assume temperature of 25 degrees Celsius and pressure of 1013 hPa, the conversion factor is 1.882)

air_quality['london_mg_per_cubic'] = air_quality['station_london']*1.882
air_quality.head()

To create a new column, use the [] brackets with the new column name at the left side of the assignment.
Note: The calculation of the values is done element_wise.This means all values in the given column are multiplied by the value 1.882 at once. You do not need to use a loop to iterate each of the rows!

I WANT TO CHECK THE RATIO OF THE VALUES IN PARIS VERSUS ANTWERP AND SAVE THE RESULT IN A NEW COLUMN
air_quality['ratio_paris_antwerp'] = (
    air_quality['station_paris'] / air_quality['station_antwerp']
)
air_quality.head()

The calculation is again element-wise, so the / is applied for the values in each row.
Also other mathematical operators (+, -, *, /) or logical operators (<, >, =,. . . ) work element wise.

I WANT TO RENAME THE DATA COLUMNS TO THE CORRESPONDING STATION IDENTIFIERS USED BY openAQ
air_quality_renamed = air_quality.rename(
    columns={
        'station_antwerp': 'BETR801',
        "station_paris": "FR04014",
        "station_london": "London Westminster",
    }
)
air_quality_renamed.head()

The rename() function can be used for both row labels and column labels. Provide a dictionary with the keys as the current names and the values as the new names to update the corresponding names.

The mapping should not be restricted to fixed names only, but can be a mapping function as well. For example, converting the column names to lowercase letters can be done using a function as well:

air_quality_renamed = air_quality_renamed.rename(columns=str.lower)
air_quality_renamed.head()

REMEMBER
* Create a new column by assigning the output to the DataFrame with a new column name in between the [].
* Operations are element-wise, no need to loop over rows.
* Use rename with a dictionary or function to rename row labels or column names.

HOW TO CALCULATE SUMMARY STATISTICS?
This tutorial uses the Titanic data set, stored as CSV. The data consists of the following data columns: • PassengerId: Id of every passenger.
• Survived: This feature have value 0 and 1. 0 for not survived and 1 for survived.
• Pclass: There are 3 classes: Class 1, Class 2 and Class 3.
• Name: Name of passenger.
• Sex: Gender of passenger.
• Age: Age of passenger.
• SibSp: Indication that passenger have siblings and spouse. • Parch: Whether a passenger is alone or have family.
• Ticket: Ticket number of passenger. • Fare: Indicating the fare.
• Cabin: The cabin of passenger.
• Embarked: The embarked category.

titanic = pd.read_csv('data/titanic.csv')
titanic.head()

AGGREGATING STATISTICS
-----------------------
What is the average age of the Titanic passengers?
titanic['Age'].mean() # 29.699

Different statistics are available and can be applied to columns with numerical data. Operations in general exclude missing data and operate across rows by default.

What is the median age and ticket fare price of the Titanic passengers?
titanic[['Age', 'Fare']].median()
Age     28.0000
Fare    14.4542
dtype: float64

The statistic applied to multiple columns of a DataFrame is calculated for each numeric column.
The aggregating statistic can be calculated for multiple columns at the same time. Remember the describe function.
titanic[['Age', 'Fare']].describe()

Instead of the predefined statistics, specific combinations of aggregating statistics for given columns can be defined using the DataFrame.agg() method:
titanic.agg({
    'Age':['min', 'max', 'median', 'skew', 'mean'],
    'Fare':['min', 'max', 'median', 'skew', 'mean', 'std'],
})

AGGREGATING STATISTICS GROUPED BY CATEGORY
-------------------------------------------
What is the average age for male versus female Titanic passengers?
titanic[['Sex','Age']].groupby('Sex').mean()
	        Age
Sex	
female	27.915709
male	30.726645

As our interest is the average age for each gender, a subselection on these two columns is made first: titanic[[ "Sex", "Age"]]. Next, the groupby() method is applied on the Sex column to make a group per category. The average age for each gender is calculated and returned.
Calculating a given statistic (e.g. mean age) for each category in a column (e.g. male/female in the Sex column) is a common pattern. The groupby method is used to support this type of operations. More general, this fits in the more general split-apply-combine pattern:
• Split the data into groups
• Apply a function to each group independently 
• Combine the results into a data structure

Split - Sex in Male and Female
Apply - Mean function on each group splitted
Combine - Combbine the result and present

The apply and combine steps are typically done together in pandas.

In the previous example, we explicitly selected the 2 columns first. If not, the mean method is applied to each column containing numerical columns:
titanic.groupby('Sex').mean()


It does not make much sense to get the average value of the Pclass. if we are only interested in the average age for each gender, the selection of columns (rectangular brackets [] as usual) is supported on the grouped data as well:
titanic.groupby('Sex')['Age'].mean()

Why option is Faster?
%timeit titanic.groupby('Sex')['Age'].mean() # 801 µs ± 45.4 µs
%timeit titanic[['Sex','Age']].groupby('Sex').mean() # 24.1 ms ± 282 µs

Note: ThePclass column contains numerical data butactually represents 3 categories(orfactors) respectively the labels ‘1’, ‘2’ and ‘3’. Calculating statistics on these does not make much sense. Therefore, pandas provides a Categorical data type to handle this type of data.

What is the mean ticket fare price for each of the sex and cabin class combinations?
titanic.groupby(['Sex', 'Pclass'])['Fare'].mean()
Grouping can be done by multiple columns at the same time. Provide the column names as a list to the groupby() method.

COUNT NUMBER OF RECORDS BY CATEGORY
What is the number of passengers in each of the cabin classes?
titanic.Pclass.value_counts()
The function is a shortcut, as it is actually a groupby operation in combination with counting of the number of records within each group:
titanic.groupby('Pclass')['Pclass'].count()

Why one is faster?
%timeit titanic.Pclass.value_counts() # 389 µs ± 44.5 µs per loop
%timeit titanic.groupby('Pclass')['Pclass'].count() # 530 µs ± 38.6 µs per loop

Note: Both size and count can be used in combination with groupby.Where as size includes NaN values and just provides the number of rows (size of the table), count excludes the missing values. In the value_counts method, use the dropna argument to include or exclude the NaN value

REMEMBER
* Aggregation statistics can be calculated on entire columns or rows
* groupby provides the power of the split-apply-combine pattern
* value_counts is a convenient shortcut to count the number of entries in each category of a variable.  

HOW TO RESHAPE THE LAYOUT OF TABLES?
------------------------------------
This tutorial uses air quality data about 𝑁𝑂2 and Particulate matter less than 2.5 micrometers, made available by openaq and using the py-openaq package. The air_quality_long.csv data set provides 𝑁 𝑂2 and 𝑃 𝑀25 values for the measurement stations FR04014, BETR801 and London Westminster in respectively Paris, Antwerp and London.
The air-quality data set has the following columns:
• city: city where the sensor is used, either Paris, Antwerp or London
• country: country where the sensor is used, either FR, BE or GB
• location: the id of the sensor, either FR04014, BETR801 or London Westminster
• parameter: the parameter measured by the sensor, either 𝑁 𝑂2 or Particulate matter • value: the measured value
• unit: the unit of the measured parameter, in this case ‘μg/m3’
and the index of the DataFrame is datetime, the datetime of the measurement.

Note: The air-quality data is provided in a so-called long format data representation with each observation on a separate row and each variable a separate column of the data table. The long/narrow format is also known as the tidy data format.

tidy data - https://www.jstatsoft.org/article/view/v059i10
(A huge amount of effort is spent cleaning data to get it ready for analysis, but there has been little research on how to make data cleaning as easy and effective as possible. This paper tackles a small, but important, component of data cleaning: data tidying. Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets. This structure also makes it easier to develop tidy tools for data analysis, tools that both input and output tidy datasets. The advantages of a consistent data structure and matching tools are demonstrated with a case study free from mundane data manipulation chores.)

air_quality = pd.read_csv('data/air_quality_long.csv', index_col='date.utc', parse_dates=True) # date.utc is a column in dataset
air_quality.head()

How to reshape the layout of tables? 

SORT TABLE ROWS

I want to sort the Titanic data according to the age of the passengers.
titanic.sort_values(by='Age').head() ## KDB equivalent - `age xasc select from ('',(,)",")0:`titanic.csv

I want to sort the Titanic data according to the cabin class and age in descending order.
titanic.sort_values(by=['Pclass','Age'], ascending=False)

With Series.sort_values(), the rows in the table are sorted according to the defined column(s). The index will follow the row order.

LONG TO WIDE TABLE FORMAT
-------------------------
Let’s use a small subset of the air quality data set. We focus on 𝑁 𝑂2 data and only use the first two measurements of each location (i.e. the head of each group). The subset of data will be called no2_subset

 # filter for no2 data only
no2 = air_quality[air_quality['parameter'] == 'no2']
no2.head()

# use 2 measurements (head) for each location (groupby)
no2_subset = no2.sort_index().groupby(['location']).head(2)
no2_subset

I want the values for the three stations(location) as separate columns next to each other
no2_subset.pivot(columns='location', values='value') ## OR
no2_subset.pivot(columns='location')['value']

Which one is faster?
%timeit no2_subset.pivot(columns='location', values='value') # 2.34 ms ± 306 µs per loop
%timeit no2_subset.pivot(columns='location')['value'] # 5.6 ms ± 204 µs per loop

The pivot() function is purely reshaping of the data: a single value for each index/column combination is required.

As pandas support plotting of multiple columns out of the box, the conversion from long to wide table format enables the plotting of the different time series at the same time.
no2 = air_quality[air_quality['parameter'] == 'no2']
no2.head()
no2.pivot(columns='location', values='value').plot(figsize = (12, 6))

Note: When the index parameter is not defined, the existing index (rowlabels) is used.

PIVOT TABLE
I want the mean concentrations for 𝑁𝑂2 and 𝑃𝑀2.5 in each of the stations(locations) in table form.
air_quality = pd.read_csv('data/air_quality_long.csv', index_col='date.utc', parse_dates=True) # date.utc is a column in dataset

air_quality.pivot_table(columns='parameter', values='value', index='location', aggfunc='mean')

Difference between pivot() and pivot_table()?
In the case of pivot(), the data is only rearranged. When multiple values need to be aggregated (in this specific case, the values on different time steps) pivot_table() can be used, providing an aggregation function (e.g. mean) on how to combine these values.

Pivot table is a well known concept in spreadsheet software. When interested in summary columns for each variable separately as well, put the margin parameter to True:
air_quality.pivot_table(columns='parameter', values='value', 
                        index='location', aggfunc='mean', margins=True,)

Note: In case you are wondering, pivot_table() is indeed directly linked to groupby().The same result can be derived by grouping on both parameter and location:
air_quality.groupby(['parameter', 'location']).mean()

WIDE TO LONG FORMAT
no2_pivoted = no2.pivot(columns="location", values='value').reset_index()
no2_pivoted.head()

I want to collect all air quality 𝑁𝑂2 measurements in a single column (long format)
no2_pivoted.melt(id_vars='date.utc')

The pandas.melt() method on a DataFrame converts the data table from wide format to long format. The column headers become the variable names in a newly created column.

The solution is the short version on how to apply pandas.melt(). The method will melt all columns NOT mentioned in id_vars together into two columns: A column with the column header names and a column with the values itself. The latter column gets by default the name value.

The pandas.melt() method can be defined in more detail
no2 = no2_pivoted.melt( id_vars='date.utc', value_vars=['BETR801','FR04014','London Westminster',], value_name='NO2', var_name='id_location', )
no2.head()

The result in the same, but in more detail defined:
• value_vars  - defines explicitly which columns to melt together
• value_name  - provides a custom column name for the values column instead of the default column name value
• var_name  - provides a custom column name for the column collecting the column header names. Otherwise it takes the index name or a default variable

Hence, the arguments value_name and var_name are just user-defined names for the two generated columns. The columns to melt are defined by id_vars and value_vars.

REMEMBER
* Sorting by one or more columns is supported by sort_values
* The pivot function is purely restructuring of the data, pivot_table supports aggregations
* The reverse of pivot (long to wide format) is melt (wide to long format)

HOW TO COMBINE DATA FROM MULTIPLE TABLES?
------------------------------------------
DataSets:
air_quality_no2 = pd.read_csv("data/air_quality_no2_long.csv", parse_dates=True)
air_quality_no2.head()
air_quality_no2 = air_quality_no2[[ 'date.utc', 'location',
                                  'parameter', 'value', ]]
air_quality_no2.head()

The air_quality_pm25_long.csv data set provides 𝑃𝑀25 values for the measurement stations FR04014, BETR801 and London Westminster in respectively Paris, Antwerp and London.
air_quality_pm25 = pd.read_csv("data/air_quality_pm25_long.csv",parse_dates=True)
air_quality_pm25.head()
air_quality_pm25 = air_quality_pm25[[ 'date.utc', 'location',
                                  'parameter', 'value', ]]
air_quality_pm25.head()

How to combine data from multiple tables? 
CONCATENATING OBJECTS
I want to combine the measurements of 𝑁𝑂2 and 𝑃𝑀25, two tables with a similar structure, in a single table

air_quality = pd.concat([air_quality_pm25, air_quality_no2], axis=0)
air_quality

The concat() function performs concatenation operations of multiple tables along one of the axis (row-wise or column-wise).
By default concatenation is along axis 0, so the resulting table combines the rows of the input tables. Let’s check the shape of the original and the concatenated tables to verify the operation:

print('Shape of the ``air_quality_pm25`` table', air_quality_pm25.shape)
print('Shape of the ``air_quality_no2`` table', air_quality_no2.shape)
print('Shape of resulting ``air_quality`` table', air_quality.shape)

Shape of the ``air_quality_pm25`` table (1110, 4)
Shape of the ``air_quality_no2`` table (2068, 4)
Shape of resulting ``air_quality`` table (3178, 4)

Hence, the resulting table has 3178 = 1110 + 2068 rows

Note: The axis argument will return in a number of pandas methods that can be applied along an axis. A DataFrame has two corresponding axes: the first running vertically downwards across rows (axis 0), and the second running horizontally across columns (axis 1). Most operations like concatenation or summary statistics are by default across rows (axis 0), but can be applied across columns as well.

Sorting the table on the datetime information illustrates also the combination of both tables, with the parameter column defining the origin of the table (either no2 from table air_quality_no2 or pm25 from table air_quality_pm25):

air_quality.sort_values('date.utc')

In this specific example, the parameter column provided by the data ensures that each of the original tables can be identified. This is not always the case. the concat function provides a convenient solution with the keys argument, adding an additional (hierarchical) row index. For example:

air_quality_ = pd.concat([air_quality_pm25, air_quality_no2], keys=["PM25", 'NO2',])
air_quality_

Note: The existence of multiple row/column indices at the same time has not been mentioned within these tutorials. Hierarchical indexing or MultiIndex is an advanced and powerful pandas feature to analyze higher dimensional data.
Multi-indexing is out of scope for this pandas introduction. For the moment, remember that the func- tion reset_index can be used to convert any level of an index to a column, e.g. air_quality. reset_index(level=0)

air_quality_ = pd.concat([air_quality_pm25, air_quality_no2], keys=["PM25", 'NO2',])
air_quality_
air_quality_.reset_index(level=0)

JOIN TABLES USING A COMMON IDENTIFIER

We have to Add the station coordinates, provided by the stations metadata table, to the corresponding rows in the measurements table.

Note: The stations used in this example (FR04014,BETR801 and LondonWestminster)are just three entries enlisted in the metadata table. We only want to add the coordinates of these three to the measurements table, each on the corresponding rows of the air_quality table.
stations_coord = pd.read_csv("data/air_quality_stations.csv")
stations_coord.head()

air_quality.head()

air_quality = pd.merge(air_quality, stations_coord, how='left', on='location')
air_quality.head()

Using the merge() function, for each of the rows in the air_quality table, the corresponding coordinates are added from the air_quality_stations_coord table. Both tables have the column location in common which is used as a key to combine the information. By choosing the left join, only the locations available in the air_quality (left) table, i.e. FR04014, BETR801 and London Westminster, end up in the resulting table. The merge function supports multiple join options similar to database-style operations.

I want to Add the parameter full description and name, provided by the parameters metadata table, to the measurements table

air_quality_parameters = pd.read_csv("data/air_quality_parameters.csv")
air_quality_parameters.head()

air_quality.head()

air_quality = pd.merge(air_quality, air_quality_parameters, how='left',
                      left_on='parameter', right_on='id')
air_quality.head()

Compared to the previous example, there is no common column name. However, the parameter column in the air_quality table and the id column in the air_quality_parameters_name both provide the measured variable in a common format. The left_on and right_on arguments are used here (instead of just on) to make the link between the two tables.
pandas supports also inner, outer, and right joins.

REMEMBER
* Multiple tables can be concatenated both column-wise and row-wise using the concat function.
* For database-like merging/joining of tables, use the merge function.

HOW TO HANDLE TIME SERIES DATA WITH EASE?
=========================================
For this tutorial, air quality data about 𝑁𝑂2 and Particulate matter less than 2.5 micrometers is used, made available by openaq and downloaded using the py-openaq package. The air_quality_no2_long.csv" data set provides 𝑁 𝑂2 values for the measurement stations FR04014, BETR801 and London Westminster in respectively Paris, Antwerp and London.

air_quality = pd.read_csv("data/air_quality_no2_long.csv")
air_quality.rename(columns = {'date.utc': 'datetime'})
air_quality.head()
air_quality.city.unique()

air_quality.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 2068 entries, 0 to 2067
Data columns (total 7 columns):
 #   Column     Non-Null Count  Dtype  
---  ------     --------------  -----  
 0   city       2068 non-null   object 
 1   country    2068 non-null   object 
 2   datetime   2068 non-null   object

Currently, datetime is object type.

Using pandas datetime properties
---------------------------------
I want to work with the dates in the column datetime as datetime objects instead of plain text.

air_quality['datetime'] = pd.to_datetime(air_quality.datetime)
air_quality.datetime.dtype # datetime64[ns, UTC]

Initially, the values in datetime are character strings and do not provide any datetime operations (e.g. extract the year, day of the week,. . . ). By applying the to_datetime function, pandas interprets the strings and convert these to datetime (i.e. datetime64[ns, UTC]) objects. In pandas we call these datetime objects similar to datetime. datetime from the standard library as pandas.Timestamp.

Note: Asmanydatasetsdocontaindatetimeinformationinoneofthecolumns,pandasinputfunctionlikepandas. read_csv() and pandas.read_json() can do the transformation to dates when reading the data using the parse_dates parameter with a list of the columns to read as Timestamp:
pd.read_csv("../data/air_quality_no2_long.csv", parse_dates=["datetime"])

Why are these pandas.Timestamp objects useful? 
Let’s illustrate the added value with some example cases. What is the start and end date of the time series data set we are working with?
air_quality.datetime.min(), air_quality.datetime.max()
(Timestamp('2019-05-07 01:00:00+0000', tz='UTC'),
 Timestamp('2019-06-21 00:00:00+0000', tz='UTC'))

 Using pandas.Timestamp for datetimes enables us to calculate with date information and make them comparable. Hence, we can use this to get the length of our time series:
 air_quality.datetime.max() -  air_quality.datetime.min()
 # Timedelta('44 days 23:00:00')

 The result is a pandas.Timedelta object, similar to datetime.timedelta from the standard Python library and defining a time duration.

 I want to add a new column to the DataFrame containing only the month of the measurement
air_quality['month'] = air_quality.datetime.dt.month
air_quality.head()

By using Timestamp objects for dates, a lot of time-related properties are provided by pandas. For example the month, but also year, weekofyear, quarter,. . . All of these properties are accessible by the dt accessor.

What is the average 𝑁𝑂2 concentration for each day of the week for each of the measurement locations?
air_quality.groupby([air_quality.datetime.dt.weekday, 'location'])['value'].mean()

Here, we want to calculate a given statistic (e.g. mean 𝑁𝑂2) for each weekday and for each measurement location. To group on weekdays, we use the datetime property weekday (with Monday=0 and Sunday=6) of pandas Timestamp, which is also accessible by the dt accessor. The grouping on both locations and weekdays can be done to split the calculation of the mean on each of these combinations.

Plot the typical 𝑁𝑂2 pattern during the day of our time series of all stations together. In other words, what is the average value for each hour of the day?

fig, axs = plt.subplots(figsize=(12,4))
air_quality.groupby(air_quality.datetime.dt.hour)['value'].mean().plot(
    kind='bar', rot=270, ax=axs)
plt.xlabel('Hour of the day')
plt.ylabel('$NO_2 (μg/m^3)$')
fig.savefig('hourlyPollution.png')

Similar to the previous case, we want to calculate a given statistic (e.g. mean 𝑁𝑂2) for each hour of the day and we can use the split-apply-combine approach again. For this case, we use the datetime property hour of pandas Timestamp, which is also accessible by the dt accessor.

Datetime as index
------------------
In the tutorial on reshaping, pivot() was introduced to reshape the data table with each of the measurements locations as a separate column:
no_2 = air_quality.pivot(index='datetime', columns='location', values='value')
no_2.head()

Note: By pivoting the data, the datetime information became the index of the table. In general, setting a column as an index can be achieved by the set_index function.

Working with a datetime index (i.e. DatetimeIndex) provides powerful functionalities. For example, we do not need the dt accessor to get the time series properties, but have these properties available on the index directly:
no_2.index.year, no_2.index.month_name

Some other advantages are the convenient subsetting of time period or the adapted time scale on plots. Let’s apply this on our data.

I want to Create a plot of the 𝑁𝑂2 values in the different stations from the 20th of May till the end of 21st of May
no_2['2019-05-20':'2019-05-21'].plot(figsize=(12,6))

By providing a string that parses to a datetime, a specific subset of the data can be selected on a DatetimeIndex.

RESAMPLE A TIME SERIES TO ANOTHER FREQUENCY
--------------------------------------------
Aggregate the current hourly(currently the dataset consists of hourly values) time series values to the monthly maximum value in each of the stations.

monthly_max = no_2.resample('M').max()
monthly_max

A very powerful method on time series data with a datetime index, is the ability to resample() time series to another frequency (e.g., converting secondly data into 5-minutely data).
The resample() method is similar to a groupby operation:
• it provides a time-based grouping, by using a string (e.g. M, 5H,. . . ) that defines the target frequency 
• it requires an aggregation function such as mean, max,. . .

no_2.resample('6H').mean().plot(figsize=(12,6))

An overview of the aliases used to define time series frequencies is given in the offset aliases overview table. When defined, the frequency of the time series is provided by the freq attribute:

monthly_max.index.freq # <MonthEnd>
monthly_max.index.freqstr # 'M'

I want to Make a plot of the daily mean 𝑁𝑂2 value in each of the stations.
no_2.resample('D').mean().plot(style='-o', figsize=(12,6))

REMEMBER
* Valid date strings can be converted to datetime objects using to_datetime function or as part of read functions.
* Datetime objects in pandas support calculations, logical operations and convenient date-related properties using the dt accessor.
* A DatetimeIndex contains these date-related properties and supports convenient slicing.
* Resample is a powerful method to change the frequency of a time series.

HOW TO MANIPULATE TEXTUAL DATA?
This tutorial uses the Titanic data set, stored as CSV.
titanic = pd.read_csv("data/titanic.csv")

Make all name characters lowercase.
To make each of the strings in the Name column lowercase, select the Name column, add the str accessor and apply the lower method. As such, each of the strings is converted element-wise.

titanic.Name.str.lower()

Create a new column Surname that contains the surname of the passengers by extracting the part before the comma.

titanic.Name.str.split(',')

Using the Series.str.split() method, each of the values is returned as a list of 2 elements. The first element is the part before the comma and the second element is the part after the comma.

titanic['Surname'] = titanic.Name.str.split(',').str.get(0)
titanic.head()

As we are only interested in the first part representing the surname (element 0), we can again use the str accessor and apply Series.str.get() to extract the relevant part. Indeed, these string functions can be concatenated to combine multiple functions at once!

I want to Extract the passenger data about the countesses on board of the Titanic.

titanic[titanic.Name.str.contains('Countess')]

The string method Series.str.contains() checks for each of the values in the column Name if the string contains the word Countess and returns for each of the values True (Countess is part of the name) or False (Countess is not part of the name). This output can be used to subselect the data using conditional (boolean) indexing introduced in the subsetting of data tutorial. As there was only one countess on the Titanic, we get one row as a result.

Note: More powerful extractions on strings are supported, as the Series.str.contains() and Series. str.extract() methods accept regular expressions,

Which passenger of the Titanic has the longest name?
To get the longest name we first have to get the lengths of each of the names in the Name column. By using pandas string methods, the Series.str.len() function is applied to each of the names individually (element-wise).

titanic.Name.str.len().idxmax()

Next, we need to get the corresponding location, preferably the index label, in the table for which the name length is the largest. The idxmax() method does exactly that. It is not a string method and is applied to integers, so no str is used.

titanic.iloc[titanic.Name.str.len().idxmax()]['Name']
titanic.loc[titanic.Name.str.len().idxmax()]['Name']

Based on the index name of the row (307) and the column (Name), we can do a selection using the loc or iloc operator.

In the “Sex” column, replace values of “male” by “M” and values of “female” by “F”.

titanic['Sex_short'] = titanic.Sex.replace({'male':'M', 'female':'F'})
titanic.head(2)

Whereas replace() is not a string method, it provides a convenient way to use mappings or vocabularies to translate certainvalues.Itrequiresadictionarytodefinethemapping{from : to}.

Warning: There is also a replace() method available to replace a specific set of characters. However, when having a mapping of multiple values, this would become:
titanic["Sex_short"] = titanic["Sex"].str.replace("female", "F")
titanic["Sex_short"] = titanic["Sex_short"].str.replace("male", "M")
This would become cumbersome and easily lead to mistakes. Just think (or try out yourself) what would happen if those two statements are applied in the opposite order.

REMEMBER
* String methods are available using the str accessor.
* String methods work element-wise and can be used for conditional indexing.
* The replace method is a convenient method to convert values according to a given dictionary.


=================================Out of the box================================
||Accessors||
=============
Pandas is a widely-used data analysis and manipulation library in Python. It provides numerous functions and methods to work with any type of data. There are also methods that work only with a specific data type. These methods are accessed through 4 accessors.
The accessors extend the capabilities of Pandas and provide specific operations. For instance, extracting the month from the date can be done using the dt accessor.
In this post, we will see various operations with 4 accessors of Pandas which are:
str: String data type
    Eg: Get only the row which have alphabets from a series.
            a = pd.Series(['a',10,'bcd1',20,'efs'])
            a[a.str.isalpha().replace(np.nan, False)]

cat: Categorical data type
    For categorical data it is more efficient to work with categorical datatype than using the object datatype. It makes a significant difference in terms of memory and speed especially when the data has low cardinality(i.e number of categories is low compared to the number of observations)

    cate = pd.Series(['A','B','A','A','B','C'], dtype='category')
    cate.cat.categories # Index(['A', 'B', 'C'], dtype='object')
    cate.cat.rename_categories({'A':1, 'B':2, 'C':3})
    #cate[0]='D' # ValueError: Cannot setitem on a Categorical with a new category, set the categories first
    cate.cat.add_categories('D', inplace=True)
    cate[0]='D'
    cate

dt: Datetime, Timedelta, Period data types
    dts = pd.Series(pd.date_range('2021.01.01', periods=5, freq='10D'))
        dts.dt.day
        dts.dt.year
        dts.dt.month
        dts.dt.date
        dts.dt.hour
        dts.dt.minute
        dts.dt.second
        dts.dt.weekday
        dts.dt.isocalendar().week
        dts.dt.is_month_start

sparse: Sparse data type

=====================================================================================

CHAPTER 2 - USER GUIDE
------------------------
2.1.1 Object creation

Creating a Series by passing a list of values, letting pandas create a default integer index:
s = pd.Series([1,3,5,np.nan,6,8])

pd.date_range() - Return a fixed frequency DatetimeIndex.
    start=None, end=None, periods=None, freq=None, tz=None, normalize=False, name=None, closed=None, **kwarg

Creating a DataFrame by passing a NumPy array, with a datetime index and labeled columns:
dates = pd.date_range("20210101", periods=6, freq='2D')
df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list("ABCD"))
df

df2 = pd.DataFrame({
    'A':1.0,
    'B':pd.Timestamp('20210102'),
    'C':pd.Series(1, index=list(range(4)), dtype='float32'),
    'D':np.array([3]*4, dtype='int32'),
    'E':pd.Categorical(['test', 'train', 'test', 'train', ]),
    'F':'foo',
})
df2.info()

The columns of the resulting DataFrame have different dtypes.
df2.dtypes


DataFrame.to_numpy() gives a NumPy representation of the underlying data. Note that this can be an expensive operation when your DataFrame has columns with different data types, which comes down to a fundamental differ- ence between pandas and NumPy: NumPy arrays have one dtype for the entire array, while pandas DataFrames have one dtype per column. When you call DataFrame.to_numpy(), pandas will find the NumPy dtype that can hold all of the dtypes in the DataFrame. This may end up being object, which requires casting every value to a Python object.

For df, our DataFrame of all floating-point values, DataFrame.to_numpy() is fast and doesn’t require copying data.
df.to_numpy()

For df2, the DataFrame with multiple dtypes, DataFrame.to_numpy() is relatively expensive.
df2.to_numpy()
array([[1.0, Timestamp('2021-01-02 00:00:00'), 1.0, 3, 'test', 'foo'],
       [1.0, Timestamp('2021-01-02 00:00:00'), 1.0, 3, 'train', 'foo'],
       [1.0, Timestamp('2021-01-02 00:00:00'), 1.0, 3, 'test', 'foo'],
       [1.0, Timestamp('2021-01-02 00:00:00'), 1.0, 3, 'train', 'foo']],
      dtype=object)

Note: DataFrame.to_numpy()does not include the index orc olumn labels in the output.

2.1.2 Viewing data
-------------------
Transposing your data:
df.T

Sorting by an axis:
df.sort_index(axis=1, ascending=False)

                D	        C	        B	        A
2021-01-01	-0.030514	-0.875529	-0.055480	-0.503135
2021-01-03	0.122823	-0.488960	-1.026299	-0.982295

df.sort_index(ascending=False)

                A	        B	        C	        D
2021-01-11	-0.537115	0.030939	-0.311905	0.598476
2021-01-09	-0.873883	0.678836	-0.255343	-0.826411

Sorting by values:
df.sort_values('B')

2.1.3 Selection
----------------
Note: While standard Python / Numpy expressions for selecting and setting are intuitive and come in handy for interactive work, for production code, we recommend the optimized pandas data access methods, .at, .iat, .loc and .iloc.

Getting
  Selecting a single column, which yields a Series, equivalent to df.A:
  df['A']

 Selecting via [], which slices the rows.
  df[0:3]
  df["20210102":"20210106"]

 Selection by label
 ------------------
 For getting a cross section using a label:
 df.loc[dates[0]]

Selecting on a multi-axis by label:
df[['A','B']] ~ df.loc[:,['A','B']]

Showing label slicing, both endpoints are included:
df.loc['20210101':'20210105',['A','C']]

Reduction in the dimensions of the returned object:
df.loc['2021-01-03',['A','C']]

For getting a scalar value
df.loc[dates[0], 'A'] # -0.5031

For getting fast access to a scalar (equivalent to the prior method):
df.at[dates[0], 'A'] # -0.5031

which one is faster?
%timeit df.loc[dates[0], 'A'] # 30.3 µs ± 1.27
%timeit df.at[dates[0], 'A'] # 23.4 µs ± 5.19 µs

Selection by position(index)
-----------------------------
Select via the position of the passed integers:
df.iloc[3]

By integer slices, acting similar to numpy/Python:
df.iloc[2:5, 2:4]

By lists of integer position locations, similar to the NumPy/Python style:
df.iloc[[1,2,4], [0,2]]

For slicing rows explicitly:
df.iloc[2:5,]

For slicing columns explicitly:
df.iloc[:, 2:4]

For getting a value explicitly:
df.iloc[2,2] # 0.79

For getting fast access to a scalar (equivalent to the prior method):
df.iat[2,2] # 0.790

Boolean indexing
-----------------
Using a single column’s values to select data.
df[df['A']>0]

Selecting values from a DataFrame where a boolean condition is met.
df[df>0]

Using the isin() method for filtering:
df2 = df.copy()
df2['E'] = [ 'one', 'one', 'two', 'three', 'four', 'three', ]
df2[df2.E.isin(['two', 'four'])]

Setting
-------
Setting a new column automatically aligns the data by the indexes.
s1 = pd.Series(np.arange(1,7), index=pd.date_range('20210101', periods=6))
s1

Setting values by label:
df.at[dates[0], 'A'] = 0

Setting values by position:
df.iat[0, 1] = 0

Setting by assigning with a NumPy array:
df['D'] = np.array([3] * len(df))

A where operation with setting.
df2 = df.copy()
df2[df2>0] = -df2

df2 = df.copy()
df2 = df2.where(df2<0,-df2)

2.1.4 Missing data
-------------------
pandas primarily uses the value np.nan to represent missing data. It is by default not included in computations.

Reindexing allows you to change/add/delete the index on a specified axis. This returns a copy of the data.

reindex - assign new index to existing dataframe.

df1 = df.reindex(index=dates[0:4], columns=list(df.columns)+['E'])
df1.loc[dates[0:2], 'E'] = 1

To drop any rows that have missing data.
df.dropna(how = 'any') ~ df.dropna()

which one is faster?
%timeit df.dropna() # 1.59 ms ± 13.5 µs per loop
%timeit df.dropna(how = 'any') # 1.67 ms ± 50.9 µs per loop

%timeit titanic.dropna() # 1.97 ms ± 117 µs per loop
%timeit titanic.dropna(how='any') # 2.2 ms ± 196 µs per loop

Filling missing data.
df1.fillna(value=5) ~ df1.fillna(5)

To get the boolean mask where values are nan.
pd.isna(df1)

2.1.5 Operations
----------------
Stats
Operations in general exclude missing data.
df.mean() # To get mean of each column

Same operation on the other axis:
df.mean(axis=1) ~ df.mean(1) # To get mean of each row
df.mean.mean() # To get a scaler which is mean of complete dataframe

Operating with objects that have different dimensionality and need alignment. In addition, pandas automatically broadcasts along the specified dimension.

s = pd.Series([1,3,5,np.nan,6,8], index=dates).shift(2)
df.sub(s, axis='index')
df.rsub(s, axis='index')

Apply
------
Applying functions to the data:
df.apply(np.cumprod)

df.apply(lambda x: x.max() - x.min())
df.apply(lambda x: x.max() - x.min(), axis=1)

Histogramming:
get the frequency.

s = pd.Series(np.random.randint(0,7,size=10))
s.value_counts()

String Methods
--------------
Series is equipped with a set of string processing methods in the str accessor that make it easy to operate on each element of the array, as in the code snippet below. Note that pattern-matching in str generally uses regular expressions by default (and in some cases always uses them).

s = pd.Series(['A','B','V','CABA',np.nan, 'RAVA'])
s.str.lower()

2.1.6 Merge
------------
Concat
------
pandas provides various facilities for easily combining together Series and DataFrame objects with various kinds of set logic for the indexes and relational algebra functionality in the case of join / merge-type operations.
Concatenating pandas objects together with concat():

df = pd.DataFrame(np.random.randn(10,4))
# let's break it into pieces
pieces = [df[:3], df[3:7], df[7:]]
pd.concat(pieces)

Note: Adding a column to a DataFrame is relatively fast. However, adding a row requires a copy, and may be expensive. We recommend passing a pre-built list of records to the DataFrame constructor instead of building a DataFrame by iteratively appending records to it.

Join
-----
SQL style merges
left = pd.DataFrame({"key": ["foo", "foo"], "lval": [1, 2]})
right = pd.DataFrame({"key": ["foo", "foo"], "rval": [4, 5]})
pd.merge(left, right, on="key")

2.1.7 Grouping
--------------
By “group by” we are referring to a process involving one or more of the following steps: • Splitting the data into groups based on some criteria
• Applying a function to each group independently
• Combining the results into a data structure

df = pd.DataFrame(
    {
    "A": ["foo", "bar", "foo", "bar", "foo", "bar", "foo", "foo"],
    "B": ["one", "one", "two", "three", "two", "two", "one", "three"],
    "C": np.random.randn(8),
    "D": np.random.randn(8),
    }
)
Grouping and then applying the sum() function to the resulting groups.
df.groupby(["A", "B"]).sum()

2.1.8 Reshaping
---------------
tuples = list(zip(*[
    ["bar", "bar", "baz", "baz", "foo", "foo", "qux", "qux"],
    ["one", "two", "one", "two", "one", "two", "one", "two"],
]))

index = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])
df = pd.DataFrame(np.random.randn(8,2), index=index, columns=['A','B'])
df2 = df[:4]
df2

The stack() method “compresses” a level in the DataFrame’s columns.
stacked = df2.stack()
stacked
(It is similar to melt but it maintains index as well)
first  second   
bar    one     A    0.709423
               B   -0.276359
       two     A    0.771991
               B   -0.523291
baz    one     A    0.033710
               B   -0.099103
       two     A   -0.046292
               B    0.984227
dtype: float64

With a “stacked” DataFrame or Series (having a MultiIndex as the index), the inverse operation of stack() is unstack(), which by default unstacks the last level:

stacked.unstack()
                		A	       B
first	second		
bar	    one	        0.709423	-0.276359
        two	        0.771991	-0.523291
baz	    one	        0.033710	-0.099103
        two	       -0.046292	0.984227

stacked.unstack([0,1])
first	bar	baz
second	one	two	one	two
A	0.709423	0.771991	0.033710	-0.046292
B	-0.276359	-0.523291	-0.099103	0.984227

Pivot tables
------------
Reshape - Long to Wide

df = pd.DataFrame({
    "A": ["one", "one", "two", "three"] * 3,
    "B": ["A", "B", "C"] * 4,
    "C": ["foo", "foo", "foo", "bar", "bar", "bar"] * 2,
    "D": np.random.randn(12),
    "E": np.random.randn(12),
})
df.pivot_table(columns=['C'], values='D', index=['A','B'])

2.1.9 Time series
------------------
pandas has simple, powerful, and efficient functionality for performing resampling operations during frequency con- version (e.g., converting secondly data into 5-minutely data). This is extremely common in, but not limited to, financial applications.

rng = pd.date_range('2021/01/01', periods=100, freq='S')
ts = pd.Series(np.random.randint(0,500,len(rng)), index=rng)
ts.resample('5M').mean()

Time zone representation:
rng = pd.date_range('3/6/2020 00:00', periods=5)
ts = pd.Series(np.random.randn(len(rng)), index=rng)
ts_utc = ts.tz_localize('UTC')
Converting to another time zone:
ts_utc.tz_convert('US/Eastern')

Converting between time span representations:
rng = pd.date_range('3/6/2020 00:00', periods=5, freq='M')
ts = pd.Series(np.random.randn(len(rng)), index=rng)
ps = ts.to_period()
ps.to_timestamp()

Converting between period and timestamp enables some convenient arithmetic functions to be used. In the following example, we convert a quarterly frequency with year ending in November to 9am of the end of the month following the quarter end:

prng = pd.period_range('1990Q1', '2000Q4', freq='Q-NOV')
ts = pd.Series(np.random.randn(len(prng)), index=prng)
ts.index = (prng.asfreq('M','e')+1).asfreq('H','s')+9
ts





Attributes:
df.T
.freqstr
.freq
.dt.month, .dt.year. .dt.hour, .dt.day, .dt.weekday
index
columns
shape
dtypes
loc
iloc

Methods:
.to_timestamp - get the first day of the month
.to_period - convert date to the freq like in Months
tz_convert - convert timezone to anotehr timezone
tz_localize - specify timezone of a particular country
pd.MultiIndex.from_tuples() 
stack()
df.rsub - opposite of sub(), first apply sub and then multiply values with -1
df.sub() - subtract
isna()
fillna()
dropna()
reindex()
date_range()
idxmax() - get the max index
replace({from:to})
read_csv(parse_dates=['colName'])
to_datetime()
merge(,,how, left_on, right_on)
concat(, axis=, keys=)
melt(id_var=, value_var=, value_name=, var_name=)
set_index()
reset_index()
pivot_table(colums=, values=, index=, aggfunc=, margins=)
pivot(columns=, values=)
size()
count()
sort_index()
sort_values()
value_counts()
mean(), median(), mode(), skew(), min(), max()
groupby().aggregateFunction()
agg({col=[fn1, fn2]})
rename(columns={'currentCol':'newCol'})
fig.savefig()
plt.subplot()
plot()
notna()
isin()
max()
head()
tail()
describe()
info()


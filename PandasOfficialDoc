PandasOfficialDoc
-----------------

conda activate myenv
jupyter notebook

pandas: powerful Python data analysis toolkit Release 1.2.3
------------------------------------------------------------

In pandas a data table is called a DataFrame.

pandas supports the integration with many file formats or data sources out of the box (csv, excel, sql, json, parquet,. . . ). Importing data from each of these data sources is provided by function with the prefix read_*. Similarly, the to_* methods are used to store data.

There is no need to loop over all rows of your data table to do calculations. Data manipulations on a column work elementwise. Adding a column to a DataFrame based on existing data in other columns is straightforward.

Split-apply-combine approach:
Basic statistics (mean, median, min, max, counts...) are easily calculable. These or custom aggregations can be applied on the entire data set, a sliding window of the data or grouped by categories. The latter is also known as the split-apply-combine approach.

Change the structure of your data table in multiple ways. You can melt() your data table from wide to long/tidy form or pivot() from long to wide format. With aggregations built-in, a pivot table is created with a single command.

Multiple tables can be concatenated both column wise as row wise and database-like join/merge operations are provided to combine multiple tables of data.

pandas has great support for time series and has an extensive set of tools for working with dates, times, and time-indexed data.

Data sets do not only contain numerical data. pandas provides a wide range of functions to clean textual data and extract useful information from it.

Handling Import Errors:
If you encounter an ImportError, it usually means that Python couldn‚Äôt find pandas in the list of available libraries. Python internally has a list of directories it searches through, to find packages. You can obtain these directories with:
import sys 
sys.path

Dependencies 
Package             Minimum supported version
setuptools              24.2.0
NumPy                   1.16.5
python-dateutil         2.7.3
pytz                    2017.3

Package Overview:
-----------------
pandas is a Python package providing fast, flexible, and expressive data structures designed to make working with ‚Äúrelational‚Äù or ‚Äúlabeled‚Äù data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real-world data analysis in Python. Additionally, it has the broader goal of becoming the most powerful and flexible open source data analysis/manipulation tool available in any language. It is already well on its way towards this goal.
pandas is well suited for many different kinds of data:
‚Ä¢ Tabular data with heterogeneously-typed columns, as in a SQL table or Excel spreadsheet
‚Ä¢ Ordered and unordered (not necessarily fixed-frequency) time series data.
‚Ä¢ Arbitrary matrix data (homogeneously typed or heterogeneous) with row and column labels
‚Ä¢ Any other form of observational / statistical data sets. The data need not be labeled at all to be placed into a pandas data structure.

The two primary data structures of pandas, Series (1-dimensional) and DataFrame (2-dimensional), handle the vast majority of typical use cases in finance, statistics, social science, and many areas of engineering. pandas is built on top of NumPy and is intended to integrate well within a scientific computing environment with many other 3rd party libraries.

Here are just a few of the things that pandas does well:
Easy handling of missing data (represented as NaN) in floating point as well as non-floating point data 
Size mutability: columns can be inserted and deleted from DataFrame and higher dimensional objects
Automatic and explicit data alignment: objects can be explicitly aligned to a set of labels,or the user can simply ignore the labels and let Series, DataFrame, etc. automatically align the data for you in computations.
Powerful, flexible group by functionality to perform split-apply-combine operations on data sets, for both aggregating and transforming data
Make it easy to convert ragged, differently-indexed data in other Python and NumPy data structures into DataFrame objects
Intelligent label-based slicing, fancy indexing, and subsetting of large data sets Intuitive merging and joining data sets
Flexible reshaping and pivoting of data sets
Hierarchical labeling of axes (possible to have multiple labels per tick)
Robust IO tools for loading data from flat files (CSV and delimited), Excel files, databases, and saving / loading data from the ultrafast HDF5 format
Time series-specific functionality: date range generation and frequency conversion, moving window statistics, date shifting, and lagging.

Many of these principles are here to address the shortcomings frequently experienced using other languages / scientific research environments. For data scientists, working with data is typically divided into multiple stages: munging and cleaning data, analyzing / modeling it, then organizing the results of the analysis into a form suitable for plotting or tabular display. pandas is the ideal tool for all of these tasks.
Some other notes
‚Ä¢ pandas is fast. Many of the low-level algorithmic bits have been extensively tweaked in Cython code. However, as with anything else generalization usually sacrifices performance. So if you focus on one feature for your application you may be able to create a faster specialized tool.
‚Ä¢ pandas is a dependency of statsmodels, making it an important part of the statistical computing ecosystem in Python.
‚Ä¢ pandas has been used extensively in production in financial applications.

DataStructures
df = DataFrame({
    'Dimensions':[1,2],
    'Name':['Series', 'DataFrame'],
    'Description':['1D labeled homogeneously-typed array',
                  'General 2D labeled, size-mutable tabular structure with potentially heterogeneously-typed column']
})
df


Dimensions	Name	    Description
	1	    Series	    1D labeled homogeneously-typed array
	2	    DataFrame	General 2D labeled, size-mutable tabular structure  with
                         potentially heterogeneously-typed column

Why more than one data structure?
The best way to think about the pandas data structures is as flexible containers for lower dimensional data. For example, DataFrame is a container for Series, and Series is a container for scalars. We would like to be able to insert and remove objects from these containers in a dictionary-like fashion.
Also, we would like sensible default behaviors for the common API functions which take into account the typical orientation of time series and cross-sectional data sets. When using the N-dimensional array (ndarrays) to store 2- and 3-dimensional data, a burden is placed on the user to consider the orientation of the data set when writing functions; axes are considered more or less equivalent (except when C- or Fortran-contiguousness matters for performance). In pandas, the axes are intended to lend more semantic meaning to the data; i.e., for a particular data set, there is likely to be a ‚Äúright‚Äù way to orient the data. The goal, then, is to reduce the amount of mental effort required to code up data transformations in downstream functions.
For example, with tabular data (DataFrame) it is more semantically helpful to think of the index (the rows) and the columns rather than axis 0 and axis 1. Iterating through the columns of the DataFrame thus results in more readable code:

for col in df.columns:
    series = df[col]
    # do something with series

Mutability and copying of data
All pandas data structures are value-mutable (the values they contain can be altered) but not always size-mutable. The length of a Series cannot be changed, but, for example, columns can be inserted into a DataFrame. However, the vast majority of methods produce new objects and leave the input data untouched. In general we like to favor immutability where sensible.

1.4.3 Getting started tutorials
WHAT KIND OF DATA DOES PANDAS HANDLE?
---------------------------------------
I want to start using pandas
import pandas as pd

To load the pandas package and start working with it, import the package. The community agreed alias for pandas is pd, so loading pandas as pd is assumed standard practice for all of the pandas documentation.

pandas data table representation
I want to store passenger data of the Titanic. For a number of passengers, I know the name (characters), age (integers) and sex (male/female) data.

df = pd.DataFrame({
    'Names' : ['Ramesh Kumar', 'Suresh Kumar', 'Rupesh Kumar'],
    'Age' : [22, 35, 58],
    'Sex' : ['male', 'male', 'female']
})
df

To manually store data in a table, create a DataFrame. When using a Python dictionary of lists, the dictionary keys will be used as column headers and the values in each list as columns of the DataFrame.

A DataFrame is a 2-dimensional data structure that can store data of different types (including characters, integers, floating point values, categorical data and more) in columns. It is similar to a spreadsheet, a SQL table 

‚Ä¢ The table has 3 columns, each of them with a column label. The column labels are respectively Name, Age and Sex.
‚Ä¢ The columnName consists of textual data with each value a string,the column Age are numbers and the column Sex is textual data.

EACH COLUMN IN A DATAFRAME IS A SERIES
I‚Äôm just interested in working with the data in the column Age
df['Age]

When selecting a single column of a pandas DataFrame, the result is a pandas Series. To select the column, use the column label in between square brackets [].

Note: If you are familiar to Python dictionaries, the selection of a single column is very similar to selection of dictionary values based on the key.

You can create a Series from scratch as well:

ages = pd.Series([22, 35, 58])
ages

0    22
1    35
2    58
dtype: int64

A pandas Series has no column labels, as it is just a single column of a DataFrame. A Series does have row labels.

Do something with a DataFrame or Series
I want to know the maximum Age of the passengers
We can do this on the DataFrame by selecting the Age column and applying max():
df['Age'].max() # DataFrame
ages.max() # Series

As illustrated by the max() method, you can do things with a DataFrame or Series. pandas provides a lot of functionalities, each of them a method you can apply to a DataFrame or Series. As methods are functions, do not forget to use parentheses ().
I‚Äôm interested in some basic statistics of the numerical data of my data table
df.describe()

The describe() method provides a quick overview of the numerical data in a DataFrame. As the Name and Sex columns are textual data, these are by default not taken into account by the describe() method.
Many pandas operations return a DataFrame or a Series. The describe() method is an example of a pandas operation returning a pandas Series.

Bullet Points:
--------------
‚Ä¢ Import the package, aka import pandas as pd
‚Ä¢ A table of data is stored as a pandas DataFrame
‚Ä¢ Each column in a DataFrame is a Series
‚Ä¢ You can do things by applying a method to a DataFrame or Series

This tutorial uses the Titanic data set, stored as CSV. The data consists of the following data columns: 
‚Ä¢ PassengerId: Id of every passenger.
‚Ä¢ Survived: This feature have value 0 and 1. 0 for not survived and 1 for survived.
‚Ä¢ Pclass: There are 3 classes: Class 1, Class 2 and Class 3.
‚Ä¢ Name: Name of passenger.
‚Ä¢ Sex: Gender of passenger.
‚Ä¢ Age: Age of passenger.
‚Ä¢ SibSp: Indication that passenger have siblings and spouse. 
‚Ä¢ Parch: Whether a passenger is alone or have family.
‚Ä¢ Ticket: Ticket number of passenger. 
‚Ä¢ Fare: Indicating the fare.
‚Ä¢ Cabin: The cabin of passenger.
‚Ä¢ Embarked: The embarked category.

HOW DO I READ AND WRITE TABULAR DATA?
--------------------------------------
I want to analyze the Titanic passenger data, available as a CSV file.
titanic = pd.read_csv('data/titanic.csv')
titanic

pandas provides the read_csv() function to read data stored as a csv file into a pandas DataFrame. pandas supports many different file formats or data sources out of the box (csv, excel, sql, json, parquet, . . . ), each of them with the prefix read_*.
Make sure to always have a check on the data after reading in the data. When displaying a DataFrame, the first and last 5 rows will be shown by default:

I want to see the first 8 rows of a pandas DataFrame.
titanic.head(8)
To see the first N rows of a DataFrame, use the head() method with the required number of rows (in this case 8) as argument.

Interested in the last N rows instead? pandas also provides a tail() method. For example, titanic.tail(10) will return the last 10 rows of the DataFrame.

A check on how pandas interpreted each of the column data types can be done by requesting the pandas dtypes attribute:
titanic.dtypes
PassengerId      int64
Pclass           int64
Name            object
Sex             object
Age            float64

For each of the columns, the used data type is enlisted. The data types in this DataFrame are integers (int64), floats (float64) and strings (object).

NOTE: 
When asking for the dtypes, no brackets are used! dtypes is an attribute of a DataFrame and Series. Attributes of DataFrame or Series do not need brackets. Attributes represent a characteristic of a DataFrame/Series, whereas a method (which requires brackets) do something with the DataFrame/Series.

My colleague requested the Titanic data as a spreadsheet.
titanic.to_excel("titanic.xlsx", sheet_name="passengers", index=False)

Whereas read_* functions are used to read data to pandas, the to_* methods are used to store data. The to_excel() method stores the data as an excel file. In the example here, the sheet_name is named passengers instead of the default Sheet1. By setting index=False the row index labels are not saved in the spreadsheet.
The equivalent read function read_excel() will reload the data to a DataFrame:
titanic = pd.read_excel("titanic.xlsx", sheet_name="passengers")

I‚Äôm interested in a technical summary of a DataFrame

titanic.info()

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   PassengerId  891 non-null    int64  
 1   Survived     891 non-null    int64  
 2   Pclass       891 non-null    int64  
 3   Name         891 non-null    object

dtypes: float64(2), int64(4), object(5)
memory usage: 36.0+ KB

The method info() provides technical information about a DataFrame, so let‚Äôs explain the output in more detail:
In [1]: import pandas as pd

This tutorial uses the Titanic data set, stored as CSV. The data consists of the following data columns:
‚Ä¢ PassengerId: Id of every passenger.
It is indeed a DataFrame.
There are 891 entries, i.e. 891 rows.
Each row has a row label (aka the index) with values ranging from 0 to 890.
The table has 12 columns. Most columns have a value for each of the rows (all 891 values are non-null). Some columns do have missing values and less than 891 non-null values.
The columns Name, Sex, Cabin and Embarked consists of textual data (strings, aka object). The other columns are numerical data with some of them whole numbers (aka integer) and others are real numbers (aka float).
* The kind of data (characters, integers,. . . ) in the different columns are summarized by listing the dtypes. The approximate amount of RAM used to hold the DataFrame is provided as well.
* Getting data in to pandas from many different file formats or data sources is supported by read_* functions. Exporting data out of pandas is provided by different to_* methods.
* The head/tail/info methods and the dtypes attribute are convenient for a first check.

HOW DO I SELECT A SUBSET OF A TABLE?
HOW DO I SELECT A SUBSET OF A DATAFRAME?
HOW DO I SELECT SPECIFIC COLUMNS FROM A DATAFRAME?
I‚Äôm interested in the age of the Titanic passengers.
ages = titanic['Age']
ages.head()

To select a single column, use square brackets [] with the column name of the column of interest.
Each column in a DataFrame is a Series. As a single column is selected, the returned object is a pandas Series.
We can verify this by checking the type of the output:
type(titanic['Age']) # pandas.core.series.Series

And have a look at the shape of the output:
titanic['Age'].shape # (891,)
titanic.shape # (891, 12)

DataFrame.shape is an attribute of a pandas Series and DataFrame containing the number of rows and columns: (nrows, ncolumns). A pandas Series is 1-dimensional and only the number of rows is returned.

I‚Äôm interested in the age and sex of the Titanic passengers.
age_sex = titanic[['Age', 'Sex']]
age_sex.head()
To select multiple columns, use a list of column names within the selection brackets [].
Note: The inner square brackets define a Python list with columnnames,whereas the outer brackets are used to select the data from a pandas DataFrame as seen in the previous example.

The returned data type is a pandas DataFrame:
age_sex = titanic[['Age', 'Sex']]
age_sex.head()
type(age_sex) # pandas.core.frame.DataFrame
age_sex.shape # (891, 2)
The selection returned a DataFrame with 891 rows and 2 columns. Remember, a DataFrame is 2-dimensional with both a row and column dimension.

How do I filter specific rows from a DataFrame?
I‚Äôm interested in the passengers older than 35 years.
above_35 = titanic[titanic['Age'] > 35]
above_35.head()

To select rows based on a conditional expression, use a condition inside the selection brackets []. The condition inside the selection brackets 
titanic["Age"] > 35
checks for which rows the Age column has a value greater than 35
(titanic['Age']>35).head() 

0    False
1     True
2     True
3    False
4    False
Name: Age, dtype: bool

The output of the conditional expression (>, but also ==, !=, <, <=,. . . would work) is actually a pandas Series of boolean values (either True or False) with the same number of rows as the original DataFrame. Such a Series of boolean values can be used to filter the DataFrame by putting it in between the selection brackets []. Only rows for which the value is True will be selected.

We know from before that the original Titanic DataFrame consists of 418 rows. Let‚Äôs have a look at the number of rows which satisfy the condition by checking the shape attribute of the resulting DataFrame above_35:
above_35.shape # (105, 11)

I‚Äôm interested in the Titanic passengers from cabin class 2 and 3.
class_23 = titanic[titanic['Pclass'].isin([2,3])]
class_23.head()

Similar to the conditional expression, the isin() conditional function returns a True for each row the values are in the provided list. To filter the rows based on such a function, use the conditional function inside the selection brackets []. In this case, the condition inside the selection brackets titanic["Pclass"].isin([2, 3]) checks for which rows the Pclass column is either 2 or 3.
The above is equivalent to filtering by rows for which the class is either 2 or 3 and combining the two statements with an | (or) operator:

class_32 = titanic[(titanic['Pclass'] == 2 ) | (titanic['Pclass'] == 3 )]

Note: When combining multiple conditional statements, each condition must be surrounded by parentheses (). Moreover, you can not use or/and but need to use the or operator | and the and operator &.

I want to work with passenger data for which the age is known.
age_not_na = titanic[titanic['Age'].notna()]
age_not_na

The notna() conditional function returns a True for each row the values are not an Null value. As such, this can be combined with the selection brackets [] to filter the data table.
You might wonder what actually changed, as the first 5 lines are still the same values. One way to verify is to check if the shape has changed:
age_not_na = titanic[titanic['Age'].notna()]
age_not_na.shape # (714, 12)

How do I select specific rows and columns from a DataFrame?
I‚Äôm interested in the names of the passengers older than 35 years.
adult_names = titanic.loc[titanic['Age']>35, 'Name']
adult_names.head()

In this case, a subset of both rows and columns is made in one go and just using selection brackets [] is not sufficient anymore. The loc/iloc operators are required in front of the selection brackets []. WHEN USING LOC/ILOC, THE PART BEFORE THE COMMA IS THE ROWS YOU WANT, AND THE PART AFTER THE COMMA IS THE COLUMNS YOU WANT TO SELECT.
When using the column names, row labels or a condition expression, use the loc operator in front of the selection brackets []. For both the part before and after the comma, you can use a single label, a list of labels, a slice of labels, a conditional expression or a colon. USING A COLON SPECIFIES YOU WANT TO SELECT ALL ROWS OR COLUMNS.

I‚Äôm interested in rows 10 till 25 and columns 3 to 5.
titanic.iloc[9:25, 2:5]

Again, a subset of both rows and columns is made in one go and just using selection brackets [] is not sufficient anymore. When specifically interested in certain rows and/or columns based on their position in the table, use the iloc operator in front of the selection brackets [].
When selecting specific rows and/or columns with loc or iloc, new values can be assigned to the selected data. For example, to assign the name anonymous to the first 3 elements of the third column:
titanic.iloc[:3,2] = 'anonymous'
titanic

Bullet Points for Indexing:
‚Ä¢ When selecting subsets of data, square brackets [] are used.
‚Ä¢ Inside these brackets, you can use a single column/row label, a list of column/row labels, a slice of labels, a conditional expression or a colon.
‚Ä¢ Select specific rows and/or columns using loc when using the row and column names
‚Ä¢ Select specific rows and/or columns using iloc when using the positions in the table
‚Ä¢ You can assign new values to a selection based on loc/iloc.

For this tutorial, air quality data about ùëÅùëÇ2 is used, made available by openaq and using the py-openaq package. The air_quality_no2.csv data set provides ùëÅùëÇ2 values for the measurement stations FR04014, BETR801 and London Westminster in respectively Paris, Antwerp and London.
air_quality = pd.read_csv("data/air_quality_no2.csv", index_col=0, parse_dates=True)
air_quality.head()

	                station_antwerp	    station_paris	station_london
datetime			
2019-05-07 02:00:00	    NaN	                NaN	            23.0
2019-05-07 03:00:00	    50.5	            25.0	        19.0
2019-05-07 04:00:00	    45.0	            27.7	        19.0
2019-05-07 05:00:00	    NaN	                50.4	        16.0
2019-05-07 06:00:00	    NaN	                61.9        	NaN

air_quality.info()
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 1035 entries, 2019-05-07 02:00:00 to 2019-06-21 02:00:00
Data columns (total 3 columns):
 #   Column           Non-Null Count  Dtype  
---  ------           --------------  -----  
 0   station_antwerp  95 non-null     float64
 1   station_paris    1004 non-null   float64
 2   station_london   969 non-null    float64
dtypes: float64(3)
memory usage: 32.3 KB

Note: The usage of the index_col and parse_dates parameters of the read_csv function to define the first (0th) column as index of the resulting DataFrame and convert the dates in the column to Timestamp objects, respectively.

HOW TO CREATE PLOTS IN PANDAS?
------------------------------
I want a quick visual check of the data.
air_quality.plot()

With a DataFrame, pandas creates by default one 'line plot' for each of the columns with numeric data.

I want to plot only the columns of the data table with the data from Paris.
air_quality['station_paris'].plot()

To plot a specific column, use the selection method of the subset data in combination with the plot() method. Hence, the plot() method works on both Series and DataFrame.

air_quality.plot.scatter(x='station_london', y='station_paris', alpha=0.7)
# alpha make scatters solid as it reaches 1.

Apart from the default line plot when using the plot function, a number of alternatives are available to plot data. Let‚Äôs use some standard Python to get an overview of the available plot methods:
code:
[
    method_name
    for method_name in dir(air_quality.plot)
    if not method_name.startswith("_")
]
['area',
 'bar',
 'barh',
 'box',
 'density',
 'hexbin',
 'hist',
 'kde',
 'line',
 'pie',
 'scatter']

Note: In many development environments as well as IPython and Jupyter Notebook, use the TAB button to get an overview of the available methods, for example air_quality.plot. + TAB.

One of the options is DataFrame.plot.box(), which refers to a boxplot. The box method is applicable on the air quality example data:
air_quality.plot.box()
air_quality.describe() # to understand the boxplot.

I want each of the columns in a separate subplot.
axs = air_quality.plot.area(figsize=(12,6) , subplots=True)

Separate subplots for each of the data columns are supported by the subplots argument of the plot functions. The builtin options available in each of the pandas plot functions that are worthwhile to have a look.

I want to further customize, extend or save the resulting plot.
fig, axs = plt.subplots(figsize = (12, 4))
air_quality.plot.area(ax=axs)
axs.set_ylabel('NO$_2$ Concentration')
fig.savefig("no2_concentration.png")

Each of the plot objects created by pandas is a matplotlib object. As Matplotlib provides plenty of options to customize plots, making the link between pandas and Matplotlib explicit enables all the power of matplotlib to the plot. This strategy is applied in the previous example:

fig, axs = plt.subplots(figsize=(12, 4)) ## Create an empty matplotlib Figure and Axes
air_quality.plot.area(ax=axs) ## Use pandas to put the area plot on the prepared Figure/Axes
axs.set_ylabel("NO$_2$ concentration") ## Do any matplotlib customization you like
fig.savefig("no2_concentrations.png") ## Save the Figure/Axes using the existing matplotlib method.

Bullet Points on plots:
‚Ä¢ The .plot.* methods are applicable on both Series and DataFrames
‚Ä¢ By default, each of the columns is plotted as a different element (line, boxplot,. .) ‚Ä¢ Any plot created by pandas is a Matplotlib object.

HOW TO CREATE NEW COLUMNS DERIVED FROM EXISTING COLUMNS?
--------------------------------------------------------
I WANT TO EXPRESS THE ùëÅùëÇ2 CONCENTRATION OF THE STATION IN LONDON IN mg/m3

(If we assume temperature of 25 degrees Celsius and pressure of 1013 hPa, the conversion factor is 1.882)

air_quality['london_mg_per_cubic'] = air_quality['station_london']*1.882
air_quality.head()

To create a new column, use the [] brackets with the new column name at the left side of the assignment.
Note: The calculation of the values is done element_wise.This means all values in the given column are multiplied by the value 1.882 at once. You do not need to use a loop to iterate each of the rows!

I WANT TO CHECK THE RATIO OF THE VALUES IN PARIS VERSUS ANTWERP AND SAVE THE RESULT IN A NEW COLUMN
air_quality['ratio_paris_antwerp'] = (
    air_quality['station_paris'] / air_quality['station_antwerp']
)
air_quality.head()

The calculation is again element-wise, so the / is applied for the values in each row.
Also other mathematical operators (+, -, *, /) or logical operators (<, >, =,. . . ) work element wise.

I WANT TO RENAME THE DATA COLUMNS TO THE CORRESPONDING STATION IDENTIFIERS USED BY openAQ
air_quality_renamed = air_quality.rename(
    columns={
        'station_antwerp': 'BETR801',
        "station_paris": "FR04014",
        "station_london": "London Westminster",
    }
)
air_quality_renamed.head()

The rename() function can be used for both row labels and column labels. Provide a dictionary with the keys as the current names and the values as the new names to update the corresponding names.

The mapping should not be restricted to fixed names only, but can be a mapping function as well. For example, converting the column names to lowercase letters can be done using a function as well:

air_quality_renamed = air_quality_renamed.rename(columns=str.lower)
air_quality_renamed.head()

REMEMBER
* Create a new column by assigning the output to the DataFrame with a new column name in between the [].
* Operations are element-wise, no need to loop over rows.
* Use rename with a dictionary or function to rename row labels or column names.

HOW TO CALCULATE SUMMARY STATISTICS?
This tutorial uses the Titanic data set, stored as CSV. The data consists of the following data columns: ‚Ä¢ PassengerId: Id of every passenger.
‚Ä¢ Survived: This feature have value 0 and 1. 0 for not survived and 1 for survived.
‚Ä¢ Pclass: There are 3 classes: Class 1, Class 2 and Class 3.
‚Ä¢ Name: Name of passenger.
‚Ä¢ Sex: Gender of passenger.
‚Ä¢ Age: Age of passenger.
‚Ä¢ SibSp: Indication that passenger have siblings and spouse. ‚Ä¢ Parch: Whether a passenger is alone or have family.
‚Ä¢ Ticket: Ticket number of passenger. ‚Ä¢ Fare: Indicating the fare.
‚Ä¢ Cabin: The cabin of passenger.
‚Ä¢ Embarked: The embarked category.

titanic = pd.read_csv('data/titanic.csv')
titanic.head()

AGGREGATING STATISTICS
-----------------------
What is the average age of the Titanic passengers?
titanic['Age'].mean() # 29.699

Different statistics are available and can be applied to columns with numerical data. Operations in general exclude missing data and operate across rows by default.

What is the median age and ticket fare price of the Titanic passengers?
titanic[['Age', 'Fare']].median()
Age     28.0000
Fare    14.4542
dtype: float64

The statistic applied to multiple columns of a DataFrame is calculated for each numeric column.
The aggregating statistic can be calculated for multiple columns at the same time. Remember the describe function.
titanic[['Age', 'Fare']].describe()

Instead of the predefined statistics, specific combinations of aggregating statistics for given columns can be defined using the DataFrame.agg() method:
titanic.agg({
    'Age':['min', 'max', 'median', 'skew', 'mean'],
    'Fare':['min', 'max', 'median', 'skew', 'mean', 'std'],
})

AGGREGATING STATISTICS GROUPED BY CATEGORY
-------------------------------------------
What is the average age for male versus female Titanic passengers?
titanic[['Sex','Age']].groupby('Sex').mean()
	        Age
Sex	
female	27.915709
male	30.726645

As our interest is the average age for each gender, a subselection on these two columns is made first: titanic[[ "Sex", "Age"]]. Next, the groupby() method is applied on the Sex column to make a group per category. The average age for each gender is calculated and returned.
Calculating a given statistic (e.g. mean age) for each category in a column (e.g. male/female in the Sex column) is a common pattern. The groupby method is used to support this type of operations. More general, this fits in the more general split-apply-combine pattern:
‚Ä¢ Split the data into groups
‚Ä¢ Apply a function to each group independently 
‚Ä¢ Combine the results into a data structure

Split - Sex in Male and Female
Apply - Mean function on each group splitted
Combine - Combbine the result and present

The apply and combine steps are typically done together in pandas.

In the previous example, we explicitly selected the 2 columns first. If not, the mean method is applied to each column containing numerical columns:
titanic.groupby('Sex').mean()


It does not make much sense to get the average value of the Pclass. if we are only interested in the average age for each gender, the selection of columns (rectangular brackets [] as usual) is supported on the grouped data as well:
titanic.groupby('Sex')['Age'].mean()

Why option is Faster?
%timeit titanic.groupby('Sex')['Age'].mean() # 801 ¬µs ¬± 45.4 ¬µs
%timeit titanic[['Sex','Age']].groupby('Sex').mean() # 24.1 ms ¬± 282 ¬µs

Note: ThePclass column contains numerical data butactually represents 3 categories(orfactors) respectively the labels ‚Äò1‚Äô, ‚Äò2‚Äô and ‚Äò3‚Äô. Calculating statistics on these does not make much sense. Therefore, pandas provides a Categorical data type to handle this type of data.

What is the mean ticket fare price for each of the sex and cabin class combinations?
titanic.groupby(['Sex', 'Pclass'])['Fare'].mean()
Grouping can be done by multiple columns at the same time. Provide the column names as a list to the groupby() method.

COUNT NUMBER OF RECORDS BY CATEGORY
What is the number of passengers in each of the cabin classes?
titanic.Pclass.value_counts()
The function is a shortcut, as it is actually a groupby operation in combination with counting of the number of records within each group:
titanic.groupby('Pclass')['Pclass'].count()

Why one is faster?
%timeit titanic.Pclass.value_counts() # 389 ¬µs ¬± 44.5 ¬µs per loop
%timeit titanic.groupby('Pclass')['Pclass'].count() # 530 ¬µs ¬± 38.6 ¬µs per loop

Note: Both size and count can be used in combination with groupby.Where as size includes NaN values and just provides the number of rows (size of the table), count excludes the missing values. In the value_counts method, use the dropna argument to include or exclude the NaN value

REMEMBER
* Aggregation statistics can be calculated on entire columns or rows
* groupby provides the power of the split-apply-combine pattern
* value_counts is a convenient shortcut to count the number of entries in each category of a variable.  

HOW TO RESHAPE THE LAYOUT OF TABLES?
------------------------------------
This tutorial uses air quality data about ùëÅùëÇ2 and Particulate matter less than 2.5 micrometers, made available by openaq and using the py-openaq package. The air_quality_long.csv data set provides ùëÅ ùëÇ2 and ùëÉ ùëÄ25 values for the measurement stations FR04014, BETR801 and London Westminster in respectively Paris, Antwerp and London.
The air-quality data set has the following columns:
‚Ä¢ city: city where the sensor is used, either Paris, Antwerp or London
‚Ä¢ country: country where the sensor is used, either FR, BE or GB
‚Ä¢ location: the id of the sensor, either FR04014, BETR801 or London Westminster
‚Ä¢ parameter: the parameter measured by the sensor, either ùëÅ ùëÇ2 or Particulate matter ‚Ä¢ value: the measured value
‚Ä¢ unit: the unit of the measured parameter, in this case ‚ÄòŒºg/m3‚Äô
and the index of the DataFrame is datetime, the datetime of the measurement.

Note: The air-quality data is provided in a so-called long format data representation with each observation on a separate row and each variable a separate column of the data table. The long/narrow format is also known as the tidy data format.

tidy data - https://www.jstatsoft.org/article/view/v059i10
(A huge amount of effort is spent cleaning data to get it ready for analysis, but there has been little research on how to make data cleaning as easy and effective as possible. This paper tackles a small, but important, component of data cleaning: data tidying. Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets. This structure also makes it easier to develop tidy tools for data analysis, tools that both input and output tidy datasets. The advantages of a consistent data structure and matching tools are demonstrated with a case study free from mundane data manipulation chores.)

air_quality = pd.read_csv('data/air_quality_long.csv', index_col='date.utc', parse_dates=True) # date.utc is a column in dataset
air_quality.head()

How to reshape the layout of tables? 

SORT TABLE ROWS

I want to sort the Titanic data according to the age of the passengers.
titanic.sort_values(by='Age').head() ## KDB equivalent - `age xasc select from ('',(,)",")0:`titanic.csv

I want to sort the Titanic data according to the cabin class and age in descending order.
titanic.sort_values(by=['Pclass','Age'], ascending=False)

With Series.sort_values(), the rows in the table are sorted according to the defined column(s). The index will follow the row order.

LONG TO WIDE TABLE FORMAT
-------------------------
Let‚Äôs use a small subset of the air quality data set. We focus on ùëÅ ùëÇ2 data and only use the first two measurements of each location (i.e. the head of each group). The subset of data will be called no2_subset

 # filter for no2 data only
no2 = air_quality[air_quality['parameter'] == 'no2']
no2.head()

# use 2 measurements (head) for each location (groupby)
no2_subset = no2.sort_index().groupby(['location']).head(2)
no2_subset

I want the values for the three stations(location) as separate columns next to each other
no2_subset.pivot(columns='location', values='value') ## OR
no2_subset.pivot(columns='location')['value']

Which one is faster?
%timeit no2_subset.pivot(columns='location', values='value') # 2.34 ms ¬± 306 ¬µs per loop
%timeit no2_subset.pivot(columns='location')['value'] # 5.6 ms ¬± 204 ¬µs per loop

The pivot() function is purely reshaping of the data: a single value for each index/column combination is required.

As pandas support plotting of multiple columns out of the box, the conversion from long to wide table format enables the plotting of the different time series at the same time.
no2 = air_quality[air_quality['parameter'] == 'no2']
no2.head()
no2.pivot(columns='location', values='value').plot(figsize = (12, 6))

Note: When the index parameter is not defined, the existing index (rowlabels) is used.

PIVOT TABLE
I want the mean concentrations for ùëÅùëÇ2 and ùëÉùëÄ2.5 in each of the stations(locations) in table form.
air_quality = pd.read_csv('data/air_quality_long.csv', index_col='date.utc', parse_dates=True) # date.utc is a column in dataset

air_quality.pivot_table(columns='parameter', values='value', index='location', aggfunc='mean')

Difference between pivot() and pivot_table()?
In the case of pivot(), the data is only rearranged. When multiple values need to be aggregated (in this specific case, the values on different time steps) pivot_table() can be used, providing an aggregation function (e.g. mean) on how to combine these values.

Pivot table is a well known concept in spreadsheet software. When interested in summary columns for each variable separately as well, put the margin parameter to True:
air_quality.pivot_table(columns='parameter', values='value', 
                        index='location', aggfunc='mean', margins=True,)

Note: In case you are wondering, pivot_table() is indeed directly linked to groupby().The same result can be derived by grouping on both parameter and location:
air_quality.groupby(['parameter', 'location']).mean()

WIDE TO LONG FORMAT
no2_pivoted = no2.pivot(columns="location", values='value').reset_index()
no2_pivoted.head()

I want to collect all air quality ùëÅùëÇ2 measurements in a single column (long format)
no2_pivoted.melt(id_vars='date.utc')

The pandas.melt() method on a DataFrame converts the data table from wide format to long format. The column headers become the variable names in a newly created column.

The solution is the short version on how to apply pandas.melt(). The method will melt all columns NOT mentioned in id_vars together into two columns: A column with the column header names and a column with the values itself. The latter column gets by default the name value.

The pandas.melt() method can be defined in more detail
no2 = no2_pivoted.melt( id_vars='date.utc', value_vars=['BETR801','FR04014','London Westminster',], value_name='NO2', var_name='id_location', )
no2.head()

The result in the same, but in more detail defined:
‚Ä¢ value_vars  - defines explicitly which columns to melt together
‚Ä¢ value_name  - provides a custom column name for the values column instead of the default column name value
‚Ä¢ var_name  - provides a custom column name for the column collecting the column header names. Otherwise it takes the index name or a default variable

Hence, the arguments value_name and var_name are just user-defined names for the two generated columns. The columns to melt are defined by id_vars and value_vars.

REMEMBER
* Sorting by one or more columns is supported by sort_values
* The pivot function is purely restructuring of the data, pivot_table supports aggregations
* The reverse of pivot (long to wide format) is melt (wide to long format)

HOW TO COMBINE DATA FROM MULTIPLE TABLES?
------------------------------------------
DataSets:
air_quality_no2 = pd.read_csv("data/air_quality_no2_long.csv", parse_dates=True)
air_quality_no2.head()
air_quality_no2 = air_quality_no2[[ 'date.utc', 'location',
                                  'parameter', 'value', ]]
air_quality_no2.head()

The air_quality_pm25_long.csv data set provides ùëÉùëÄ25 values for the measurement stations FR04014, BETR801 and London Westminster in respectively Paris, Antwerp and London.
air_quality_pm25 = pd.read_csv("data/air_quality_pm25_long.csv",parse_dates=True)
air_quality_pm25.head()
air_quality_pm25 = air_quality_pm25[[ 'date.utc', 'location',
                                  'parameter', 'value', ]]
air_quality_pm25.head()

How to combine data from multiple tables? 
CONCATENATING OBJECTS
I want to combine the measurements of ùëÅùëÇ2 and ùëÉùëÄ25, two tables with a similar structure, in a single table

air_quality = pd.concat([air_quality_pm25, air_quality_no2], axis=0)
air_quality

The concat() function performs concatenation operations of multiple tables along one of the axis (row-wise or column-wise).
By default concatenation is along axis 0, so the resulting table combines the rows of the input tables. Let‚Äôs check the shape of the original and the concatenated tables to verify the operation:

print('Shape of the ``air_quality_pm25`` table', air_quality_pm25.shape)
print('Shape of the ``air_quality_no2`` table', air_quality_no2.shape)
print('Shape of resulting ``air_quality`` table', air_quality.shape)

Shape of the ``air_quality_pm25`` table (1110, 4)
Shape of the ``air_quality_no2`` table (2068, 4)
Shape of resulting ``air_quality`` table (3178, 4)

Hence, the resulting table has 3178 = 1110 + 2068 rows

Note: The axis argument will return in a number of pandas methods that can be applied along an axis. A DataFrame has two corresponding axes: the first running vertically downwards across rows (axis 0), and the second running horizontally across columns (axis 1). Most operations like concatenation or summary statistics are by default across rows (axis 0), but can be applied across columns as well.

Sorting the table on the datetime information illustrates also the combination of both tables, with the parameter column defining the origin of the table (either no2 from table air_quality_no2 or pm25 from table air_quality_pm25):

air_quality.sort_values('date.utc')

In this specific example, the parameter column provided by the data ensures that each of the original tables can be identified. This is not always the case. the concat function provides a convenient solution with the keys argument, adding an additional (hierarchical) row index. For example:

air_quality_ = pd.concat([air_quality_pm25, air_quality_no2], keys=["PM25", 'NO2',])
air_quality_

Note: The existence of multiple row/column indices at the same time has not been mentioned within these tutorials. Hierarchical indexing or MultiIndex is an advanced and powerful pandas feature to analyze higher dimensional data.
Multi-indexing is out of scope for this pandas introduction. For the moment, remember that the func- tion reset_index can be used to convert any level of an index to a column, e.g. air_quality. reset_index(level=0)

air_quality_ = pd.concat([air_quality_pm25, air_quality_no2], keys=["PM25", 'NO2',])
air_quality_
air_quality_.reset_index(level=0)

JOIN TABLES USING A COMMON IDENTIFIER

We have to Add the station coordinates, provided by the stations metadata table, to the corresponding rows in the measurements table.

Note: The stations used in this example (FR04014,BETR801 and LondonWestminster)are just three entries enlisted in the metadata table. We only want to add the coordinates of these three to the measurements table, each on the corresponding rows of the air_quality table.
stations_coord = pd.read_csv("data/air_quality_stations.csv")
stations_coord.head()

air_quality.head()

air_quality = pd.merge(air_quality, stations_coord, how='left', on='location')
air_quality.head()

Using the merge() function, for each of the rows in the air_quality table, the corresponding coordinates are added from the air_quality_stations_coord table. Both tables have the column location in common which is used as a key to combine the information. By choosing the left join, only the locations available in the air_quality (left) table, i.e. FR04014, BETR801 and London Westminster, end up in the resulting table. The merge function supports multiple join options similar to database-style operations.

I want to Add the parameter full description and name, provided by the parameters metadata table, to the measurements table

air_quality_parameters = pd.read_csv("data/air_quality_parameters.csv")
air_quality_parameters.head()

air_quality.head()

air_quality = pd.merge(air_quality, air_quality_parameters, how='left',
                      left_on='parameter', right_on='id')
air_quality.head()

Compared to the previous example, there is no common column name. However, the parameter column in the air_quality table and the id column in the air_quality_parameters_name both provide the measured variable in a common format. The left_on and right_on arguments are used here (instead of just on) to make the link between the two tables.
pandas supports also inner, outer, and right joins.

REMEMBER
* Multiple tables can be concatenated both column-wise and row-wise using the concat function.
* For database-like merging/joining of tables, use the merge function.

HOW TO HANDLE TIME SERIES DATA WITH EASE?
=========================================
For this tutorial, air quality data about ùëÅùëÇ2 and Particulate matter less than 2.5 micrometers is used, made available by openaq and downloaded using the py-openaq package. The air_quality_no2_long.csv" data set provides ùëÅ ùëÇ2 values for the measurement stations FR04014, BETR801 and London Westminster in respectively Paris, Antwerp and London.

air_quality = pd.read_csv("data/air_quality_no2_long.csv")
air_quality.rename(columns = {'date.utc': 'datetime'})
air_quality.head()
air_quality.city.unique()

air_quality.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 2068 entries, 0 to 2067
Data columns (total 7 columns):
 #   Column     Non-Null Count  Dtype  
---  ------     --------------  -----  
 0   city       2068 non-null   object 
 1   country    2068 non-null   object 
 2   datetime   2068 non-null   object

Currently, datetime is object type.

Using pandas datetime properties
---------------------------------
I want to work with the dates in the column datetime as datetime objects instead of plain text.

air_quality['datetime'] = pd.to_datetime(air_quality.datetime)
air_quality.datetime.dtype # datetime64[ns, UTC]

Initially, the values in datetime are character strings and do not provide any datetime operations (e.g. extract the year, day of the week,. . . ). By applying the to_datetime function, pandas interprets the strings and convert these to datetime (i.e. datetime64[ns, UTC]) objects. In pandas we call these datetime objects similar to datetime. datetime from the standard library as pandas.Timestamp.

Note: Asmanydatasetsdocontaindatetimeinformationinoneofthecolumns,pandasinputfunctionlikepandas. read_csv() and pandas.read_json() can do the transformation to dates when reading the data using the parse_dates parameter with a list of the columns to read as Timestamp:
pd.read_csv("../data/air_quality_no2_long.csv", parse_dates=["datetime"])

Why are these pandas.Timestamp objects useful? 
Let‚Äôs illustrate the added value with some example cases. What is the start and end date of the time series data set we are working with?
air_quality.datetime.min(), air_quality.datetime.max()
(Timestamp('2019-05-07 01:00:00+0000', tz='UTC'),
 Timestamp('2019-06-21 00:00:00+0000', tz='UTC'))

 Using pandas.Timestamp for datetimes enables us to calculate with date information and make them comparable. Hence, we can use this to get the length of our time series:
 air_quality.datetime.max() -  air_quality.datetime.min()
 # Timedelta('44 days 23:00:00')

 The result is a pandas.Timedelta object, similar to datetime.timedelta from the standard Python library and defining a time duration.

 I want to add a new column to the DataFrame containing only the month of the measurement
air_quality['month'] = air_quality.datetime.dt.month
air_quality.head()

By using Timestamp objects for dates, a lot of time-related properties are provided by pandas. For example the month, but also year, weekofyear, quarter,. . . All of these properties are accessible by the dt accessor.

What is the average ùëÅùëÇ2 concentration for each day of the week for each of the measurement locations?
air_quality.groupby([air_quality.datetime.dt.weekday, 'location'])['value'].mean()

Here, we want to calculate a given statistic (e.g. mean ùëÅùëÇ2) for each weekday and for each measurement location. To group on weekdays, we use the datetime property weekday (with Monday=0 and Sunday=6) of pandas Timestamp, which is also accessible by the dt accessor. The grouping on both locations and weekdays can be done to split the calculation of the mean on each of these combinations.

Plot the typical ùëÅùëÇ2 pattern during the day of our time series of all stations together. In other words, what is the average value for each hour of the day?

fig, axs = plt.subplots(figsize=(12,4))
air_quality.groupby(air_quality.datetime.dt.hour)['value'].mean().plot(
    kind='bar', rot=270, ax=axs)
plt.xlabel('Hour of the day')
plt.ylabel('$NO_2 (Œºg/m^3)$')
fig.savefig('hourlyPollution.png')

Similar to the previous case, we want to calculate a given statistic (e.g. mean ùëÅùëÇ2) for each hour of the day and we can use the split-apply-combine approach again. For this case, we use the datetime property hour of pandas Timestamp, which is also accessible by the dt accessor.

Datetime as index
------------------
In the tutorial on reshaping, pivot() was introduced to reshape the data table with each of the measurements locations as a separate column:
no_2 = air_quality.pivot(index='datetime', columns='location', values='value')
no_2.head()

Note: By pivoting the data, the datetime information became the index of the table. In general, setting a column as an index can be achieved by the set_index function.

Working with a datetime index (i.e. DatetimeIndex) provides powerful functionalities. For example, we do not need the dt accessor to get the time series properties, but have these properties available on the index directly:
no_2.index.year, no_2.index.month_name

Some other advantages are the convenient subsetting of time period or the adapted time scale on plots. Let‚Äôs apply this on our data.

I want to Create a plot of the ùëÅùëÇ2 values in the different stations from the 20th of May till the end of 21st of May
no_2['2019-05-20':'2019-05-21'].plot(figsize=(12,6))

By providing a string that parses to a datetime, a specific subset of the data can be selected on a DatetimeIndex.

RESAMPLE A TIME SERIES TO ANOTHER FREQUENCY
--------------------------------------------
Aggregate the current hourly(currently the dataset consists of hourly values) time series values to the monthly maximum value in each of the stations.

monthly_max = no_2.resample('M').max()
monthly_max

A very powerful method on time series data with a datetime index, is the ability to resample() time series to another frequency (e.g., converting secondly data into 5-minutely data).
The resample() method is similar to a groupby operation:
‚Ä¢ it provides a time-based grouping, by using a string (e.g. M, 5H,. . . ) that defines the target frequency 
‚Ä¢ it requires an aggregation function such as mean, max,. . .

no_2.resample('6H').mean().plot(figsize=(12,6))

An overview of the aliases used to define time series frequencies is given in the offset aliases overview table. When defined, the frequency of the time series is provided by the freq attribute:

monthly_max.index.freq # <MonthEnd>
monthly_max.index.freqstr # 'M'

I want to Make a plot of the daily mean ùëÅùëÇ2 value in each of the stations.
no_2.resample('D').mean().plot(style='-o', figsize=(12,6))

REMEMBER
* Valid date strings can be converted to datetime objects using to_datetime function or as part of read functions.
* Datetime objects in pandas support calculations, logical operations and convenient date-related properties using the dt accessor.
* A DatetimeIndex contains these date-related properties and supports convenient slicing.
* Resample is a powerful method to change the frequency of a time series.

HOW TO MANIPULATE TEXTUAL DATA?
This tutorial uses the Titanic data set, stored as CSV.
titanic = pd.read_csv("data/titanic.csv")

Make all name characters lowercase.
To make each of the strings in the Name column lowercase, select the Name column, add the str accessor and apply the lower method. As such, each of the strings is converted element-wise.

titanic.Name.str.lower()

Create a new column Surname that contains the surname of the passengers by extracting the part before the comma.

titanic.Name.str.split(',')

Using the Series.str.split() method, each of the values is returned as a list of 2 elements. The first element is the part before the comma and the second element is the part after the comma.

titanic['Surname'] = titanic.Name.str.split(',').str.get(0)
titanic.head()

As we are only interested in the first part representing the surname (element 0), we can again use the str accessor and apply Series.str.get() to extract the relevant part. Indeed, these string functions can be concatenated to combine multiple functions at once!

I want to Extract the passenger data about the countesses on board of the Titanic.

titanic[titanic.Name.str.contains('Countess')]

The string method Series.str.contains() checks for each of the values in the column Name if the string contains the word Countess and returns for each of the values True (Countess is part of the name) or False (Countess is not part of the name). This output can be used to subselect the data using conditional (boolean) indexing introduced in the subsetting of data tutorial. As there was only one countess on the Titanic, we get one row as a result.

Note: More powerful extractions on strings are supported, as the Series.str.contains() and Series. str.extract() methods accept regular expressions,

Which passenger of the Titanic has the longest name?
To get the longest name we first have to get the lengths of each of the names in the Name column. By using pandas string methods, the Series.str.len() function is applied to each of the names individually (element-wise).

titanic.Name.str.len().idxmax()

Next, we need to get the corresponding location, preferably the index label, in the table for which the name length is the largest. The idxmax() method does exactly that. It is not a string method and is applied to integers, so no str is used.

titanic.iloc[titanic.Name.str.len().idxmax()]['Name']
titanic.loc[titanic.Name.str.len().idxmax()]['Name']

Based on the index name of the row (307) and the column (Name), we can do a selection using the loc or iloc operator.

In the ‚ÄúSex‚Äù column, replace values of ‚Äúmale‚Äù by ‚ÄúM‚Äù and values of ‚Äúfemale‚Äù by ‚ÄúF‚Äù.

titanic['Sex_short'] = titanic.Sex.replace({'male':'M', 'female':'F'})
titanic.head(2)

Whereas replace() is not a string method, it provides a convenient way to use mappings or vocabularies to translate certainvalues.Itrequiresadictionarytodefinethemapping{from : to}.

Warning: There is also a replace() method available to replace a specific set of characters. However, when having a mapping of multiple values, this would become:
titanic["Sex_short"] = titanic["Sex"].str.replace("female", "F")
titanic["Sex_short"] = titanic["Sex_short"].str.replace("male", "M")
This would become cumbersome and easily lead to mistakes. Just think (or try out yourself) what would happen if those two statements are applied in the opposite order.

REMEMBER
* String methods are available using the str accessor.
* String methods work element-wise and can be used for conditional indexing.
* The replace method is a convenient method to convert values according to a given dictionary.


=================================Out of the box================================
||Accessors||
=============
Pandas is a widely-used data analysis and manipulation library in Python. It provides numerous functions and methods to work with any type of data. There are also methods that work only with a specific data type. These methods are accessed through 4 accessors.
The accessors extend the capabilities of Pandas and provide specific operations. For instance, extracting the month from the date can be done using the dt accessor.
In this post, we will see various operations with 4 accessors of Pandas which are:
str: String data type
    Eg: Get only the row which have alphabets from a series.
            a = pd.Series(['a',10,'bcd1',20,'efs'])
            a[a.str.isalpha().replace(np.nan, False)]

cat: Categorical data type
    For categorical data it is more efficient to work with categorical datatype than using the object datatype. It makes a significant difference in terms of memory and speed especially when the data has low cardinality(i.e number of categories is low compared to the number of observations)

    cate = pd.Series(['A','B','A','A','B','C'], dtype='category')
    cate.cat.categories # Index(['A', 'B', 'C'], dtype='object')
    cate.cat.rename_categories({'A':1, 'B':2, 'C':3})
    #cate[0]='D' # ValueError: Cannot setitem on a Categorical with a new category, set the categories first
    cate.cat.add_categories('D', inplace=True)
    cate[0]='D'
    cate

dt: Datetime, Timedelta, Period data types
    dts = pd.Series(pd.date_range('2021.01.01', periods=5, freq='10D'))
        dts.dt.day
        dts.dt.year
        dts.dt.month
        dts.dt.date
        dts.dt.hour
        dts.dt.minute
        dts.dt.second
        dts.dt.weekday
        dts.dt.isocalendar().week
        dts.dt.is_month_start

sparse: Sparse data type

=====================================================================================

CHAPTER 2 - USER GUIDE
------------------------
2.1.1 Object creation

Creating a Series by passing a list of values, letting pandas create a default integer index:
s = pd.Series([1,3,5,np.nan,6,8])

pd.date_range() - Return a fixed frequency DatetimeIndex.
    start=None, end=None, periods=None, freq=None, tz=None, normalize=False, name=None, closed=None, **kwarg

Creating a DataFrame by passing a NumPy array, with a datetime index and labeled columns:
dates = pd.date_range("20210101", periods=6, freq='2D')
df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list("ABCD"))
df

df2 = pd.DataFrame({
    'A':1.0,
    'B':pd.Timestamp('20210102'),
    'C':pd.Series(1, index=list(range(4)), dtype='float32'),
    'D':np.array([3]*4, dtype='int32'),
    'E':pd.Categorical(['test', 'train', 'test', 'train', ]),
    'F':'foo',
})
df2.info()

The columns of the resulting DataFrame have different dtypes.
df2.dtypes


DataFrame.to_numpy() gives a NumPy representation of the underlying data. Note that this can be an expensive operation when your DataFrame has columns with different data types, which comes down to a fundamental differ- ence between pandas and NumPy: NumPy arrays have one dtype for the entire array, while pandas DataFrames have one dtype per column. When you call DataFrame.to_numpy(), pandas will find the NumPy dtype that can hold all of the dtypes in the DataFrame. This may end up being object, which requires casting every value to a Python object.

For df, our DataFrame of all floating-point values, DataFrame.to_numpy() is fast and doesn‚Äôt require copying data.
df.to_numpy()

For df2, the DataFrame with multiple dtypes, DataFrame.to_numpy() is relatively expensive.
df2.to_numpy()
array([[1.0, Timestamp('2021-01-02 00:00:00'), 1.0, 3, 'test', 'foo'],
       [1.0, Timestamp('2021-01-02 00:00:00'), 1.0, 3, 'train', 'foo'],
       [1.0, Timestamp('2021-01-02 00:00:00'), 1.0, 3, 'test', 'foo'],
       [1.0, Timestamp('2021-01-02 00:00:00'), 1.0, 3, 'train', 'foo']],
      dtype=object)

Note: DataFrame.to_numpy()does not include the index orc olumn labels in the output.

2.1.2 Viewing data
-------------------
Transposing your data:
df.T

Sorting by an axis:
df.sort_index(axis=1, ascending=False)

                D	        C	        B	        A
2021-01-01	-0.030514	-0.875529	-0.055480	-0.503135
2021-01-03	0.122823	-0.488960	-1.026299	-0.982295

df.sort_index(ascending=False)

                A	        B	        C	        D
2021-01-11	-0.537115	0.030939	-0.311905	0.598476
2021-01-09	-0.873883	0.678836	-0.255343	-0.826411

Sorting by values:
df.sort_values('B')

2.1.3 Selection
----------------
Note: While standard Python / Numpy expressions for selecting and setting are intuitive and come in handy for interactive work, for production code, we recommend the optimized pandas data access methods, .at, .iat, .loc and .iloc.

Getting
  Selecting a single column, which yields a Series, equivalent to df.A:
  df['A']

 Selecting via [], which slices the rows.
  df[0:3]
  df["20210102":"20210106"]

 Selection by label
 ------------------
 For getting a cross section using a label:
 df.loc[dates[0]]

Selecting on a multi-axis by label:
df[['A','B']] ~ df.loc[:,['A','B']]

Showing label slicing, both endpoints are included:
df.loc['20210101':'20210105',['A','C']]

Reduction in the dimensions of the returned object:
df.loc['2021-01-03',['A','C']]

For getting a scalar value
df.loc[dates[0], 'A'] # -0.5031

For getting fast access to a scalar (equivalent to the prior method):
df.at[dates[0], 'A'] # -0.5031

which one is faster?
%timeit df.loc[dates[0], 'A'] # 30.3 ¬µs ¬± 1.27
%timeit df.at[dates[0], 'A'] # 23.4 ¬µs ¬± 5.19 ¬µs

Selection by position(index)
-----------------------------
Select via the position of the passed integers:
df.iloc[3]

By integer slices, acting similar to numpy/Python:
df.iloc[2:5, 2:4]

By lists of integer position locations, similar to the NumPy/Python style:
df.iloc[[1,2,4], [0,2]]

For slicing rows explicitly:
df.iloc[2:5,]

For slicing columns explicitly:
df.iloc[:, 2:4]

For getting a value explicitly:
df.iloc[2,2] # 0.79

For getting fast access to a scalar (equivalent to the prior method):
df.iat[2,2] # 0.790

Boolean indexing
-----------------
Using a single column‚Äôs values to select data.
df[df['A']>0]

Selecting values from a DataFrame where a boolean condition is met.
df[df>0]

Using the isin() method for filtering:
df2 = df.copy()
df2['E'] = [ 'one', 'one', 'two', 'three', 'four', 'three', ]
df2[df2.E.isin(['two', 'four'])]

Setting
-------
Setting a new column automatically aligns the data by the indexes.
s1 = pd.Series(np.arange(1,7), index=pd.date_range('20210101', periods=6))
s1

Setting values by label:
df.at[dates[0], 'A'] = 0

Setting values by position:
df.iat[0, 1] = 0

Setting by assigning with a NumPy array:
df['D'] = np.array([3] * len(df))

A where operation with setting.
df2 = df.copy()
df2[df2>0] = -df2

df2 = df.copy()
df2 = df2.where(df2<0,-df2)

2.1.4 Missing data
-------------------
pandas primarily uses the value np.nan to represent missing data. It is by default not included in computations.

Reindexing allows you to change/add/delete the index on a specified axis. This returns a copy of the data.

reindex - assign new index to existing dataframe.

df1 = df.reindex(index=dates[0:4], columns=list(df.columns)+['E'])
df1.loc[dates[0:2], 'E'] = 1

To drop any rows that have missing data.
df.dropna(how = 'any') ~ df.dropna()

which one is faster?
%timeit df.dropna() # 1.59 ms ¬± 13.5 ¬µs per loop
%timeit df.dropna(how = 'any') # 1.67 ms ¬± 50.9 ¬µs per loop

%timeit titanic.dropna() # 1.97 ms ¬± 117 ¬µs per loop
%timeit titanic.dropna(how='any') # 2.2 ms ¬± 196 ¬µs per loop

Filling missing data.
df1.fillna(value=5) ~ df1.fillna(5)

To get the boolean mask where values are nan.
pd.isna(df1)

2.1.5 Operations
----------------
Stats
Operations in general exclude missing data.
df.mean() # To get mean of each column

Same operation on the other axis:
df.mean(axis=1) ~ df.mean(1) # To get mean of each row
df.mean.mean() # To get a scaler which is mean of complete dataframe

Operating with objects that have different dimensionality and need alignment. In addition, pandas automatically broadcasts along the specified dimension.

s = pd.Series([1,3,5,np.nan,6,8], index=dates).shift(2)
df.sub(s, axis='index')
df.rsub(s, axis='index')

Apply
------
Applying functions to the data:
df.apply(np.cumprod)

df.apply(lambda x: x.max() - x.min())
df.apply(lambda x: x.max() - x.min(), axis=1)

Histogramming:
get the frequency.

s = pd.Series(np.random.randint(0,7,size=10))
s.value_counts()

String Methods
--------------
Series is equipped with a set of string processing methods in the str accessor that make it easy to operate on each element of the array, as in the code snippet below. Note that pattern-matching in str generally uses regular expressions by default (and in some cases always uses them).

s = pd.Series(['A','B','V','CABA',np.nan, 'RAVA'])
s.str.lower()

2.1.6 Merge
------------
Concat
------
pandas provides various facilities for easily combining together Series and DataFrame objects with various kinds of set logic for the indexes and relational algebra functionality in the case of join / merge-type operations.
Concatenating pandas objects together with concat():

df = pd.DataFrame(np.random.randn(10,4))
# let's break it into pieces
pieces = [df[:3], df[3:7], df[7:]]
pd.concat(pieces)

Note: Adding a column to a DataFrame is relatively fast. However, adding a row requires a copy, and may be expensive. We recommend passing a pre-built list of records to the DataFrame constructor instead of building a DataFrame by iteratively appending records to it.

Join
-----
SQL style merges
left = pd.DataFrame({"key": ["foo", "foo"], "lval": [1, 2]})
right = pd.DataFrame({"key": ["foo", "foo"], "rval": [4, 5]})
pd.merge(left, right, on="key")

2.1.7 Grouping
--------------
By ‚Äúgroup by‚Äù we are referring to a process involving one or more of the following steps: ‚Ä¢ Splitting the data into groups based on some criteria
‚Ä¢ Applying a function to each group independently
‚Ä¢ Combining the results into a data structure

df = pd.DataFrame(
    {
    "A": ["foo", "bar", "foo", "bar", "foo", "bar", "foo", "foo"],
    "B": ["one", "one", "two", "three", "two", "two", "one", "three"],
    "C": np.random.randn(8),
    "D": np.random.randn(8),
    }
)
Grouping and then applying the sum() function to the resulting groups.
df.groupby(["A", "B"]).sum()

2.1.8 Reshaping
---------------
tuples = list(zip(*[
    ["bar", "bar", "baz", "baz", "foo", "foo", "qux", "qux"],
    ["one", "two", "one", "two", "one", "two", "one", "two"],
]))

index = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])
df = pd.DataFrame(np.random.randn(8,2), index=index, columns=['A','B'])
df2 = df[:4]
df2

The stack() method ‚Äúcompresses‚Äù a level in the DataFrame‚Äôs columns.
stacked = df2.stack()
stacked
(It is similar to melt but it maintains index as well)
first  second   
bar    one     A    0.709423
               B   -0.276359
       two     A    0.771991
               B   -0.523291
baz    one     A    0.033710
               B   -0.099103
       two     A   -0.046292
               B    0.984227
dtype: float64

With a ‚Äústacked‚Äù DataFrame or Series (having a MultiIndex as the index), the inverse operation of stack() is unstack(), which by default unstacks the last level:

stacked.unstack()
                		A	       B
first	second		
bar	    one	        0.709423	-0.276359
        two	        0.771991	-0.523291
baz	    one	        0.033710	-0.099103
        two	       -0.046292	0.984227

stacked.unstack([0,1])
first	bar	baz
second	one	two	one	two
A	0.709423	0.771991	0.033710	-0.046292
B	-0.276359	-0.523291	-0.099103	0.984227

Pivot tables
------------
Reshape - Long to Wide

df = pd.DataFrame({
    "A": ["one", "one", "two", "three"] * 3,
    "B": ["A", "B", "C"] * 4,
    "C": ["foo", "foo", "foo", "bar", "bar", "bar"] * 2,
    "D": np.random.randn(12),
    "E": np.random.randn(12),
})
df.pivot_table(columns=['C'], values='D', index=['A','B'])

2.1.9 Time series
------------------
pandas has simple, powerful, and efficient functionality for performing resampling operations during frequency con- version (e.g., converting secondly data into 5-minutely data). This is extremely common in, but not limited to, financial applications.

rng = pd.date_range('2021/01/01', periods=100, freq='S')
ts = pd.Series(np.random.randint(0,500,len(rng)), index=rng)
ts.resample('5M').mean()

Time zone representation:
rng = pd.date_range('3/6/2020 00:00', periods=5)
ts = pd.Series(np.random.randn(len(rng)), index=rng)
ts_utc = ts.tz_localize('UTC')
Converting to another time zone:
ts_utc.tz_convert('US/Eastern')

Converting between time span representations:
rng = pd.date_range('3/6/2020 00:00', periods=5, freq='M')
ts = pd.Series(np.random.randn(len(rng)), index=rng)
ps = ts.to_period()
ps.to_timestamp()

Converting between period and timestamp enables some convenient arithmetic functions to be used. In the following example, we convert a quarterly frequency with year ending in November to 9am of the end of the month following the quarter end:

prng = pd.period_range('1990Q1', '2000Q4', freq='Q-NOV')
ts = pd.Series(np.random.randn(len(prng)), index=prng)
ts.index = (prng.asfreq('M','e')+1).asfreq('H','s')+9
ts

Get the first date od provided month.
pd.Timestamp('2020-01', freq='M').to_period().to_timestamp()

2.1.10 Categoricals
-------------------
pandas can include categorical data in a DataFrame.

df = pd.DataFrame({
    'id':np.arange(1,7),
    'raw_grade': [ 'a', 'b', 'b', 'a', 'a', 'e', ],
})

Convert the raw grades to a categorical data type.
df['grade']=df['raw_grade'].astype('category')

Rename the categories to more meaningful names (assigning to Series.cat.categories() is in place!).
df['grade'].cat.categories = ['vg', 'g', 'b']

Reorder the categories and simultaneously add the missing categories (methods under Series.cat() return a new Series by default).
df['grade'] = df['grade'].cat.set_categories(['vb','b','m','g','vg'])

Sorting is per order in the categories, not lexical order.
df.sort_values('grade')

Grouping by a categorical column also shows empty categories.
df.groupby('grade').count()

2.1.11 Plotting
----------------
We use the standard convention for referencing the matplotlib API:
ts = pd.Series(np.random.randn(1000), index=pd.date_range(
    '2019.01.01', periods=1000, ))
ts = ts.cumsum()
ts.plot()

On a DataFrame, the plot() method is a convenience to plot all of the columns with labels:
df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, 
                  columns=['A','B','C','D'])
df = df.cumsum()

plt.figure()
df.plot(figsize=(12,6))
plt.legend(loc='best')

2.1.12 Getting data in/out
CSV
Writing to a csv file. - df.to_csv("foo.csv")
Reading from a csv file. - pd.read_csv("foo.csv")

2.2 Intro to data structures
============================
We‚Äôll start with a quick, non-comprehensive overview of the fundamental data structures in pandas to get you started. The fundamental behavior about data types, indexing, and axis labeling / alignment apply across all of the objects.

Here is a basic tenet to keep in mind: DATA ALIGNMENT IS INTRINSIC. The link between labels and data will not be broken unless done so explicitly by you.

===========What is Vectorization ===========================
https://realpython.com/numpy-array-programming/#getting-into-shape-intro-to-numpy-arrays

This practice of replacing explicit loops with array expressions is commonly referred to as vectorization. In general, vectorized array operations will often be one or two (or more) orders of magnitude faster than their pure Python equivalents, with the biggest impact [seen] in any kind of numerical computations.

When looping over an array or any data structure in Python, there‚Äôs a lot of overhead involved. Vectorized operations in NumPy delegate the looping internally to highly optimized C and Fortran functions, making for cleaner and faster Python code.

consider a 1-dimensional vector of True and False for which you want to count the number of ‚ÄúFalse to True‚Äù transitions in the sequence:
In simple words we need to count the number of cases in which True comes after False.
Eg:
x = np.array([False, False, True, True,False])
np.count_nonzero(x[:-1]<x[1:]) # 1 - only once True comes after false at position 3

Classic Python:
def get_count(x) -> int:
    count = 0
    for i,j in list(zip(x[:-1], x[1:])):
        if j and not i:
            count += 1
    return count
%timeit get_count(x) # 1.1 ms ¬± 19.7 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1000 loops each)

numpy:
%timeit np.count_nonzero(x[:-1]<x[1:]) # 1.1 ms ¬± 19.7 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1000 loops each)

Numpy solution is around 157 times faster.
from timeit import timeit

setup = 'from __main__ import count_transitions, x; import numpy as np'
num = 100
t1 = timeit('count_transitions(x)', setup = setup, number=num)
print(t1)
t2 = timeit('np.count_nonzero(x[:-1]<x[1:])', setup = setup, number=num)
print(t2)
print(f'Speed difference: {(t1/t2)}')

17.00445253200087
0.10794881399851874
Speed difference: 157.52329184676546

Question: Given a stock‚Äôs price history as a sequence, and assuming that you are only allowed to make one purchase and one sale, what is the maximum profit that can be obtained? For example, given prices = (20, 18, 14, 17, 20, 21, 15), the max profit would be 7, from buying at 14 and selling at 21.

6 solutions:
p = np.array([20, 18, 14, 17, 20, 21, 15])

def bruteforce(a) -> int:
    max_diff = 0
    for i in p:
        for j in p[1:]:
            t = i - j
            if t > max_diff:
                max_diff = t
    return max_diff

def ordern(a) -> int:
    max_val  = min_val = p[0]
    for i in p[1:]:
        if i>max_val:
            max_val = i
        elif i< min_val:
            min_val = i
    return max_val - min_val

def realordern(p) -> int:
    diff_px = 0
    min_px = p[0]
    for px in p[1:]:
        min_px = min(min_px, px)
        diff_px = max(px-min_px, diff_px)
    return diff_px

def basic(p) -> int:
    return max(p)-min(p)

from timeit import timeit

setup = 'from __main__ import realordern, ordern, p, basic, bruteforce; import numpy as np'
num = 10000
t6 = timeit('np.max(p - np.minimum.accumulate(p))', setup = setup, number=num)
print('numpy accumulate', t6)
t5 = timeit('np.max(p) - np.min(p)', setup = setup, number=num)
print('numpy max min', t5)
t4 = timeit('basic(p)', setup = setup, number=num)
print('python max min', t4)
t1 = timeit('realordern(p)', setup = setup, number=num)
print('solution suggested by realPython', t1)
t2 = timeit('ordern(p)', setup = setup, number=num)
print('My if else solution', t2)
t3 = timeit('bruteforce(p)', setup = setup, number=num)
print('BruteForce way ', t3)
print(min(t1, t2, t3, t4, t5 ,t6))

numpy accumulate 0.13581958799841232
numpy max min 0.10029465600018739
python max min 0.03708621999976458
solution suggested by realPython 0.051151576000847854
My if else solution 0.02522610599771724
BruteForce way  0.21499461300118128
0.02522610599771724

AXES can be confusing term so in short - summing an array for axis=0 collapses the rows of the array with a column-wise computation.

df = DataFrame({
    'a':[10,20],
    'b':[100, 200]
})
print(df)
df.sum(axis=0)

    a    b
0  10  100
1  20  200
a     30
b    300

random.normal(loc=0.0, scale=1.0, size=None)
Draw random samples from a normal (Gaussian) distribution.
The probability density function of the normal distribution, first derived by De Moivre and 200 years later by both Gauss and Laplace independently [2], is often called the bell curve because of its characteristic shape (see the example below).
The normal distributions occurs often in nature. For example, it describes the commonly occurring distribution of samples influenced by a large number of tiny, random disturbances, each with its own unique distribution [2].

http://scipy-lectures.org/intro/numpy/array_object.html#what-are-numpy-and-numpy-arrays

Interactive help for numpy
np.array? --> Interactive help
------------
Docstring:
array(object, dtype=None, *, copy=True, order='K', subok=False, ndmin=0)
Create an array.

Looking for something:
np.lookfor('create a random array')
-----------------------------------
Search results for 'create a random array'
------------------------------------------
numpy.matlib.rand
    Return a matrix of random values with given shape.

np.con*?
-----------
np.concatenate
np.conj
np.conjugate
np.convolve
===================================================================================



2.2.1 Series
=============
Series is a one-dimensional labeled array capable of holding any data type (integers, strings, floating point numbers, Python objects, etc.). The axis labels are collectively referred to as the index. The basic method to create a Series is to call:

s = pd.Series(data, index=index)

Here, data can be many different things: ‚Ä¢ a Python dict
‚Ä¢ an ndarray
‚Ä¢ a scalar value (like 5)
The passed index is a list of axis labels. Thus, this separates into a few cases depending on what data is:

From ndarray
If data is an ndarray, index must be the same length as data. If no index is passed, one will be created having values [0, ..., len(data) - 1].

s = pd.Series(np.arange(5), index=['a','b','c','d','e'])
s.index

pd.Series(np.arange(5))

Note: pandas supports non-unique index values. If an operation that does not support duplicate index values is attempted, an exception will be raised at that time. The reason for being lazy is nearly all performance-based (there are many instances in computations, like parts of GroupBy, where the index is not used).

## DUPLICATE INDEX
s = pd.Series(np.arange(5), index=['a','b','a','d','e']) 

From dict
Series can be instantiated from dicts:
s = pd.Series({
    'b':1,
    'a':0,
    'c':2,
})

Note: When the data is a dict, and an index is not passed, the Series index will be ordered by the dict‚Äôs insertion order, if you‚Äôre using Python version >= 3.6 and pandas version >= 0.23.

pd.Series(s, index=['b','c','d','a'])

Note: NaN (not a number)is the standard missing data marker used in pandas.

From scalar value
If data is a scalar value, an index can be provided. The value will be repeated to match the length of index.
pd.Series(5)
pd.Series(5, index=['b','c','d','a'])

Series is ndarray-like
Series acts very similarly to a ndarray, and is a valid argument to most NumPy functions. However, operations such as slicing will also slice the index.

s[0] is s['a'] ## False
s[0] == s['a'] ## True

print(id(s[0])) # 4949691408
id(s['a']) # 4949692112

s[:3]

s[s>s.median()]

s[[4,3,1]]

np.exp(s)

Like a NumPy array, a pandas Series has a dtype.
s.dtype # dtype('float64')

This is often a NumPy dtype. However, pandas and 3rd-party libraries extend NumPy‚Äôs type system in a few places, in which case the dtype would be an ExtensionDtype. Some examples within pandas are Categorical data and Nullable integer data type. 

If you need the actual array backing a Series, use Series.array.
s.array
<PandasArray>
[   0.9662603314256619,  -0.10565599919230215, -0.011807389416360094,
   -0.3584626404849454,   -1.4087244659737785]
Length: 5, dtype: float64

Accessing the array can be useful when you need to do some operation without the index (to disable automatic alignment, for example).
type(s.array) # pandas.core.arrays.numpy_.PandasArray

Series.array will always be an ExtensionArray. Briefly, an ExtensionArray is a thin wrapper around one or more concrete arrays like a numpy.ndarray. pandas knows how to take an ExtensionArray and store it in a Series or a column of a DataFrame.

While Series is ndarray-like, if you need an actual ndarray, then use Series.to_numpy().
s.to_numpy()

Even if the Series is backed by a ExtensionArray, Series.to_numpy() will return a NumPy ndarray.

Series is dict-like
--------------------
A Series is like a fixed-size dict in that you can get and set values by index label:

s['a'] # 0.9662603314256619
'e' in s # True
'f' in s # False

If a label is not contained, an exception is raised:
s['f'] # KeyError: 'f'

Using the get method, a missing label will return None or specified default:
s.get('f')
s.get('f', 10) # 10
s.get('f', np.nan) nan

Vectorized operations and label alignment with Series
------------------------------------------------------
When working with raw NumPy arrays, looping through value-by-value is usually not necessary. The same is true when working with Series in pandas. Series can also be passed into most NumPy methods expecting an ndarray.

s+s
s * 2
np.exp(s)

A key difference between Series and ndarray is that operations between Series automatically align the data based on label. Thus, you can write computations without giving consideration to whether the Series involved have the same labels.

s1 = pd.Series(np.arange(5), index=['a','b','c','d','e'])
print(s1[:-1])
print(s1[1:])
s1[:-1] + s1[1:]
a    0
b    1
c    2
d    3
dtype: int64
b    1
c    2
d    3
e    4
dtype: int64
a    NaN
b    2.0
c    4.0
d    6.0
e    NaN
dtype: float64

a = np.arange(5)
print(a)
a[:-1] + a[1:]

[0 1 2 3 4]
array([1, 3, 5, 7])


The result of an operation between unaligned Series will have the union of the indexes involved. If a label is not found in one Series or the other, the result will be marked as missing NaN. Being able to write code without doing any explicit data alignment grants immense freedom and flexibility in interactive data analysis and research. The integrated data alignment features of the pandas data structures set pandas apart from the majority of related tools for working with labeled data.


Note: In general, we chose to make the default result of operations between differently indexed objects yield the union of the indexes in order to avoid loss of information. Having an index label, though the data is missing, is typically important information as part of a computation. You of course have the option of dropping labels with missing data via the dropna function.

Name attribute
---------------
Series can also have a name attribute:

s1 = pd.Series(np.arange(5), index=['a','b','c','d','e'], name='something')
print(s1)
print(s1.name) # something

The Series name will be assigned automatically in many cases, in particular when taking 1D slices of DataFrame as you will see below.
You can rename a Series with the pandas.Series.rename() method.

s2 = s1.rename('different')
s2.name # 'different'
print(id(s1)) # 4939539792
print(id(s2)) # 4939597952

Note that s1 and s2 refer to different objects.

You can also name an existing series with rename method.
s3 = pd.Series(np.arange(5), index=['a','b','c','d','e'],)
s3.rename('Ramesha') # Name: Ramesha, dtype: int64

2.2.2 DataFrame
----------------
DataFrame is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet or SQL table, or a dict of Series objects. It is generally the most commonly used pandas object. Like Series, DataFrame accepts many different kinds of input:
‚Ä¢ Dict of 1D ndarrays, lists, dicts, or Series 
‚Ä¢ 2-D numpy.ndarray
‚Ä¢ Structured or record ndarray
‚Ä¢ A Series
‚Ä¢ Another DataFrame

--------------------------------------------------------------------------------
Structured or record ndarray - 

Structured arrays are ndarrays whose datatype is a composition of simpler datatypes organized as a sequence of named fields. For example,

x = np.array([
    ('Rex', 9, 81.0,),
    ('Fido', 3, 27.0,),
], dtype=[('name', 'U10'), ('age','i4'), ('weight', 'f4')]
)
x.dtype

Here x is a one-dimensional array of length two whose datatype is a structure with three fields: 
1. A string of length 10 or less named ‚Äòname‚Äô, 
2. a 32-bit integer named ‚Äòage‚Äô, and 
3. a 32-bit float named ‚Äòweight‚Äô.

If you index x at position 1 you get a structure:
x[1] # ('Fido', 3, 27.)
x[1]['age'] # 3
You can access and modify individual fields of a structured array by indexing with the field name:
x[1]['age'] = 30
x[1] #  ('Fido', 30, 27.)

## Dataframe from structured array
df = DataFrame(x)
df

Structured datatypes are designed to be able to mimic ‚Äòstructs‚Äô in the C language, and share a similar memory layout. They are meant for interfacing with C code and for low-level manipulation of structured buffers, for example for interpreting binary blobs. For these purposes they support specialized features such as subarrays, nested datatypes, and unions, and allow control over the memory layout of the structure.

Users looking to manipulate tabular data, such as stored in csv files, may find other pydata projects more suitable, such as xarray, pandas, or DataArray. These provide a high-level interface for tabular data analysis and are better optimized for that use. For instance, the C-struct-like memory layout of structured arrays in numpy can lead to poor cache behavior in comparison
---------------------------------------------------------------------------------

back to dataframe - 
Along with the data, you can optionally pass index (row labels) and columns (column labels) arguments. If you pass an index and / or columns, you are guaranteeing the index and / or columns of the resulting DataFrame. Thus, a dict of Series plus a specific index will discard all data not matching up to the passed index.
If axis labels are not passed, they will be constructed from the input data based on common sense rules.

df = DataFrame({
    'a':[10,20],
    'b':[100, 200]
}, index=['chandu', 'maghu']
)
df

Note: When the data is a dict,and columns is not specified,the DataFrame columns will be ordered by the dict‚Äôs insertion order, if you are using Python version >= 3.6 and pandas >= 0.23.

If you want to create a dataframe with a dict but different names:
d = {
    'a':[10,20],
    'b':[100, 200]
}
df = DataFrame(list(d.values()), columns=['Ramesh', 'Suresh'])
df

From dict of Series or dicts
----------------------------
The resulting index will be the union of the indexes of the various Series. If there are any nested dicts, these will first be converted to Series. If no columns are passed, the columns will be the ordered list of dict keys.

d = {
    'one': pd.Series(np.arange(1,4).astype(float), index=['a','b','c']),
    'two': pd.Series(np.arange(1,5).astype(float), index=['a','b','c','d']),
}
df = pd.DataFrame(d)
df

df = pd.DataFrame(d, index=['d','b','a'])

df = pd.DataFrame(d, index=['d','b','a'], columns=['two', 'three'])

The row and column labels can be accessed respectively by accessing the index and columns attributes:

Note: When a particular set of columns is passed along with a dict of data, the passed columns override the keys in the dict.

df.index # Index(['d', 'b', 'a'], dtype='object')
df.columns # Index(['two', 'three'], dtype='object')

From dict of ndarrays / lists
-----------------------------
The ndarrays must all be the same length. If an index is passed, it must clearly also be the same length as the arrays. If no index is passed, the result will be range(n), where n is the array length.

d = {"one": [1.0, 2.0, 3.0, 4.0], "two": [4.0, 3.0, 2.0, 1.0]}
pd.DataFrame(d, index=["a", "b", "c", "d"])

From structured or record array
-------------------------------
This case is handled identically to a dict of arrays.
data = np.zeros((2,), dtype=[("A", "i4"), ("B", "f4"), ("C", "a10")])
data[:] = [(1, 2.0, "Hello"), (2, 3.0, "World")]
pd.DataFrame(data)
pd.DataFrame(data, index=['on', 'tw'])
pd.DataFrame(data, columns=list('cab'.upper()))

Note: DataFrame is not intended to work exactly like a 2-dimensional NumPy ndarray.

From a list of dicts
---------------------
data2 = [{"a": 1, "b": 2}, {"a": 5, "b": 10, "c": 20}]
pd.DataFrame(data2)
pd.DataFrame(data2, index=["first", "second"])
pd.DataFrame(data2, columns=["a", "b"])

From a dict of tuples
---------------------
You can automatically create a MultiIndexed frame by passing a tuples dictionary.
d = {
    ("a", "b"): {("A", "B"): 1, ("A", "C"): 2},
    ("a", "a"): {("A", "C"): 3, ("A", "B"): 4},
    ("a", "c"): {("A", "B"): 5, ("A", "C"): 6},
    ("b", "a"): {("A", "C"): 7, ("A", "B"): 8},
    ("b", "b"): {("A", "D"): 9, ("A", "B"): 10},
}
DataFrame(d)

From a Series
-------------
The result will be a DataFrame with the same index as the input Series, and with one column whose name is the original name of the Series (only if no other column name provided).

From a list of namedtuples
---------------------------
The field names of the first namedtuple in the list determine the columns of the DataFrame. The remaining namedtuples (or tuples) are simply unpacked and their values are fed into the rows of the DataFrame. If any of those tuples is shorter than the first namedtuple then the later columns in the corresponding row are marked as missing values. If any are longer than the first namedtuple, a ValueError is raised.

Point = namedtuple('Point', 'x y')
DataFrame([Point(0,0), Point(0,1), Point(0,2)])

Point = namedtuple('Point3D', 'x y z')
DataFrame([Point(0,0,0), Point(0,1,1), Point(0,1,2)])

From a list of dataclasses
--------------------------
Passing a list of dataclasses is equivalent to passing a list of dictionaries.
Please be aware, that all values in the list should be dataclasses, mixing types in the list would result in a TypeError.

from dataclasses import make_dataclass
Point = make_dataclass('Point', [('x', int), ('y', int)])
pd.DataFrame([Point(0, 0), Point(0,1), Point(0,2)])

Missing data
Much more will be said on this topic in the Missing data section. To construct a DataFrame with missing data, we use np.nan to represent missing values. Alternatively, you may pass a numpy.MaskedArray as the data argument to the DataFrame constructor, and its masked entries will be considered missing.

Alternate constructors
-----------------------
DataFrame.from_dict --
----------------------
DataFrame.from_dict takes a dict of dicts or a dict of array-like sequences and returns a DataFrame. It operates like the DataFrame constructor except for the orient parameter which is 'columns' by default, but which can be set to 'index' in order to use the dict keys as row labels
pd.DataFrame.from_dict(dict([('A', np.arange(1,4)), ('B', np.arange(4,7))]))

If you pass orient='index', the keys will be the row labels. In this case, you can also pass the desired column names:
pd.DataFrame.from_dict(
    dict([('A', np.arange(1,4)), ('B', np.arange(4,7))]),
    orient = 'index',
    columns=['one', 'two', 'three'])

DataFrame.from_records
------------------------
DataFrame.from_records takes a list of tuples or an ndarray with structured dtype. It works analogously to the normal DataFrame constructor, except that the resulting DataFrame index may be a specific field of the structured dtype. For example:
x = np.array([
    ('Rex', 9, 81.0,),
    ('Fido', 3, 27.0,),
], dtype=[('name', 'U10'), ('age','i4'), ('weight', 'f4')]
)

pd.DataFrame.from_records(x)

Column selection, addition, deletion
-------------------------------------
You can treat a DataFrame semantically like a dict of like-indexed Series objects. Getting, setting, and deleting columns works with the same syntax as the analogous dict operations:

d = {
    'one': pd.Series(np.arange(1,4).astype(float), index=['a','b','c']),
    'two': pd.Series(np.arange(1,5).astype(float), index=['a','b','c','d']),
}
df = pd.DataFrame(d)
df['one']   ## Selection
df['three'] = df['one'] * df['two'] ## Column Addition
df['flag'] = df['one'] > 2  ## Column Addition
df

Columns can be deleted or popped like with a dict:
del df['two']   ## permanently deleted
three = df.pop('three')  ## Poped and deleated

When inserting a scalar value, it will naturally be propagated to fill the column:
df['foo'] = 'bar'

When inserting a Series that does not have the same index as the DataFrame, it will be conformed to the DataFrame‚Äôs index:
df['one_trunc'] = df["one"][:2]

If the index of the series to be added does not match with df, then np.nan is filled.
s = pd.Series(np.arange(1,4), index=['z','y','x'])
df1 = df
print(df1)
df1['trunc_one'] = s
df1

one	flag	    foo	trunc_one
a	1.0	False	bar	NaN
b	2.0	False	bar	NaN
c	3.0	True	bar	NaN
d	NaN	False	bar	NaN

You can insert raw ndarrays but their length must match the length of the DataFrame‚Äôs index.
By default, columns get inserted at the end. The insert function is available to insert at a particular location in the columns:
df.insert(1, 'bar', df["one"])

Assigning new columns in method chains
--------------------------------------
assign - DataFrame has an assign() method that allows you to easily create new columns that are potentially derived from existing columns.
assign always returns a copy of the data, leaving the original DataFrame untouched.
iris = pd.read_csv("data/iris.data")
iris.head()
iris.assign(SepalRation = iris.SepalWidth / iris.SepalLength).head()

In the example above, we inserted a precomputed value. We can also pass in a function of one argument to be evaluated on the DataFrame being assigned to.
iris.assign(SepalRatio=lambda x:(x['SepalWidth']/x['SepalLength'])).head()

Passing a callable, as opposed to an actual value to be inserted, is useful when you don‚Äôt have a reference to the DataFrame at hand. This is common when using assign in a chain of operations. For example, we can limit the DataFrame to just those observations with a Sepal Length greater than 5, calculate the ratio, and plot:

iris.query("SepalLength > 5").assign(
    SepalRatio = iris.SepalWidth / iris.SepalLength,
    PetalRatio = lambda x: x.PetalWidth / x.PetalLength
).plot(kind="scatter", x="SepalRatio", y="PetalRatio")

Since a function is passed in, the function is computed on the DataFrame being assigned to. Importantly, this is the DataFrame that‚Äôs been filtered to those rows with sepal length greater than 5. The filtering happens first, and then the ratio calculations. This is an example where we didn‚Äôt have a reference to the filtered DataFrame available.

The function signature for assign is simply **kwargs. The keys are the column names for the new fields, and the values are either a value to be inserted (for example, a Series or NumPy array), or a function of one argument to be called on the DataFrame. A copy of the original DataFrame is returned, with the new values inserted.
Starting with Python 3.6 the order of **kwargs is preserved. This allows for dependent assignment, where an expression later in **kwargs can refer to a column created earlier in the same assign().

dfa = pd.DataFrame({"A": [1, 2, 3], "B": [4, 5, 6]})
dfa.assign(C=lambda x:x.A+x.B, D=lambda x:x.A+x.C)

In the second expression, x['C'] will refer to the newly created column,that‚Äôs equal to dfa['A'] + dfa['B'].

Indexing / selection
---------------------
The basics of indexing are as follows:
Operation                           Syntax                      Result
Select column                       df[col]                     Series
Select row by label                 df.loc[label]               Series
Select row by integer location      df.iloc[loc]                Series
Slice rows                          df[5:10]                    DataFrame
Select rows by boolean vector       df[bool_vec]                DataFrame

Row selection, for example, returns a Series whose index is the columns of the DataFrame:
df.loc["b"]
df.iloc[2]

Data alignment and arithmetic
------------------------------

Data alignment between DataFrame objects automatically align on BOTH THE COLUMNS AND THE INDEX (ROW LABELS). Again, the resulting object will have the union of the column and row labels.
df1 = pd.DataFrame(np.random.randn(10, 4), columns=['A', 'B', 'C', 'D'])
df2 = pd.DataFrame(np.random.randn(7,3), columns=['A', 'B', 'C'])
df1 + df2

When doing an operation between DataFrame and Series, the default behavior is to align the Series index on the DataFrame columns, thus broadcasting row-wise. For example:
df1 - df1.iloc[0]

Operations with scalars are just as you would expect:
df = pd.DataFrame(np.arange(20).reshape(5,4), columns=['A', 'B', 'C', 'D'])
df*5+2
1/df
df**4

Boolean operators work as well:
df1 = pd.DataFrame({'a':[1,0,1], 'b':[0,1,1]}, dtype=bool)
df2 = pd.DataFrame({'a':[0,1,1], 'b':[1,1,0]}, dtype=bool)
df1 & df2
df1 | df2
df1 ^ df2
-df1

Transposing
To transpose, access the T attribute (also the transpose function), similar to an ndarray:
df[:3].T

DataFrame interoperability with NumPy functions
------------------------------------------------
Elementwise NumPy ufuncs (log, exp, sqrt, . . . ) and various other NumPy functions can be used with no issues on Series and DataFrame, assuming the data within are numeric:
np.exp(df)
np.asanyarray(df)

DataFrame is not intended to be a drop-in replacement for ndarray as its indexing semantics and data model are quite different in places from an n-dimensional array.
Series implements __array_ufunc__, which allows it to work with NumPy‚Äôs universal functions. The ufunc is applied to the underlying array in a Series.
s = pd.Series(np.arange(1,5).astype(int))
np.exp(s)

Changed in version 0.25.0: When multiple Series are passed to a ufunc, they are aligned before performing the operation.
Like other parts of the library, pandas will automatically align labeled inputs as part of a ufunc with multiple inputs. For example, using numpy.remainder() on two Series with differently ordered labels will align before the operation.
s1 = pd.Series(np.arange(1,4), index=['a', "b", 'c'])
s2 = pd.Series([1,3,5], index=['b', 'a', 'c'])
s1+s2
np.remainder(s1,s2)

As usual, the union of the two indices is taken, and non-overlapping values are filled with missing values.
s1 = pd.Series(np.arange(1,4), index=['a', "b", 'c'])
s2 = pd.Series([1,3,5], index=['b', 'a', 'c'])
s3 = pd.Series(np.arange(2,7,2), index=['b', 'c', 'd'])
s1+s2
np.remainder(s1, s2)
np.remainder(s1, s3)

When a binary ufunc is applied to a Series and Index, the Series implementation takes precedence and a Series is returned.
ser = pd.Series([1,2,3])
idx = pd.Index([4,5,6])
type(np.maximum(ser, idx)) # pandas.core.series.Series

NumPy ufuncs are safe to apply to Series backed by non-ndarray arrays, for example arrays.SparseArray. If possible, the ufunc is applied without converting the underlying data to an ndarray.

Console display
---------------
Very large DataFrames will be truncated to display them in the console. You can also get a summary using info().
However, using to_string will return a string representation of the DataFrame in tabular form, though it won‚Äôt always fit the console width:
baseball = pd.read_csv("data/baseball.csv")
baseball
baseball.info()
print(baseball.iloc[-20:,:12].to_string())

Wide DataFrames will be printed across multiple rows by default:
pd.DataFrame(np.random.randn(3,12))

You can change how much to print on a single row by setting the display.width option:
pd.set_option('display.width', 40)
pd.DataFrame(np.random.randn(3, 12))

You can adjust the max width of the individual columns by setting display.max_colwidth
pd.set_option("display.max_colwidth", 30)

You can also disable this feature via the expand_frame_repr option. This will print the table in one block.

DataFrame column attribute access and IPython completion
---------------------------------------------------------
If a DataFrame column label is a valid Python variable name, the column can be accessed like an attribute:
df = pd.DataFrame({"foo1": np.random.randn(5), "foo2": np.random.randn(5)})
df.foo1
The columns are also connected to the IPython completion mechanism so they can be tab-completed:
df.fo<TAB>

2.3 Essential basic functionality
==================================
Here we discuss a lot of the essential functionality common to the pandas data structures. To begin, let‚Äôs create some example objects like we did in the 10 minutes to pandas section:
s = pd.Series(np.random.randn(5), index=['a','b','c','d','e'])
index = pd.date_range('1.1.2000', periods=8)
df = pd.DataFrame(np.random.randn(8,3), index=index, columns=['A', 'B', 'C'])

2.3.1 Head and tail
-------------------
To view a small sample of a Series or DataFrame object, use the head() and tail() methods. The default number of elements to display is five, but you may pass a custom number.
long_series = pd.Series(np.random.randn(1000))
long_series.head()
long_series.tail(3)

2.3.2 Attributes and underlying data
-------------------------------------
pandas objects have a number of attributes enabling you to access the metadata ‚Ä¢ shape: gives the axis dimensions of the object, consistent with ndarray
‚Ä¢ Axis labels
‚Äì Series: index (only axis)
‚Äì DataFrame: index (rows) and columns Note, these attributes can be safely assigned to!
df[:2]
df.columns = [col.lower() for col in df.columns]

pandas objects (Index, Series, DataFrame) can be thought of as containers for arrays, which hold the actual data and do the actual computation. For many types, the underlying array is a numpy.ndarray. However, pandas and 3rd party libraries may extend NumPy‚Äôs type system to add support for custom arrays (see dtypes).
To get the actual data inside a Index or Series, use the .array property
s.array
df.index.array

array will always be an ExtensionArray. The exact details of what an ExtensionArray is and why pandas uses them are a bit beyond the scope of this introduction. We will see it in future chapter.
s.to_numpy()
np.asarray(s)

--------------Extension Array-----------------

3.15.6 pandas.api.extensions.ExtensionArray
class pandas.api.extensions.ExtensionArray Abstract base class for custom 1-D array types.
pandas will recognize instances of this class as proper arrays with a custom type and will not attempt to coerce them to objects. They may be stored directly inside a DataFrame or Series.

----------------------------------------------

When the Series or Index is backed by an ExtensionArray, to_numpy() may involve copying data and coercing values. See dtypes for more.
to_numpy() gives some control over the dtype of the resulting numpy.ndarray. For example, consider date- times with timezones. NumPy doesn‚Äôt have a dtype to represent timezone-aware datetimes, so there are two possibly useful representations:
1. An object-dtype numpy.ndarray with Timestamp objects, each with the correct tz
2. A datetime64[ns] -dtype numpy.ndarray, where the values have been converted to UTC and the time-
zone discarded
Timezones may be preserved with dtype=object
ser = pd.Series(pd.date_range('2000', periods=2, tz='CET'))
ser # dtype: datetime64[ns, CET]
ser.to_numpy() # dtype=object
ser.to_numpy(dtype=object) # array([Timestamp('2000-01-01 00:00:00+0100', tz='CET', freq='D'),
                           # Timestamp('2000-01-02 00:00:00+0100', tz='CET', freq='D')],
                           # dtype=object)

Or Timezones can be thrown away with dtype='datetime64[ns]'
ser.to_numpy(dtype='datetime64[ns]') # array(['1999-12-31T23:00:00.000000000', '2000-01-01T23:00:00.000000000'],
                                      #dtype='datetime64[ns]')

Getting the ‚Äúraw data‚Äù inside a DataFrame is possibly a bit more complex. When your DataFrame only has a single data type for all the columns, DataFrame.to_numpy() will return the underlying data:
df = pd.DataFrame(np.random.randn(8,3), index=index, columns=['A', 'B', 'C'])
df.columns = [col.lower() for col in df.columns]
df.to_numpy()

If a DataFrame contains homogeneously-typed data, the ndarray can actually be modified in-place, and the changes will be reflected in the data structure. For heterogeneous data (e.g. some of the DataFrame‚Äôs columns are not all the same dtype), this will not be the case. The values attribute itself, unlike the axis labels, cannot be assigned to.
Note: When working with heterogeneous data,the dtype of the resulting ndarray will be chosen to accommodate all of the data involved. For example, if strings are involved, the result will be of object dtype. If there are only floats and integers, the resulting array will be of float dtype.

In the past, pandas recommended Series.values or DataFrame.values for extracting the data from a Series or DataFrame. You‚Äôll still find references to these in old code bases and online. Going forward, we recommend avoiding .values and using .array or .to_numpy(). .values has the following drawbacks:
  1. When your Series contains an extension type, it‚Äôs unclear whether Series.values returns a NumPy array or the extension array. Series.array will always return an ExtensionArray, and will never copy data. Series.to_numpy() will always return a NumPy array, potentially at the cost of copying / coercing values.
  2. When your DataFrame contains a mixture of data types, DataFrame.values may involve copying data and coercing values to a common dtype, a relatively expensive operation. DataFrame.to_numpy(), being a method, makes it clearer that the returned NumPy array may not be a view on the same data in the DataFrame.
--------------- Extension Types --------------------------

4.7.2 Extension types
pandas defines an interface for implementing data types and arrays that extend NumPy‚Äôs type system. pandas itself uses the extension system for some types that aren‚Äôt built into NumPy (categorical, period, interval, datetime with timezone).
Libraries can define a custom array and data type. When pandas encounters these objects, they will be handled properly (i.e. not converted to an ndarray of objects). Many methods like pandas.isna() will dispatch to the extension type‚Äôs implementation.
If you‚Äôre building a library that implements the interface, please publicize it on ecosystem.extensions. The interface consists of two classes.

ExtensionDtype
A pandas.api.extensions.ExtensionDtype is similar to a numpy.dtype object. It describes the data type. Implementors are responsible for a few unique items like the name.
One particularly important item is the type property. This should be the class that is the scalar type for your data. For example, if you were writing an extension array for IP Address data, this might be ipaddress.IPv4Address.
See the extension dtype source for interface definition.
New in version 0.24.0.
pandas.api.extension.ExtensionDtype can be registered to pandas to allow creation via a string dtype name. This allows one to instantiate Series and .astype() with a registered string name, for example 'category' is a registered string accessor for the CategoricalDtype.
See the extension dtype dtypes for more on how to register dtypes. ExtensionArray
This class provides all the array-like functionality. ExtensionArrays are limited to 1 dimension. An ExtensionArray is linked to an ExtensionDtype via the dtype attribute.
pandas makes no restrictions on how an extension array is created via its __new__ or __init__, and puts no restrictions on how you store your data. We do require that your array be convertible to a NumPy array, even if this is relatively expensive (as it is for Categorical).
They may be backed by none, one, or many NumPy arrays. For example, pandas.Categorical is an extension array backed by two arrays, one for codes and one for categories. An array of IPv6 addresses may be backed by a NumPy structured array with two fields, one for the lower 64 bits and one for the upper 64 bits. Or they may be backed by some other storage type, like Python lists.

----------------------------------------------------------

2.3.3 Accelerated operations
-----------------------------
pandas has support for accelerating certain types of binary numerical and boolean operations using the numexpr library and the bottleneck libraries.
These libraries are especially useful when dealing with large data sets, and provide large speedups. numexpr uses smart chunking, caching, and multiple cores. bottleneck is a set of specialized cython routines that are especially fast when dealing with arrays that have nans.
You are highly encouraged to install both libraries.
These are both enabled to be used by default, you can control this by setting the options:
 pd.set_option("compute.use_bottleneck", False)
 pd.set_option("compute.use_numexpr", False)

2.3.4 Flexible binary operations
---------------------------------

With binary operations between pandas data structures, there are two key points of interest:
‚Ä¢ Broadcasting behavior between higher- (e.g. DataFrame) and lower-dimensional (e.g. Series) objects.
‚Ä¢ Missing data in computations.
We will demonstrate how to manage these issues independently, though they can be handled simultaneously.

Matching / broadcasting behavior
--------------------------------
DataFrame has the methods add(), sub(), mul(), div() and related functions radd(), rsub(), ... for carrying out binary operations. For broadcasting behavior, Series input is of primary interest. Using these functions, you can use to either match on the index or columns via the axis keyword:
df = pd.DataFrame({
    'one':pd.Series(np.arange(1,4), index=['a','b','c',]),
    'two':pd.Series(np.arange(1,5), index=['a','b','c','d',]),
    'three':pd.Series(np.arange(1,4), index=['b','c','d']),
})
print(df)
row = df.iloc[1] # 2, 2, 1
column = df['two'] # 1, 2, 3, 4
print(df.sub(row, axis='columns'))
print(df.sub(row, axis=1))

df.sub(column, axis='index')
df.sub(column, axis=0)

Furthermore you can align a level of a MultiIndexed DataFrame with a Series.
dfmi = df.copy()
dfmi.index = pd.MultiIndex.from_tuples(
    [(1,'a'), (1,'b'), (1,'c'), (2, 'a')],
    names=['first', 'second']
    )
dfmi
column = df['two'] # 1, 2, 3, 4
dfmi.sub(column, axis=0, level=1)
dfmi.sub(column, axis=0, level="second")

Series and Index also support the divmod() builtin. This function takes the floor division and modulo operation at the same time returning a two-tuple of the same type as the left hand side. For example:
s = pd.Series(np.arange(0,31,10))
quo, rem = dm = divmod(s, 3)
print(dm)
print('*'*8)
print(rem)
print('*'*8)
print(quo)

idx = pd.Index(np.arange(0,31,10))
quo, rem = divmod(idx, 3)
quo
rem

We can also do elementwise divmod():
quo, rem = divmod(s, [1,2,3,5])

Missing data / operations with fill values
-------------------------------------------
In Series and DataFrame, the arithmetic functions have the option of inputting a fill_value, namely a value to substitute when at most one of the values at a location are missing. For example, when adding two DataFrame objects, you may wish to treat NaN as 0 unless both DataFrames are missing that value, in which case the result will be NaN (you can later replace NaN with some other value using fillna if you wish).
df1 = DataFrame({
    'a':[1,2,3,],
    'b':[11,22,33]
})

df2 = DataFrame({
    'a':[1,2,3,],
    'b':[11,22,33],
    'c':[10, 20, 30]
})
print(df1 + df2)
df1.add(df2, fill_value=0)
df1.add(df2, fill_value=1)
   a   b   c
0  2  22 NaN
1  4  44 NaN
2  6  66 NaN
a	b	c
0	2	22	10.0
1	4	44	20.0
2	6	66	30.0
	a	b	c
0	2	22	11.0
1	4	44	21.0
2	6	66	31.0

Flexible comparisons
---------------------
Series and DataFrame have the binary comparison methods eq, ne, lt, gt, le, and ge whose behavior is analogous to the binary arithmetic operations described above:
df1.gt(df2)
df1.ne(df2)

These operations produce a pandas object of the same type as the left-hand-side input that is of dtype bool. These boolean objects can be used in indexing operations.

Boolean reductions
-------------------
You can apply the reductions: empty, any(), all(), and bool() to provide a way to summarize a boolean result.
df1.gt(2).all(axis=1)
df1.gt(2).any(axis=1)

You can reduce to a final boolean value.
df1.gt(2).any(axis=1).any() # True

You can test if a pandas object is empty, via the empty property.
df1.empty # False
pd.DataFrame(columns=list('ABC')).empty # True

To evaluate single-element pandas objects in a boolean context, use the method bool():
pd.Series([True]).bool() # True
pd.DataFrame([False]).bool() # False

Warning: You might be tempted to do the following:
if df:
    pass

df1 and df2

These will both raise errors, as you are trying to compare multiple values.:
ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().

Comparing if objects are equivalent
------------------------------------
Often you may find that there is more than one way to compute the same result. As a simple example, consider df + df and df * 2. To test that these two computations produce the same result, given the tools shown above, you might imagine using(df + df == df * 2).all().But in fact,this expression is False:
( (df1 + df1) == (df1 * 2) ).all()

To get a single boolean value use
( (df1 + df1) == (df1 * 2) ).all().all()

Notice that the boolean DataFrame df + df == df * 2 contains some False values! This is because NaNs do not compare as equals:
np.nan == np.nan # False
So, NDFrames (such as Series and DataFrames) have an equals() method for testing equality, with NaNs in corre- sponding locations treated as equal.
pd.Series(np.nan).equals(pd.Series(np.nan)) # True
(df1*2).equals(df1 + df1) # True

Note that the Series or DataFrame index needs to be in the same order for equality to be True:
df1 = DataFrame({'a':['foo', 0, np.nan]})
df2 = DataFrame({'a':[0, 'foo', np.nan]}, index=[1,0,2])

df1.equals(df2) # False
df1.equals(df2.sort_index()) # True

Comparing array-like objects
-----------------------------
You can conveniently perform element-wise comparisons when comparing a pandas data structure with a scalar value:
pd.Series(["foo", "bar", "baz"]) == "foo"
pd.Index(["foo", "bar", "baz"]) == "foo"

pandas also handles element-wise comparisons between different array-like objects of the same length:
pd.Series(["foo", "bar", "baz"]) == pd.Index(["foo", "bar", "qux"])
pd.Series(["foo", "bar", "baz"]) == np.array(["foo", "bar", "qux"]) ## True True False
pd.Series(["foo", "bar", "baz"]).equals( np.array(["foo", "bar", "qux"]) ) # False

Trying to compare Index or Series objects of different lengths will raise a ValueError:
pd.Series(['foo', 'bar', 'baz']) == pd.Series(['foo', 'bar']) ValueError: Series lengths must match to compare
pd.Series(['foo', 'bar', 'baz']) == pd.Series(['foo']) ValueError: Series lengths must match to compare

Note that this is different from the NumPy behavior where a comparison can be broadcast:
np.array([1, 2, 3]) == np.array([2]) #  array([False, True, False])
or it can return False if broadcasting can not be done:
np.array([1, 2, 3]) == np.array([1, 2]) # False

Combining overlapping data sets
--------------------------------
A problem occasionally arising is the combination of two similar data sets where values in one are preferred over the other. An example would be two data series representing a particular economic indicator where one is considered to be of ‚Äúhigher quality‚Äù. However, the lower quality series might extend further back in history or have more complete data coverage. As such, we would like to combine two DataFrame objects where missing values in one DataFrame are conditionally filled with like-labeled values from the other DataFrame. The function implementing this operation is combine_first(), which we illustrate:
df1 = pd.DataFrame({
    'A':[ 1, np.nan, 3, 5, np.nan, ],
    'B':[ np.nan, 2, 3, np.nan, 6, ],
})

df2 = pd.DataFrame({
    "A": [5.0, 2.0, 4.0, np.nan, 3.0, 7.0],
    "B": [np.nan, np.nan, 3.0, 4.0, 6.0, 8.0],
})

df1.combine_first(df2)

General DataFrame combine
--------------------------
The combine_first() method above calls the more general DataFrame.combine(). This method takes another DataFrame and a combiner function, aligns the input DataFrame and then passes the combiner function pairs of Series (i.e., columns whose names are the same).
So, for instance, to reproduce combine_first() as above:
def combiner(x,y):
    return np.where(pd.isna(x),y,x)

df1.combine(df2, combiner)

2.3.5 Descriptive statistics
-----------------------------
There exists a large number of methods for computing descriptive statistics and other related operations on Series, DataFrame. Most of these are aggregations (hence producing a lower-dimensional result) like sum(), mean(), and quantile(), but some of them, like cumsum() and cumprod(), produce an object of the same size. Generally speaking, these methods take an axis argument, just like ndarray.{sum, std, . . . }, but the axis can be specified by name or integer:

‚Ä¢ Series: no axis argument needed
‚Ä¢ DataFrame: ‚Äúindex‚Äù (axis=0, default), ‚Äúcolumns‚Äù (axis=1)

df.mean(0)
df.mean(1)

All such methods have a skipna option signaling whether to exclude missing data (True by default):
df.sum(0, skipna=False)
df.sum(0, skipna=True)

Combined with the broadcasting / arithmetic behavior, one can describe various statistical procedures, like standard- ization (rendering data zero mean and standard deviation of 1), very concisely:
standardization formula z = x - ¬µ / std

ts_stand = (df - df.mean()) / df.std()
ts_stand.std()
ts_stand.mean()

xs_stand = df.sub(df.mean(1), axis=0).div(df.std(1), axis=0)
xs_stand.std(1)
xs_stand.mean(1)

Note that methods like cumsum() and cumprod() preserve the location of NaN values. This is somewhat different from expanding() and rolling() since NaN behavior is furthermore dictated by a min_periods parameter.
df.cumsum()

Here is a quick reference summary table of common functions. Each also takes an optional level parameter which applies only if the object has a hierarchical index.
Function             Description
count               Number of non-NA observations
sum                 Sum of values
mean                Mean of values
mad                 Mean absolute deviation
median              Arithmetic median of values
min                 Minimum
max                 Maximum
mode                Mode
abs                 Absolute Value
prod                Product of values
std                 Bessel-corrected sample standard deviation
var                 Unbiased variance
sem                 Standard error of the mean
skew                Sample skewness (3rd moment)
kurt                Sample kurtosis (4th moment)
quantile            Sample quantile (value at %)
cumsum              Cumulative sum
cumprod             Cumulative product
cummax              Cumulative maximum
cummin              Cumulative minimum

Note that by chance some NumPy methods, like mean, std, and sum, will exclude NAs on Series input by default:
np.mean(df.one) # 2.0
np.mean(df.one.to_numpy()) # nan

Series.nunique() will return the number of unique non-NA values in a Series:
series = pd.Series(np.arange(1,11))
series[6:] = np.nan
series[3:6] = 5
series.nunique()

Summarizing data: describe
----------------------------
There is a convenient describe() function which computes a variety of summary statistics about a Series or the columns of a DataFrame (excluding NAs of course):
series = pd.Series(np.random.randn(1000))
series[::2] = np.nan
series.describe()

frame = pd.DataFrame(np.random.randn(1000, 5), columns=["a", "b", "c", "d","e"])
frame.iloc[::2] = np.nan
frame.describe()

You can select specific percentiles to include in the output:
series.describe(percentiles=[0.05, 0.25, 0.75, 0.95])

By default, the median is always included.
For a non-numerical Series object, describe() will give a simple summary of the number of unique values and most frequently occurring values:
s = pd.Series(["a", "a", "b", "b", "a", "a", np.nan, "c", "d", "a"])
s.describe()

Note that on a mixed-type DataFrame object, describe() will restrict the summary to include only numerical columns or, if none are, only categorical columns:
frame = pd.DataFrame({"a": ["Yes", "Yes", "No", "No"], "b": range(4)})
frame.describe()

This behavior can be controlled by providing a list of types as include/exclude arguments. The special value all can also be used:
frame.describe(include=['object'])
frame.describe(include=['number'])
frame.describe(include='all')

That feature relies on select_dtypes.

Index of min/max values
------------------------
The idxmin() and idxmax() functions on Series and DataFrame compute the index labels with the minimum and maximum corresponding values:
s = pd.Series(np.random.choice(10, 5, replace=False))
0    4
1    6
2    7
3    8
4    0
s.idxmin(), s.idxmax() # (4, 3)

df1 = pd.DataFrame(
    np.random.choice(20, 15, replace=False).reshape(5,3),
    columns=['A', 'B', 'C']
)
df1
df1.idxmax(), df1.idxmin()
df1.idxmax(axis=1), df1.idxmin(axis=1)

When there are multiple rows (or columns) matching the minimum or maximum value, idxmin() and idxmax() return the first matching index:
df1 = pd.DataFrame(
    np.random.choice(20, 15).reshape(5,3),
    columns=['A', 'B', 'C']
)
df1

    A	B	C
0	4	5	13
1	19	12	16
2	1	4	13
3	19	19	8
4	13	6	15
df1['A'].idxmax(), df1.idxmin()
df1.idxmax().idxmax() - To get the index of the max value in a df

Note: idxmin and idxmax are called argmin and argmax in NumPy.

Value counts (histogramming) / mode
-------------------------------------
The value_counts() Series method and top-level function computes a histogram of a 1D array of values. It can also be used as a function on regular arrays:
data = np.random.randint(0,4, size=50)
s = pd.Series(data)
s.value_counts()
pd.value_counts(s)

The value_counts() method can be used to count combinations across multiple columns. By default all columns
are used but a subset can be selected using the subset argument.
data = {"a": [1, 2, 3, 4,1], "b": ["x", "x", "y", "y",'x']}
frame = pd.DataFrame(data)
frame.value_counts()
a  b
1  x    2
2  x    1

To find if a row is duplicated in a dataframe then we can go with duplicated method.
frame[frame.duplicated()]

So, if we just want to rows which is duplicated then we can go with duplicated method, if we want to counts also then we can use value_counts().
frame.value_counts()[frame.value_counts() > 1]

Similarly, you can get the most frequently occurring value(s), i.e. the mode, of the values in a Series or DataFrame:
s5 = pd.Series([1, 1, 3, 3, 3, 3, 5, 5, 7, 7, 7,])
s5.mode() #     0    3

df = pd.DataFrame({
    'A': np.random.randint(0,7, size=5),
    'B': np.random.randint(-10, 15, size=5)
})
df.mode()

Discretization and quantiling
-------------------------------
Continuous values can be discretized using the cut() (bins based on values) and qcut() (bins based on sample quantiles) functions:
arr = np.arange(1,9).astype(float)
factor = pd.cut(arr, [-5, -1, 0, 1, 5])
factor

qcut() computes sample quantiles. For example, we could slice up some normally distributed data into equal-size quartiles like so:
arr = np.arange(1,9).astype(float)
factor = pd.qcut(arr, [0, .25, .5, .75, 1])
factor
pd.value_counts(factor)

We can also pass infinite values to define the bins:
arr = np.arange(1,9).astype(float)
factor = pd.cut(arr, [-np.inf, 0, np.inf])
factor

2.3.6 Function application
==========================
To apply your own or another library‚Äôs functions to pandas objects, you should be aware of the three methods below. The appropriate method to use depends on whether your function expects to operate on an entire DataFrame or Series, row- or column-wise, or elementwise.
1. Tablewise Function Application: pipe()
2. Row or Column-wise Function Application: apply()
3. Aggregation API: agg() and transform()
4. Applying Elementwise Functions: applymap()

Tablewise function application
-------------------------------
DataFrames and Series can be passed into functions. However, if the function needs to be called in a chain, consider using the pipe() method.

df_p = pd.DataFrame({'city_and_code': ['Chicago, IL']})
df_p

def extract_city_name(df):
    """
        Chicago, IL -> Chicago for city_name column
    """
    df['city_name'] = df['city_and_code'].str.split(",").str.get(0)
    return df

def add_country_name(df, country_name=None):
    """
        Chicago -> Chicago-US for city_name column
    """
    col = 'city_name'
    df['city_and_country'] = df[col] + country_name
    return df

extract_city_name and add_country_name are functions taking and returning DataFrames.

Now compare the following:
add_country_name(extract_city_name(df_p), country_name='US')

Is equivalent to:
df_p.pipe(extract_city_name).pipe(add_country_name, country_name='US')

pandas encourages the second style, which is known as method chaining. pipe makes it easy to use your own or another library‚Äôs functions in method chains, alongside pandas‚Äô methods.

In the example above, the functions extract_city_name and add_country_name each expected a DataFrame as the first positional argument. What if the function you wish to apply takes its data as, say, the second argument? In this case, provide pipe with a tuple of (callable, data_keyword). .pipe will route the DataFrame to the argument specified in the tuple.

For example, we can fit a regression using statsmodels. Their API expects a formula first and a DataFrame as the second argument,data.We pass in the function, keyword pair(sm.ols, 'data')to pipe:
bb.query("h>0").assign(ln_h=lambda df:np.log(df.h)).pipe(
    (sm.ols, 'data'), "hr ~ ln_h + year + g + C(lg)").fit().summary()

The pipe method is inspired by unix pipes and more recently dplyr and magrittr, which have introduced the popular (%>%) (read pipe) operator for R. The implementation of pipe here is quite clean and feels right at home in Python. We encourage you to view the source code of pipe().

Row or column-wise function application
----------------------------------------
Arbitrary functions can be applied along the axes of a DataFrame using the apply() method, which, like the descriptive statistics methods, takes an optional axis argument:

df.apply(np.mean)
df.apply(np.mean)
df.apply(np.mean, axis=1)
df.apply(lambda x:x.max() - x.min())
df.apply(np.cumsum)
df.apply(np.exp)

The apply() method will also dispatch on a string method name.
df.apply("mean")
df.apply("mean", axis=1)

The return type of the function passed to apply() affects the type of the final output from DataFrame.apply for the default behaviour:
‚Ä¢ If the applied function returns a Series, the final output is a DataFrame. The columns match the index of the Series returned by the applied function.
‚Ä¢ If the applied function returns any other type, the final output is a Series.

This default behaviour can be overridden using the result_type, which accepts three options: reduce,
broadcast, and expand. These will determine how list-likes return values expand (or not) to a DataFrame.

apply() combined with some cleverness can be used to answer many questions about a data set. For example,
suppose we wanted to extract the date where the maximum value for each column occurred:

tsdf.apply("idxmax")

You may also pass additional arguments and keyword arguments to the apply() method. For instance, consider the following function you would like to apply:
def sub_and_div(x, sub, div=1):
    return (x - sub)/div
df.apply(sub_and_div, args=(3,),div=2)

Another useful feature is the ability to pass Series methods to carry out some Series operation on each column or row:
df.apply(pd.Series.interpolate)
-----------------------------------------------------------------------
pd.Series.interpolate - Fill NaN values using an interpolation method.
-----------------------------------------------------------------------
Finally, apply() takes an argument raw which is False by default, which converts each row or column into a Series before applying the function. When set to True, the passed function will instead receive an ndarray object, which has positive performance implications if you do not need the indexing functionality.
df.apply(np.mean, raw=True)

Which one is faster?
%timeit df.apply(np.mean, raw=True) ## 312 ¬µs ¬± 28.5 ¬µs per loop
%timeit df.apply('mean') ## 727 ¬µs ¬± 4.89 ¬µs per loop

np is way faster, another example
%timeit df.apply(np.median, raw=True) # 423 ¬µs ¬± 35.3 ¬µs per loop, (mean ¬± std. dev. of 7 runs, 1000 loops each)
%timeit df.apply('median') ## 1.6 ms ¬± 49.1 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)

Aggregation API
----------------
The aggregation API allows one to express possibly multiple aggregation operations in a single concise way. This API is similar across pandas objects, see groupby API, the window API, and the resample API. The entry point for aggregation is DataFrame.aggregate(), or the alias DataFrame.agg().
Using a single function is equivalent to apply(). You can also pass named methods as strings. These will return a Series of the aggregated output:
We will use a similar starting frame from above:
tsdf = pd.DataFrame(np.random.choice(20, 15, replace=False).reshape(5,3),
                    columns=['A', 'B', 'C'],
                   index = pd.date_range('2020.01.01', periods=5))
tsdf.iloc[2:4] = np.nan
tsdf.agg(np.sum)
tsdf.agg('sum')
# these are equivalent to a ``.sum()`` because we are aggregating # on a single function
tsdf.sum()

Single aggregations on a Series this will return a scalar value:
tsdf["A"].agg("sum") # 31.0

Aggregating with multiple functions
-----------------------------------
You can pass multiple aggregation arguments as a list. The results of each of the passed functions will be a row in the resulting DataFrame. These are naturally named from the aggregation function.
Multiple functions yield multiple rows:
tsdf.agg(['sum'])
tsdf.agg(['sum', 'mean'])

On a Series, multiple functions return a Series, indexed by the function names:
tsdf['A'].agg(['sum', 'mean'])

Passing a lambda function will yield a <lambda> named row:
tsdf['A'].agg(['sum', lambda x: x.mean()])
sum         31.000000
<lambda>    10.333333

Passing a named function will yield that name for the row:
def my_mean(x):
    return x.mean()
tsdf['A'].agg(['sum', my_mean])
sum        31.000000
my_mean    10.333333

Aggregating with a dict
------------------------
Passing a dictionary of column names to a scalar or a list of scalars, to DataFrame.agg allows you to customize which functions are applied to which columns. Note that the results are not in any particular order, you can use an OrderedDict instead to guarantee ordering.
tsdf.agg({
    'A':'mean',
    'B':'sum'
})

Passing a list-like will generate a DataFrame output. You will get a matrix-like output of all of the aggregators. The output will consist of all unique functions. Those that are not noted for a particular column will be NaN:
tsdf.agg({
    'A':['max', 'min'],
    'B':'sum'
})

Mixed dtypes
------------
When presented with mixed dtypes that cannot aggregate, .agg will only take the valid aggregations. This is similar to how .groupby.agg works.
mdf = pd.DataFrame({
    'A':[1,2,3],
    'B':[1.0,2.0,3.0],
    'C':['foo', 'bar', 'baz'],
    'D':pd.date_range('2020.01.01', periods=3, freq='2D')
})
mdf.dtypes
mdf.agg(['min', 'sum'])

Custom describe
-----------------
With .agg() it is possible to easily create a custom describe function, similar to the built in describe function.
from functools import partial
from functools import partial
q_25 = partial(pd.Series.quantile, q=0.25)
q_25.__name__ = "25%"
q_75 = partial(pd.Series.quantile, q=0.75)
q_75.__name__ = "75%"
tsdf.agg(["count", "mean", "std", "min", q_25, "median", q_75, "max"])

Transform API
===============
The transform() METHOD RETURNS AN OBJECT THAT IS INDEXED THE SAME (SAME SIZE) AS THE ORIGINAL. This API allows you to provide multiple operations at the same time rather than one-by-one. Its API is quite similar to the .agg API.
Transform the entire frame. .transform() allows input functions as: a NumPy function, a string function name or a user defined function.
tsdf.transform(np.abs)
tsdf.transform('abs')
tsdf.transform(lambda x: x.abs())

Here transform() received a single function; this is equivalent to a ufunc application.
np.abs(tsdf)

Passing a single function to .transform() with a Series will yield a single Series in return.
tsdf['A'].transform('abs')
tsdf['A'].agg('abs') # transform is similar to agg.

Transform with multiple functions
Passing multiple functions will yield a column MultiIndexed DataFrame. The first level will be the original frame column names; the second level will be the names of the transforming functions.
tsdf.transform([np.abs, lambda x:x+1])

                        A	                B	            C
            absolute	<lambda>	absolute	<lambda>	absolute	<lambda>
2021-01-01	4.0	            5.0	        12.0	13.0	        18.0	19.0
2021-01-04	7.0	            8.0	        3.0	    4.0	            5.0     12.0

Passing multiple functions to a Series will yield a DataFrame. The resulting column names will be the transforming functions.
tsdf['A'].transform([np.abs, lambda x:x+1])

Transforming with a dict
Passing a dict of functions will allow selective transforming per column.
tsdf.transform({"A": np.abs, "B": lambda x: x + 1})

Passing a dict of lists will generate a MultiIndexed DataFrame with these selective transforms.
tsdf.transform({"A": np.abs, "B": [lambda x: x + 1, "sqrt"]})

Difference between agg and transform?
https://stackoverflow.com/questions/40957932/transform-vs-aggregate-in-pandas#:~:text=1%20Answer&text=maybe%20you%20want%20these%20values,as%20what%20you%20started%20with.&text=agg%20is%20used%20when%20you,run%20on%20the%20same%20column.

consider the dataframe df
dft = pd.DataFrame(dict(A=list('aabb'), B=[1, 2, 3, 4], C=[0, 9, 0, 9]))

groupby is the standard use aggregater
dft.groupby('A').mean()

	B	C
A
a	1.5	4.5
b	3.5	4.5

maybe you want these values broadcast across the whole group and return something with the same index as what you started with.
use transform
dft.groupby('A').transform('mean')
	B	C
0	1.5	4.5
1	1.5	4.5
2	3.5	4.5
3	3.5	4.5

dft.set_index('A').groupby(level='A').transform('mean')
	B	C
A
a	1.5	4.5
a	1.5	4.5
b	3.5	4.5
b	3.5	4.5

agg is used when you have specific things you want to run for different columns or more than one thing run on the same column.
df.groupby('A').agg(['mean', 'std'])

        B	            C
    mean	std	    mean	std
A
a	1.5	0.707107	4.5	6.363961
b	3.5	0.707107	4.5	6.363961

dft.groupby('A').agg(dict(B='sum', C=['mean', 'prod']))

        B	        C
        sum	    mean	prod
A
a	    3	    4.5	    0
b	    7	    4.5	    0

Applying elementwise functions
===============================
Since not all functions can be vectorized (accept NumPy arrays and return another array or value), the methods applymap() on DataFrame and analogously map() on Series accept any Python function taking a single value and returning a single value. For example:
def f(x):
    return len(str(x))

tsdf.A.map(f)
tsdf.applymap(f)

Series.map() has an additional feature; it can be used to easily ‚Äúlink‚Äù or ‚Äúmap‚Äù values defined by a secondary series. This is closely related to merging/joining functionality:
s = pd.Series(["six", "seven", "six", "seven", "six"], index=["a", "b", "c", "d", "e"])
t = pd.Series({"six": 6.0, "seven": 7.0})
s.map(t)

2.3.7 Reindexing and altering labels
=====================================
reindex() is the fundamental data alignment method in pandas. It is used to implement nearly all other features relying on label-alignment functionality. To reindex means to conform the data to match a given set of labels along a particular axis. This accomplishes several things:
‚Ä¢ Reorders the existing data to match a new set of labels
‚Ä¢ Inserts missing value (NA) markers in label locations where no data for that label existed
‚Ä¢ If specified, fill data for missing labels using logic (highly relevant to working with time series data)
Here is a simple example:
s = pd.Series(np.random.randn(5), index=["a", "b", "c", "d", "e"])
s.reindex(["e", "b", "f", "d"])

Here, the f label was not contained in the Series and hence appears as NaN in the result.
With a DataFrame, you can simultaneously reindex the index and columns:
df = DataFrame(
    np.random.randn(4, 3),
    columns=['one', 'two', 'three'],
    index=['a', 'b', 'c', 'd']
)
df.loc['a']['three'] = np.nan
df.loc['d']['one'] = np.nan
df

df.reindex(index=['c', 'f', 'a'], columns=['three', 'two', 'one'])

which one is faster to rearrange columns?
%timeit df.reindex(columns=['three', 'two', 'one']) # 435 ¬µs ¬± 17.1 ¬µs per loop
%timeit df[['three', 'two', 'one']] # 572 ¬µs ¬± 26.2 ¬µs per loop

You may also use reindex with an axis keyword:
df.reindex(["c", "f", "b"], axis="index")

Note that the Index objects containing the actual axis labels can be shared between objects. So if we have a Series and a DataFrame, the following can be done:
rs = s.reindex(df.index)
rs.index is df.index # True

This means that the reindexed Series‚Äôs index is the same Python object as the DataFrame‚Äôs index.
DataFrame.reindex() also supports an ‚Äúaxis-style‚Äù calling convention, where you specify a single labels argument and the axis it applies to.
df.reindex(["c", "f", "b"], axis="index")
df.reindex(["three", "two", "one"], axis="columns")

Note: When writing performance-sensitive code, there is a good reason to spend some time becoming a reindexing ninja: many operations are faster on pre-aligned data. Adding two unaligned DataFrames internally triggers a reindexing step. For exploratory analysis you will hardly notice the difference (because reindex has been heavily optimized), but when CPU cycles matter sprinkling a few explicit reindex calls here and there can have an impact.

Reindexing to align with another object
----------------------------------------
You may wish to take an object and reindex its axes to be labeled the same as another object. While the syntax for this is straightforward albeit verbose, it is a common enough operation that the reindex_like() method is available to make this simpler:
This means, you can change the shape of the dataframe as per another dataframe.
In below example we are changing the shape of df(4,3) similar to df2(3,2)
df2 = DataFrame(
    np.random.randn(3,2),
    columns=['one', 'two'],
    index = ['a', 'b', 'c']
)
df2

df.reindex_like(df2)

Aligning objects with each other with align:
--------------------------------------------
The align() method is the fastest way to simultaneously align two objects. It supports a join argument (related to joining and merging):
‚Ä¢ join='outer': take the union of the indexes (default)
‚Ä¢ join='left': use the calling object‚Äôs index
‚Ä¢ join='right': use the passed object‚Äôs index
‚Ä¢ join='inner': intersect the indexes

It returns a tuple with both of the reindexed Series:
s = pd.Series(np.arange(1,6), index=['a', 'b', 'c', 'd', 'e'])
s1 = s[:4]
s2 = s[1:]
s1.align(s2)
s1.align(s2, join='inner')
s1.align(s2, join='left')

For DataFrames, the join method will be applied to both the index and the columns by default:
df2.align(df)
df2.align(df, join='inner')
df2.align(df, join='right')

You can also pass an axis option to only align on the specified axis:
df.align(df2, join='inner', axis=0)

If you pass a Series to DataFrame.align(), you can choose to align both objects either on the DataFrame‚Äôs index or columns using the axis argument:
df.align(df.iloc[0], axis=1)

Filling while reindexing
-------------------------
reindex() takes an optional parameter method which is a filling method chosen from the following table:
Method              Action
pad / ffill         Fill values forward
bfill / backfill    Fill values backward
nearest             Fill from the nearest index value

We illustrate these fill methods on a simple Series:
rng = pd.date_range('2021.01.01', periods=8)
ts = pd.Series(np.arange(1,9), index=rng)
ts2 = ts[[0, 3, 6]]
ts2.reindex(ts.index)
ts2.reindex(ts.index, method='ffill')
ts2.reindex(ts.index, method='bfill')
ts2.reindex(ts.index, method='nearest')

These methods require that the indexes are ordered increasing or decreasing.
Note that the same result could have been achieved using fillna (except for method='nearest') or interpolate:
ts2.reindex(ts.index).fillna(method="ffill")
ts2.reindex(ts.index).fillna(method="bfill")

reindex() will raise a ValueError if the index is not monotonically increasing or decreasing. fillna() and interpolate() will not perform any checks on the order of the index.
tst = pd.Series(np.arange(1,5), index=['4', '1', '3', '2'])
tst.reindex(index = ['2', '8', '4'], method='ffill') # ValueError
tst.reindex(index = ['2', '8', '4']).fillna(method='ffill')

Limits on filling while reindexing
------------------------------------
The limit and tolerance arguments provide additional control over filling while reindexing. Limit specifies the maximum count of consecutive matches:
ts2.reindex(ts.index, method='ffill', limit=1)

In contrast, tolerance specifies the maximum distance between the index and indexer value
ts2.reindex(ts.index, method='ffill', tolerance='1 day')

Notice that when used on a DatetimeIndex, TimedeltaIndex or PeriodIndex, tolerance will coerced into a Timedelta if possible. This allows you to specify tolerance with appropriate strings.

Dropping labels from an axis
-----------------------------
A method closely related to reindex is the drop() function. It removes a set of labels from an axis:
df.drop(['a', 'd'], axis=0)
df.drop(['two'], axis=1)
Note that the following also works, but is a bit less obvious / clean:
df.reindex(df.index.difference(['a', 'd']))

Renaming / mapping labels
--------------------------
The rename() method allows you to relabel an axis based on some mapping (a dict or Series) or an arbitrary function.
s = pd.Series(['ram', 'sur', 'cha', 'mag', 'pho'], index=['a', 'b', 'c', 'd', 'e'])
s.rename(str.upper) # renames the index

If you pass a function, it must return a value when called with any of the labels (and must produce a set of unique values). A dict or Series can also be used:
df.rename(columns={'one': 'foo', 'two':'bar'},
         index={'a':'aam', 'b':'badaaam', 'd':'doorhaiaam'}
         )

If the mapping doesn‚Äôt include a column/index label, it isn‚Äôt renamed. Note that extra labels in the mapping don‚Äôt throw an error.
Added 'e' to column but it didn't throw an error
df.rename(columns={'one': 'foo', 'two':'bar'},
         index={'a':'aam', 'b':'badaaam', 'd':'doorhaiaam', 'e':'eaam'}
         )
DataFrame.rename() also supports an ‚Äúaxis-style‚Äù calling convention, where you specify a single mapper and the axis to apply that mapping to.
df.rename({"one": "foo", "two": "bar"}, axis="columns")
df.rename({"a": "apple", "b": "banana", "d": "durian"}, axis="index")

The rename() method also provides an inplace named parameter that is by default False and copies the under- lying data. Pass inplace=True to rename the data in place.
Finally, rename() also accepts a scalar or list-like for altering the Series.name attribute.
s.rename('scaler-name')

The methods DataFrame.rename_axis() and Series.rename_axis() allow specific names of a MultiIndex to be changed (as opposed to the labels).
df = pd.DataFrame({
    'x':np.arange(1,7), 'y':np.arange(10,61,10)
}, index=pd.MultiIndex.from_product(
    [['a', 'b', 'c'], [1, 2]], names=['let', 'num']),
)
df.rename_axis(index={'let':'abc'})
df.rename_axis(index=str.upper)

================
2.3.8 Iteration
================


Attributes:
.name
.array - get the series or index in the form of Extension array
.cat.categories
df.T
.freqstr
.freq
.dt.month, .dt.year. .dt.hour, .dt.day, .dt.weekday
.index
columns
shape
dtypes
loc
iloc

Methods:
rename_axis() - rename the name of axis labels i.e index and column for multiindex df.
rename() - rename the name of axis labels i.e index and column.
align() - to get the values of both df, series in the form of tuple by join across various options.
reindex_like() - change the shape of a df, series similar to another df, series
reindex() - reindex the index or columns, it is faster than normal rearrange of columns.
applymap(), map() - since all functions are not vectorized, these functions are applied on elements. applymap on dict and map on series.
.transform() - aggregating the data, similar to agg but the index remains same
.agg(), .aggregate() -- aggregate the series or dataframe based on the function provided
.apply() -- apply the function row-wise or column-wise.
.pipe() -- apply the function on a dataframe by passing the dataframe as an arg to the function.
cut(), qcut() - discretize and quantile continuous data
mode() - get the most occurring value in a series or df
duplicated() - get the duplicated rows in a df
value_counts() - count the occurrence of an element in a series or df
idxmin(), idxmax() - get the index of the min and max value in the series or dataframe
describe([percentiles=[], include=[]) - summary desc stats, you can modify the percentiles as per your needs.
nunique - provide the count of the unique elements in a series/df excluding np.nan
combine - combine two dataframes based on a provided function
combine_first() - get the non- NAN values from both df while merging
gt(),lt(),ne() - greater than, less than, not equal
divmod() - provide the quotient and remainder in the form of a tuple
equals() - compare NDFrames at once, == will return boolean values per element
fill_value - fill the nan with a value within a method
.fillna() - fill the nan with a value
pd.set_option() - console width, height. Not use any dependent lib.
np.asarray(s,df) - convert a dataframe or series to numpy
to_numpy() - convert a dataframe or series to numpy
query() - query a dataframe, condition/filter can be applied on df
assign() - assign a column to a df using calculation or lambda
insert(position_index, column_name, value) - insert a column at a particular position
rename() - rename a series
.cat.set_categories()
period.to_timestamp - get the first day of the month
timestamp.to_period - convert date to the freq like in Months
tz_convert - convert timezone to anotehr timezone
tz_localize - specify timezone of a particular country
pd.MultiIndex.from_tuples() 
stack()
df.rsub - opposite of sub(), first apply sub and then multiply values with -1
df.sub() - subtract
isna()
fillna()
dropna()
reindex()
date_range()
idxmax() - get the max index
replace({from:to})
read_csv(parse_dates=['colName'])
to_datetime()
merge(,,how, left_on, right_on)
concat(, axis=, keys=)
melt(id_var=, value_var=, value_name=, var_name=)
set_index()
reset_index()
pivot_table(colums=, values=, index=, aggfunc=, margins=)
pivot(columns=, values=)
size()
count()
sort_index()
sort_values()
value_counts()
mean(), median(), mode(), skew(), min(), max()
groupby().aggregateFunction()
agg({col=[fn1, fn2]}) - aggreagate the data based on columns of a dataframe
rename(columns={'currentCol':'newCol'})
fig.savefig()
plt.subplot()
plot()
notna()
isin()
max()
head()
tail()
describe()
info()


Python Tricks:

Assertions:
The proper use of assertions is to tell the programmer of the unreoverable errors.
Assertions are not intended to signal expected error conditions, where the user can take corrective actions or can just try again.
Assertions are meant to be internal self checks for your program. They work by declaring some condition as impossible in your code.
If one of these condition doesn't hold that means there is a bug in the program.

Like in below example other than using if statement and raising an exception we can go with assertion.
def apply_discount(product, discount):
    price = int((product['price']) * (1.0 - discount))
    assert 0 <= price <= product['price']
    return price

shoes = {'name':'Fancy Shoes', 'price':14900}
+ive case:
    (apply_discount(shoes, .2) / 100)
-ive case:
    (apply_discount(shoes, 2.0) / 100)

Caveat #1 - Don't use asserts for Data Validation
The biggest caveat with asserts in python is tht assertions can be globally disabled using -O or -OO.
This turns any asserts statement into a null operation which simply gets compiled away and won't be executed.

Caveat #2 - Assert that never fails
Non-Empty tuple is always True in Python.
>>> assert (1==2, 'This should fail')
<stdin>:1: SyntaxWarning: assertion is always true, perhaps remove parentheses?
It will give a warning but never fail.

Key Takeaways:
Python's assert statement is a debugging aid that tests a condition as an internal self check in your program.
Asserts should only be used to help developers identify bugs. They are not a mechanism for handling runtime errors.
Asserts can be globally disabled with an interpreter setting.

Commas are very important in python:
Its always a good practice to keep list, set, dict and tuple multiline.
In case you forgot to add a comma between two items of a list, python will merge it.
>>> ['ab','ms','ch' 'ra'] ## ['ab', 'ms', 'chra']
This 'string literal concatenation' is an intentional and documented behaviour.

Solution: It's all about the habits
    Always place items of list multiline.
    place a comma after each item, even the last item.

Key takeaways:
    Smart formatting and comma placesment can make your list, dict or set constants easy to maintain.
    Python's string literal concatenation feature can work to your benifits or introduce hard to catch bugs.

Context managers and the with statement
The with statement in python is regarded as an obscure feature by some. But when you peek behind the scenes, you'll see that there's no magic involved, and it's actually a highly useful feature that can help you write cleaner and more readable python code.
So what'st he with statement good for? It helps simplify some common resource management patterns by abstracting their functionality and allowing them to be factored out and reused. 
A good way to see this feature used effectively is by looking at examples in the Python standard library. 
The built in open function provides us with an excellent use case:
with open('abc.txt', 'w') as f:
    f.write('hello')

Opening the file using a with statement is generally recommended because it ensures open file descriptors are closed automatically after program execution leaves the context of the with statement. Internally, the above code sample transalates to something like this:
f = open('hello.txt', 'w')
try:
    f.write('hello')
finally:
    f.close()
You can already tell that this is quite a bit verbose. Note that the try.. finally statement is significant. It wouldn't be enough to write something like this:
f = open('hello.txt','w')
f.write('hello')
f.close()
This implementation won't gurantee the file is closed if there's an exception during the f.write() call and therefore our program might leak a file descriptor. That's why the with statement is so useful. It make properly acquiring and releasing resources a breeze.

Using a with statement allows you to abstract away most of the resource handling logic. Instead of having to write explicitly try..finally statement each time, usually the with statement takes care of that for us.
The with statement can make code that deals with system resources more redable. It also helps you avoid bugs or leaks by making it practically impossible to forget to cleanup or release a resource when it's no longer needed.

Supporting 'with' in your own objects:
We can provide the same functionality of open statement with our own classes and functions by implementing so-called context managers.

What's a context manager? It's a simple protocol or interface that your object needs to follow in order to support the with statement.
All you need to do is add two methods:
__enter__ and __exit__ 
to make your class a Context Manager.

class ManagerFile:
    def __init__(self, name):
        self.name = name
    def __enter__(self):
        self.file = open(self.name, 'w')
        return self.file
    def __exit__(self, exc_type, exc_val, ex_tb):
        if self.file:
            self.file.close()

Our ManagerFile class follows the context manager protocol and now support the with statement. just like the original open() example did.
with ManagerFile('open.txt') as f:
    f.write('Hello ji\n')
    f.write('Close yourself\n')

Python calls __enter__ when execution enters the context of the with statement and it's time to acquire the resource. When execution leaves the context again, python calls __exit__ to free up the resource.

Writing a class based context manager is not the only way to support the with statement in python. The contextlib utility module is a standard library which provides a few more abstraction build on the top of basic context manager protocol. This can make your life a little easier if your use case match with what's offered by contextlib.

For example - you can use the contextlib.contextmanager decorator to define a generator based factory function for a resource that will then automatically support the with statement. Here's what rewriting our ManagerFile context manager example with this technique look like:
from contextlib2 import contextmanager
@contextmanager
def manager_file(name):
    try:
        f = open(name, 'w')
        yield f
    finally:
        f.close()
with manager_file('new_open.txt') as f:
    f.write('This is another way of using context manager\n')
    f.write('byebye')

In this case managed_file() is a generator that first acquires the resource. After that, it temporarily suspends its own execution and yields the resource so it can be used by the caller. When the caller leaves the with context, the generator continues to execute so that any remaining clean up steps can occur and the resources can get released back to the system.

The class based implementation and the generator based implementation are essentially equivalent. You might prefer one over the other depending on which approcach you find more readable.

A downside of the @contextmanager based implementation might be that it requires some understanding of advanced python concepts like decorator and generator.

Writing pretty APIs with Context Managers:
Create a timer class with context manager using time().
import time
class MeasureTime: 
        
    def __enter__(self):
        self.start = time.time()
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.end = time.time()
        print(self.end - self.start)

with MeasureTime():
    for i in range(10000000):
        pass

Same example using contextlib2 and contextmanager decorator
from contextlib2 import contextmanager

@contextmanager
def time_taken():
    try:
        st = time.time()
        yield st
    finally:
        print(time.time()  - st)

with time_taken():
    for i in range(100000000):
        pass

Takeaways:
The with statement simplifies exception handling by encapsulating standard use of try/finally statements in a so called context managers.
Most commonly it is used to manage the safe acquisition and release of system resources. Resources are acquired by the with statement and released automatically when execution leaves the with context.
Using with effectively can help you avoid resource leaks and make your code easier to read.

Underscores, Dunders and More.
------------------------------
Single and Double underscores have a meaning in python variables and method. Some of that meaning is merely by convention and intended as a hint to the programmer and some of it is enforced by python interpreter.
There could be 5 types of variables or methods with underscores:
1. Single leading underscore - _var
2. Single trailing underscore - var_
3. Double leading underscores - __var
4. Double leading and trailing underscores - __var__
5. Single underscore - _

1. Single leading underscore - It has a meaning by convention, it is a hint to the programmer that the variable is private i.e it is used for internal purpose only.
The underscore prefix is meant as a hint to tell other programmers that a variable or method starting with a single underscore is intended for internal use. This convetion is defined in pep8 - https://pep8.org/
This convention is not enforced by python interpreter. Python does not have strong distinctions between private and public variables like Java does.
class Test:
    def __init__(self)
        self.foo = 11
        self._bar = 23

What's going to happen if you instantiate this class and try to access foo and _bar attributes defined in the constructor.
t = Test()
t.foo # 11
t._bar # 23
There was no difference.

But if you have a function named _func in a module and you imported that module as from module import * in that case _func will throw error.
“def external_func():
    return 23

“Now, if you use a wildcard import to import all the names from the module, Python will not import names with a leading underscore (unless the module defines an __all__ list that overrides this behavior):”

def _internal_func():
    return 42”
“>>> _internal_func()
NameError: "name '_internal_func' is not defined”

But importing the module as import module with not throw any error.
“>>> import my_module
>>> my_module.external_func()
23
>>> my_module._internal_func()
42”

Single Trailing Underscore:
---------------------------
Sometimes the most fitting name is already taken by the python keyword, in order to use same variable name as class_.

Double Leading Underscore:
--------------------------
“A double underscore prefix causes the Python interpreter to rewrite the attribute name in order to avoid naming conflicts in subclasses.”
“This is also called name mangling—the interpreter changes the name of the variable in a way that makes it harder to create collisions when the class is extended later.”
class Test:
    def __init__(self):
        self.foo = 11
        self._bar = 12
        self.__baz = 13
        
t = Test()
dir(t)
['_Test__baz','_bar', 'foo']
self.__baz is converted to _Test__baz

NameMangling
“If you look closely, you’ll see there’s an attribute called _Test__baz on this object. This is the name mangling that the Python interpreter applies. It does this to protect the variable from getting overridden in subclasses”

Now let's extend the Test class
    def __init__(self):
        super().__init__()
        self.foo = 'overriden'
        self._bar = 'overriden'
        self.__baz = 'overriden'
t2 = ETest()
print (t.foo, t2.foo) # 11 overriden
print (t._bar, t2._bar) # 11 overriden
print (t.__baz, t2.__baz) # AttributeError: 'Test' object has no attribute '__baz'

“As you can see, __baz got turned into _ExtendedTest__baz to prevent accidental modification. But the original _Test__baz is also still around:”
print (t._Test__baz, t2._ETest__baz) # 13 overriden

We can always use getter method to get the dunders(variables/methods) with double __
class ManglingTest:
    def __init__(self):
        self.__mangled = 'hello'
    def get_mangled(self):
        return self.__mangled
ManglingTest().get_mangled()


“Does name mangling also apply to method names? It sure does! Name mangling affects all names that start with two underscore characters (“dunders”) in a class context:”
“For example, you’d pronounce __baz as “dunder baz.” Likewise, __init__ would be pronounced as “dunder init,” even though one might think it should be “dunder init dunder.”

Double Leading and Trailing Underscores:
----------------------------------------
“Perhaps surprisingly, name mangling is not applied if a name starts and ends with double underscores. Variables surrounded by a double underscore prefix and postfix are left unscathed by the Python interpreter:”
“However, names that have both leading and trailing double underscores are reserved for special use in the language. This rule covers things like __init__ for object constructors, or __call__ to make objects callable.

These dunder methods are often referred to as magic methods—but many people in the Python community, including myself, don’t like that word. It implies that the use of dunder methods is discouraged, which is entirely not the case. They’re a core feature in Python and should be used as needed. There’s nothing “magical” or arcane about them.

However, as far as naming conventions go, it’s best to stay away from using names that start and end with double underscores in your own programs to avoid collisions with future changes to the Python language.”

Single Underscore:
------------------
“Per convention, a single stand-alone underscore is sometimes used as a name to indicate that a variable is temporary or ”
“For example, in the following loop we don’t need access to the running index and we can use “_” to indicate that it is just a temporary value:”
for _ in range(10):
    print('variable')

“You can also use single underscores in unpacking expressions as a “don’t care” variable to ignore particular values. Again, this meaning is per convention only and it doesn’t trigger any special behaviors in the Python parser. The single underscore is simply a valid variable name that’s sometimes used for this purpose.”

car = ('red','benz',12,'4 wheel')
color,_,milage,_ = car # If interested only in color and milage
print(color, milage) # red, milage
print(_) # '4 wheel'

“Besides its use as a temporary variable, “_” is a special variable in most Python REPLs that represents the result of the last expression evaluated by the interpreter.

This is handy if you’re working in an interpreter session and you’d like to access the result of a previous calculation:”
>>> 20+3
23
>>> _
23
>>> print(_)
23

“It’s also handy if you’re constructing objects on the fly and want to interact with them without assigning them a name first:”
“>>> list()
[]
>>> _.append(1)
>>> _.append(2)
>>> _.append(3)
>>> _
[1, 2, 3]”

Strings in pythons:
format method used while printing is old, f string is the new norm.

f' strings :
PEP 498 introduced a new string formatting mechanism known as Literal String Interpolation or more commonly as F-strings (because of the leading f character preceding the string literal). The idea behind f-strings is to make string interpolation simpler. 
To create an f-string, prefix the string with the letter “ f ”. The string itself can be formatted in much the same way that you would with str.format(). F-strings provide a concise and convenient way to embed python expressions inside string literals for formatting. 

It is better, more convinient and faster then format.
Eg: name = 'Utsav'
age = 31
print('My name is {}, and my age is {}'.format(name, age))
print(f'My name is {name}, and my age is {age}')

Expressions can be used directly with f string
import datetime
today = datetime.datetime.today()
print(f"{today: %B %d, %Y}")

Backslash cannot be used directly in a format string
print(f'newline {"\n"}') # SyntaxError: f-string expression part cannot include a backslash
solution:
newl = '\n'
print(f'newline, {newl}and we have a new line')

Inline arithmatic can be done with formatted String Literal.
a = 10
b = 15
print(f'Ten plus five is {a+b} and not {2*(a+b)}')

Template String - String with less capabilities but more SECURE.
It can be used in case when you have a user supplied string.
Eg:
from string import Template
n = 'Utsav'
t = Template('Hey, $name')
t.substitute(name = n)

Eg. When a malacious user can get your secret key is you are using formatted string
SECRET = 'this_is_a_secret_key'
class Error:
    def __init__(self):
        pass
err = Error()
user_input = '{anyErrorObjRef.__init__.__globals__[SECRET]}'

# Now any user can use your formatted string user_input and get your secret from it
print(user_input.format(anyErrorObjRef=err))

# Same example with Template String
user_input = '$anyErrorObjRef.__init__.__globals__[SECRET]'
Template(user_input).substitute(anyErrorObjRef = err)

Hence user cannot get your secret string using Template string.

Which String Formatting should you use?
Template String for user input string else always Formated String literal(3.6+) else format string. 
  
Functions:
==========
First class object in python.

They can be returned from another function:
def f():
    return 100
def nf():
    return f
nf()()

They can be passed as arguments to other functions
def f():
    print(100)
def nf(g):
    g()
nf(f)

They can be stored in a data structure.
def f(a,b):
    print(a+b)
def g(a,b):
    print(a*b)

l = [f,g]
print(l[0](10,20))
print(l[1](10,20))

They can be assigned to a variable
def f(a,b):
    print(a+b)
def g(a,b):
    print(a*b)
add = f
prd = g
print(add(10,20))
print(prd(10,20))

Python attaches a string identifier to every function at creation time for debugging purposes. You can access this internal identifier with the __name__ attribute.
Functions are object which are pointed by a reference which is the name of the function.
You can also assign another reference variable to same function object.
bark = yell
print(id(yell), id(bark))
print(yell.__name__, bark.__name__)
del yell
print(id(bark), bark.__name__)

Functions can be passed as arguments to other functions. Functions that can accept other functions as arguments are also called higher-order functions. They are a necessity for the functional programming style.
list(map(bark, ['hey','hello','hi']))

Inner functions which has access to the variables of outer function are called lexical scoping and when the outer function returns the inner function which can be assigned to a reference variable outside outer function this is called lexical closure.
(Lexical scoping is not supported by q but lexical closure is supported)
Eg:
def outer(text):
    def inner():
        return text.upper() ## Lexical Scoping i.e outer function variable can be used inside inner function
    return inner # Lexical Closure i.e inner function is returned from outer funtion
oi = outer('hello') 
oi()
In KDB : Lexical Closure is supported by lexical scoping is not supported.
q)o:{[t] i:{[t] :upper t}; :i}

While all functions are objects in Python, the reverse isn’t true. Objects aren’t functions. But they can be made callable, which allows you to treat them like functions in many cases.

If an object is callable it means you can use the round parentheses function call syntax on it and even pass in function call arguments. This is all powered by the __call__ dunder method. Here’s an example of class defining a callable object:

class Adder:
    def __init__(self, n):
         self.n = n

    def __call__(self, x):
        return self.n + x

>>> plus_3 = Adder(3)
>>> plus_3(4)
7

callable(object):
------------------
A callable is something that can be called like function, method etc.
The built in callable function returns True if the object passed appears to be callable otherwise false.
def geek():
    return 5
num = 5*5
callable(geek) # True
callable(num) # False

The built in callable() method checks if the argument is either of the two:
An instance of a class with __call__ method.
An instance has a method which is callable.

class Geek:
    def __call__(self):
        print('Hello')
        
callable(Geek) #True
geekObject = Geek()
callable(geekObject) # True
geekObject() # Hello

class Geek:
    def testCallable(self):
        print('Callable')
callable(Geek) # True
geekObject = Geek()
callable(geekObject) # False, because it does not have __call__ method

In fact, any object which implements the special method __call__() is termed callable. So, in the most basic sense, a decorator is a callable that returns a callable.
Basically, a decorator takes in a function, adds some functionality and returns it. 

Behind the scenes, “calling” an object instance as a function attempts to execute the object’s __call__ method.

Of course, not all objects will be callable. That’s why there’s a built-in callable function to check whether an object appears to be callable or not:

>>> callable(plus_3)
True
>>> callable(yell)
True
>>> callable('hello')
False

“Objects can be made callable. In many cases this allows you to treat them like functions.”

Lambda:
-------
Lambda are anonymous functions which are shorform of writing small anonymous functions.
normal function is defined using def keyword, lambda are defined using lambda keyword.
They are very handy when we have to use function objects.

(lambda x,y:print(x+y))(10,20)

Eg: To sort a list of tuples based on the second values in the tuples
loft = [(3,'b'),(2,'d'),(1,'e'),(4,'a')]
sorted(loft, key=lambda x:x[1]) # [(4, 'a'), (3, 'b'), (2, 'd'), (1, 'e')]

loft = [(3,'b',30),(2,'d',20),(1,'e',10),(4,'c',40)]
## To get sorted output based on first item 
print(sorted(loft, key=lambda x:x[1]))
## To get reverse of sorted output based on first item
for r in reversed(sorted(loft, key=lambda x:x[1])):
    print(r)

sort the numbers based on their squares:
sorted(range(-5,6), key= lambda x:x*x) # [0, -1, 1, -2, 2, -3, 3, -4, 4, -5, 5]

get the name of the person from a dict with max age
d = {'Ramesh': 35, 'Suresh':25, 'Chandu':31}
max(d, key=lambda x:d[x])

Before writing a lambda - “Always ask yourself: Would using a regular (named) function or a list comprehension offer more clarity?”

Decorators:
If we want to provide an additional feature to an already existing function[s] then you can opt for decorators.
decorators are higher order funcs which takes another function as input and return that function with additional ability.
Input and output for decorator are callable.

Real world example - If your manager comes on Monday and asks you to modify all the 30 functions of a report with additional logging facility then you can write a decorator and annotate all those 30 funtions with the decorator name and you are done. Other wise you will have to modify all 30 functions separately.

If you annotate the function with decorator then the functions loses its initial capability, in that case you will have to find hacks.
Other option to use a decorator is to call the decorator with existing function and assign it to another function.
g = func_decor(greet)
In this case you have new function g with the added feature and existing function greet is intact.

Applying Decorator to multiple functions:
-----------------------------------------
Eg: Write a decorator which coverts the output to uppercase for 2 input functions.
def decor_upper(func):
    def wrapper():
        orig_func = func()
        modified_ver = func().upper() ## Original function is called here
        return modified_ver
    return wrapper

@decor_upper
def greet():
    return 'hello'
@decor_upper
def abuse():
    return 'bastard'

print(greet()) # HELLO
print(abuse()) # BASTARD

Applying Multiple Decorators to a function:
--------------------------------------------
We can apply multiple decorators to a function in that case it will follow bottor up approach i.e the decorator placed at the bottom will be applied first and the decorator placed the top will be applied last.

Eg: Apply strong and emphasis tags of html on the existing function value.
def strong(func):
    def wrapper():
        return '<strong>' + func() + '</strong>'
    return wrapper
def emphasis(func):
    def wrapper():
        return '<em>' + func() + '</em>'
    return wrapper

@strong
@emphasis
def greet():
    return 'hello'

greet()

It is equivalent to 
decorated_greet = strong(emphasis(greet))

“This also means that deep levels of decorator stacking will eventually have an effect on performance because they keep adding nested function calls. In practice, this usually won’t be a problem, but it’s something to keep in mind if you’re working on performance-intensive code that frequently uses decoration.”


Decorator function that accepts arguments:
------------------------------------------
Decorators make extensive use of closures.
Closure is when the inner function is returned from a function and the inner function is assigned to a variables.
So that by executing the variable we can get the output of inner function even after execution of functions is called closure.

def trace(func):
    def wrapper(*args, **kwargs):
        print(f'TRACE: calling {func.__name__}() '
              f'with {args}, {kwargs}')
        original_result = func(*args, **kwargs)
        print(f'TRACE: {func.__name__}() '
              f'returned {original_result!r}')
        return original_result
    return wrapper

@trace
def say(name, line):
    return f'{name}: {line}'

say('Ramesh', 'Hello Duniya')

How to write debuggable Decorators:
-----------------------------------
Whenever we write a decorater for a function then the identity and attributes of the original function are lost. Hence it becomes very difficult to debug the code.

def uppercase(func):
    def wrapper():
        return func().upper()
    return wrapper

@uppercase
def greet():
    """This is an awesome greet function"""
    return 'hello duniya'

print(greet.__name__) ## wrapper
print(greet.__doc__) ## This is a wrapper

We can maintain the name and attributes of original function here using @functool.wraps.
This helps a lot during debugging and is very common in google code.

import functools
def uppercase(func):
    """This is upperclass decorator"""
    @functools.wraps(func)
    def wrapper():
        """This is a wrapper"""
        return func().upper()
    return wrapper

@uppercase
def greet():
    """This is an awesome greet function"""
    return 'hello duniya'

print(greet.__name__) # greet
print(greet.__doc__) # This is an awesome greet function

Key Takeaways of Decorator
==========================
* Decorators define reusable building blocks you can apply to a callable to modify its behavior without permanently modifying the callable itself.
* The @ syntax is just a shorthand for calling the decorator on an input function. Multiple decorators on a single function are applied bottom to top (decorator stacking).
* As a debugging best practice, use the functools.wraps helper in your own decorators to carry over metadata from the undecorated callable to the decorated one.
* Just like any other tool in the software development toolbox, decorators are not a cure-all and they should not be overused. It’s important to balance the need to “get stuff done” with the goal of “not getting tangled up in a horrible, unmaintainable mess of a code base.

*args and **kwargs:
===================
What are *args and **kwargs parameters used for? They allow a function to accept optional arguments, so you can create flexible APIs in your modules and classes.
def foo(required,*args,**kwargs):
    print(required)
    if args:
        print(args)
    if kwargs:
        print(kwargs)

foo('hello')
foo ('hello',1,2,3,4,key1='Val1', key2='val2')

The above function requires at least one argument called “required,” but it can accept extra positional and keyword arguments as well.
If we call the function with additional arguments, args will collect extra positional arguments as a tuple because the parameter name has a * prefix.
Likewise, kwargs will collect extra keyword arguments as a dictionary because the parameter name has a ** prefix.

Forwarding Optional or keyword arguments:
-----------------------------------------
It's possible to pass optinal and keyword parameters i.e * and ** from one function to another.
This also gives you an opportunity to modify the arguments before you pass them along.

def bar(required, *args, **kwargs):
    print(required)
    if args:
        print(args)
    if kwargs:
        print(kwargs)

def foo(required, *args, **kwargs):
    kwargs['name'] = 'Alice'
    new_args = args + ('ramesh',)
    bar(required, *new_args, **kwargs)
    
foo('Required','suresh','chandu',age='23', job='engg')

This technique can be useful for subclassing and writing wrapper functions. For example, you can use it to extend the behavior of a parent class without having to replicate the full signature of its constructor in the child class.
This can be quite convenient if you’re working with an API that might change outside of your control:

class Car:
    def __init__(self, color, milage):
        self.color = color
        self.milage = milage
class AlwaysWhiteCar:
    def __init__(self, *args, **kwargs):
        super().__init__()
        self.color = 'White'
        
AlwaysWhiteCar('Green', 12).color  

The AlwaysBlueCar constructor simply passes on all arguments to its parent class and then overrides an internal attribute. This means if the parent class constructor changes, there’s a good chance that AlwaysBlueCar would still function as intended.

The downside here is that the AlwaysBlueCar constructor now has a rather unhelpful signature—we don’t know what arguments it expects without looking up the parent class.

Typically you wouldn’t use this technique with your own class hierarchies. The more likely scenario would be that you’ll want to modify or override behavior in some external class which you don’t control.

One more scenario where this technique is potentially helpful is writing wrapper functions such as decorators. There you typically also want to accept arbitrary arguments to be passed through to the wrapped function.

And, if we can do it without having to copy and paste the original function’s signature, that might be more maintainable:
import functools
def trace(f):
    functools.wraps(f)
    def decorated_function(*args, **kwargs):
        print(f, args, kwargs)
        result = f(*args, **kwargs)
        print('result -', result)
        return result
    return decorated_function

@trace
def greet(greeting, name):
    return f'Hello {name}, {greeting}'

greet('How are you?', 'Ramesh')

Output -
<function greet at 0x112c0fd90> ('How are you?', 'Ramesh') {}
result - Hello Ramesh, How are you?
'Hello Ramesh, How are you?'

You can find same pattern in google finance api - https://github.com/google/tf-quant-finance/blob/13336f99bd1cab56f783f38d320dddf3da00e7cc/tf_quant_finance/datetime/holiday_calendar_test.py
def test_both_impls(test_fn):
  # Decorator to run the test with both BoundedHolidayCalendar and
  # UnboundedHolidayCalendar.
  # Create the calendar as `self.impl(args)`.
  def create_unbounded_calendar(**kwargs):
    kwargs.pop("start_year", None)
    kwargs.pop("end_year", None)
    return unbounded_holiday_calendar.UnboundedHolidayCalendar(**kwargs)

  @functools.wraps(test_fn)
  def wrapped(*args, **kwargs):
    self = args[0]
    with self.subTest("Bounded"):
      self.impl = bounded_holiday_calendar.BoundedHolidayCalendar
      test_fn(*args, **kwargs)
    with self.subTest("Unbounded"):
      self.impl = create_unbounded_calendar
      test_fn(*args, **kwargs)
  return wrapped


With techniques like this one, it’s sometimes difficult to balance the idea of making your code explicit enough and yet adhere to the Don’t Repeat Yourself (DRY) principle. This will always be a tough choice to make. If you can get a second opinion from a colleague, I’d encourage you to ask for one.

Key Takeaways from *args and **kwargs
======================================
*args and **kwargs let you write functions with a variable number of arguments in Python.
*args collects extra positional arguments as a tuple. **kwargs collects the extra keyword arguments as a dictionary.
The actual syntax is * and **. Calling them args and kwargs is just a convention (and one you should stick to).

``` - Fenced Code blocks
Inside the doc string we can use it to enclose the code snippet.

def fn():
    """
    ```cmd
        with getData(uat) as gd:
            df = gd()
    ```
    """
    print(100)

Function Argument Unpacking:
----------------------------
Sometimes you function takes more than one argument and you need to pass a sequence be it list, tuple, generator or a dict with those many items as args of function to the function.
In that case function argument unpacking comes in handy
Eg:
def unpack(a,b,c):
    print(f'<{a},{b},{c}>')
unpack(1,0,1)

What if we need to pass a tuple(1,0,1) or a list [1,0,1] or a generator of three elements to the function.
In that case function argument unpack variable * comes in handy, it will unpack the sequence to seperate elements and pass them to the function.
It is similar to the unlist operation in kdb which can be performed using raze in kdb.

Eg: print((1,2,3)) ## (1, 2, 3)
print(*(1,2,3)) ## 1 2 3

coming back to upack function where we need to pass a list [1,0,1]
def unpack(a,b,c):
    print(f'<{a},{b},{c}>')
unpack(*[1,0,1])

passing generator:
unpack(*(x*x for x in range(3))) # <0,1,4>

What if we have a dict {'x':1, 'y':0, 'z':1}? Nw, we go with ** 
def unpack(a,b,c):
    print(f'<{a},{b},{c}>')
unpack(**{'a':1, 'b':0, 'c':1}) # <1,0,1>
Here, keys of dict are matching with args of function hence values are passed. If keys of dict would not match with func args then it would result in TypeError: unpack() got an unexpected keyword argument 'x'.

If we pass dict with *, then keys are passed as args of the function:
unpack(*{'x':1, 'y':0, 'z':1}) # <x,y,z>

Key Takeaways from function unpacking
=====================================
1. The * and ** operators can be used to “unpack” function arguments from sequences and dictionaries.
2. Using argument unpacking effectively can help you write more flexible interfaces for your modules and functions.

Return None statements from function:
======================================
If a function doesn’t specify a return value, it returns None. Whether to explicitly return None is a stylistic decision.
This is a core Python feature but your code might communicate its intent more clearly with an explicit return None statement.

Difference in being identical and equal:
=========================================
1 is True ## False - reference comparision - kdb equivalent 1 ~ 1.0 / 0b
1 == True ## True  - Content comparision - kdb equivalent 1 = 1.0 / 1b

a = [1,2,3]
b = a
c = list(a)
print(id(b), id(a), id(c))
a is b is c # False
a == b == c # True

repr() or __repr__:
repr() is similar to str() with a difference that str() is used for creating output for end user whereas repr() is mainly used for debugging purpose.
Eg: 
import datetime

print(str(datetime.datetime.now())) # 2021-02-21 10:01:57.157195
print(repr(datetime.datetime.now())) # datetime.datetime(2021, 2, 21, 10, 1, 57, 157463)

difference can be seen in the output of str() and repr()
repr() is a representation which has all the information about the object.

Internally for a class __repr__ method can be overriden to get the desired representation of the class object.

Every class needs a __repr__:
-----------------------------
Whenever we print a class object the interpreter shows the name of the class and id of it.
But there can be more to it.
class Car:
    def __init__(self, color, milage):
        self.color = color
        self.milage = milage
print(Car('Green',21)) ### <__main__.Car at 0x108dd9f28>

We can use __str__ dunder to provide some meaningful output when we print an object.
class Car:
    def __init__(self, color, milage):
        self.color = color
        self.milage = milage
    
    def __str__(self):
        return f'A {self.color} car'

print(Car('Green',21)) ## A Green car
Car('Green',21) ## <__main__.Car at 0x108dc64a8>

By overridind the __str__() we get the desired output when we use print statement to print the object.
Without print statement it still provides the same output.

Now the question is why we are again getting same Car class and id when we are not using print.
The answer is __repr__, so in python3 there are 2 dunders which maintains toString feature of the class and the two dunders are __str__ and __repr__ and their corresponding methods are str() and repr() respectively.

class Car:
    def __init__(self, color, milage):
        self.color = color
        self.milage = milage
    
    def __str__(self):
        return f'A {self.color} car'
    
    def __repr__(self):
        return f'A shiny {self.color} car with milage of {self.milage}'
        
my_car = Car('Blue',21)
print('pr ', my_car) ### pr  A Blue car
print('str ', my_car) ### str  A Blue car
print('repr ', repr(my_car)) ### repr  A shiny Blue car with milage of 21
my_car ### A shiny Blue car with milage of 21

Now, even in case of printing the object without print statement we are getting the output of __repr__.

Interestingly, containers like lists and dicts always use the result of __repr__ to represent the objects they contain. Even if you call str on the container itself

print(str([my_car])) # [A shiny Blue car with milage of 21]
print(str(tuple([my_car]))) # (A shiny Blue car with milage of 21,)

But still it isn't very clear when we should go for __str__ and __repr__?
The result of the date object’s __str__ function should primarily be readable.
With __repr__, the idea is that its result should be, above all, unambiguous. The resulting string is intended more as a debugging aid for developers.
Eg: Let's check python's core lib datetime class

import datetime
today = datetime.date.today()
str(today) # '2021-02-21' - More readable clear concise output
repr(today) # 'datetime.date(2021, 2, 21)' 

We could copy and paste the string returned by __repr__ and execute it as valid Python to recreate the original date object. This is a neat approach and a good goal to keep in mind while writing your own reprs.

On the other hand, I find that it is quite difficult to put into practice. Usually it won’t be worth the trouble and it’ll just create extra work for you. My rule of thumb is to make my __repr__ strings unambiguous and helpful for developers, but I don’t expect them to be able to restore an object’s complete state.

Why every class needs a __repr__ 
----------------------------------
If you don’t add a __str__ method, Python falls back on the result of __repr__ when looking for __str__. Therefore, I recommend that you always add at least a __repr__ method to your classes. This will guarantee a useful string conversion result in almost all cases, with a minimum of implementation work.

If we define the __repr__ dunder in a class then in inner __str__ dunder also calls it to display the output. Hence it is always a good practise to add a __repr__ dunder with a class name and basic information in the class.

class Car:
    def __init__(self, color, milage):
        self.color = color
        self.milage = milage
    
    def __repr__(self):
        return f'{self.__class__.__name__}({self.color!r},{self.milage})'

my_car = Car('Blue', 21)
print(str(my_car))
print(repr(my_car))
print(my_car)

Note 4 things:
1. Please note that I’m using the !r conversion flag to make sure the output string uses repr(self.color) and repr(self.mileage) instead of str(self.color) and str(self.mileage).
2. str() also calls __repr__ because default __str__ drill down to __repr__
3. We have used __class__.__name__ dunder so that in future if the class name changes we do not have to modify __repr__ dunder. This supports our Don't Repeat Yourself (DRY) principle.
4. We can always have an optional __str__ dunder in the class


class Car:
    def __init__(self, color, milage):
        self.color = color
        self.milage = milage
    
    def __repr__(self):
        return f'{self.__class__.__name__}({self.color!r},{self.milage})'

    def __str__(self):
        return f'A shiny {self.color} car'

Google quants finance code also uses it extensively
  def __repr__(self):
    output = "PeriodTensor: shape={}".format(self.shape)
    if tf.executing_eagerly():
      return output + ", quantities={}".format(repr(self._quantity.numpy()))
    return output

Key Takeaways of repr()
========================
1. You can control to-string conversion in your own classes using the __str__ and __repr__ “dunder” methods.
2. The result of __str__ should be readable. The result of __repr__ should be unambiguous and debuggable.
3. Always add a __repr__ to your classes. The default implementation for __str__ just calls __repr__.

Defining your own Exception classes:
====================================
* It is always good to create your custom exception classes and raise it otherwise raising exceptions like ValueError can be vague and your colleague and future you might not able to understand the issue in one go.
* Whenever you’re publicly releasing a Python package, or even if you’re creating a reusable module for your company, it’s good practice to create a custom exception base class for the module and then derive all of your other exceptions from it.

Here’s how to create a custom exception hierarchy for all exceptions in a module or package. The first step is to declare a base class that all of our concrete errors will inherit from:

class BaseValidationError(ValueError):
    pass
class NameTooShortError(BaseValidationError):
    pass
class NameTooLongError(BaseValidationError):
    pass
class NameTooCuteError(BaseValidationError):
    pass

For example, this allows users of your package to write try…except statements that can handle all of the errors from this package without having to catch them manually:
def validate(name):
    if len(name) < 8:
        raise NameTooShortError
    
try:
    validate('joe')
except BaseValidationError as err:
    print('Error - ', err)
    
In conclusion, defining custom exception classes makes it easier for your users to adopt an it’s easier to ask for forgiveness than permission (EAFP) coding style that’s considered more Pythonic.

Key Takeaways

1. Defining your own exception types will state your code’s intent more clearly and make it easier to debug.
2. Derive your custom exceptions from Python’s built-in Exception class or from more specific exception classes like ValueError or KeyError.
3. You can use inheritance to define logically grouped exception hierarchies.

Cloning objects for Fun and Profit:
===================================
“Python’s built-in mutable collections like lists, dicts, and sets can be copied by calling their factory functions on an existing collection:

new_list = list(original_list)
new_dict = dict(original_dict)
new_set = set(original_set)

“However, this method won’t work for custom objects and, on top of that, it only creates shallow copies. For compound objects like lists, dicts, and sets, there’s an important difference between shallow and deep copying:

“A shallow copy means constructing a new collection object and then populating it with references to the child objects found in the original. In essence, a shallow copy is only one level deep. The copying process does not recurse and therefore won’t create copies of the child objects themselves.
The children are not copied in shallow copy they are merely referenced again in the copied object.

Eg:
l1=[10,20,[30,40],50]
l2=l1.copy() ## Shallow copy
print('l1', l1)
print('l2', l2)
l1[2][0]=888
print('l1', l1)
print('l2', l2)
print(id(l1[2])) # 4451347400
print(id(l2[2])) # 4451347400

l1 [10, 20, [30, 40], 50]
l2 [10, 20, [30, 40], 50]
l1 [10, 20, [888, 40], 50]
l2 [10, 20, [888, 40], 50]
4451347400
4451347400

A deep copy makes the copying process recursive. It means first constructing a new collection object and then recursively populating it with copies of the child objects found in the original. Copying an object this way walks the whole object tree to create a fully independent clone of the original object and all of its children.

Eg:
import copy
l1=[10,20,[30,40],50]
l2=copy.deepcopy(l1) ## Deep copy
print('l1', l1)
print('l2', l2)
l1[2][0]=888
print('l1', l1)
print('l2', l2)

l1 [10, 20, [30, 40], 50]
l2 [10, 20, [30, 40], 50]
l1 [10, 20, [888, 40], 50]
l2 [10, 20, [30, 40], 50]
-------------------------------------

“This is useful if you need to clearly communicate that you’re creating a shallow copy somewhere in your code. Using copy.copy() lets you indicate this fact. However, for built-in collections it’s considered more Pythonic to simply use the list, “dict, and set factory functions to create shallow copies.

Let's follow a example of copy methods on a custom class
import copy
class point:
    def __init__(self,p1,p2):
        self.p1 = p1
        self.p2 = p2
    def __repr__(self):
        return (f'{self.__class__.__name__} ({self.p1!r}, {self.p2!r})')
    
class rectangle:
    def __init__(self, topright, bottomleft):
        self.topright = topright
        self.bottomleft = bottomleft
    def __repr__(self):
        return (f'{self.__class__.__name__} ({self.topright!r}, {self.bottomleft!r})')

rect = rectangle(point(1,2), point(3,4))
srect = copy.copy(rect)
drect = copy.deepcopy(rect)
print('rect - ', rect)
print('srect - ', srect)
print('drect - ', drect)
rect.topright.p1 = 11
print('rect - ', rect)
print('srect - ', srect)
print('drect - ', drect)

rect -  rectangle (point (1, 2), point (3, 4))
srect -  rectangle (point (1, 2), point (3, 4))
drect -  rectangle (point (1, 2), point (3, 4))
rect -  rectangle (point (11, 2), point (3, 4))
srect -  rectangle (point (11, 2), point (3, 4))
drect -  rectangle (point (1, 2), point (3, 4))

-----------------------------------------------------

Key Takeaways from Shallow and Deep Copy
=========================================
1. Making a shallow copy of an object won’t clone child objects. Therefore, the copy is not fully independent of the original.
2. A deep copy of an object will recursively clone child objects. The clone is fully independent of the original, but creating a deep copy is slower.
3. You can copy arbitrary objects (including custom classes) with the copy module.

Abstract Base Classes (abc module) Keep inheritance in check
-------------------------------------------------------------
While writing code which can be maintainable in future, the designer/developer has to think of many things.
Like:
1. Have a base class which cannot be instantiated for abstraction purpose
2. All the child classes of the base class should implement all the methods of base class.

abc module is the most widely used module to cater to this problem, it can be seen in any google python code.

In order to make the base class as non instantiable we can use either of the two options
from abc import ABC, ABCMeta
class BaseClass(ABC) ## Widely used
class BaseClass(metaclass = ABCMeta)

To make the method as abstract method in base class so that if the method is not overridden in child class then even the child class cannot be instantiated.

Eg: Case 1 -
Base class cannot be instantiated 

Eg: 
from abc import ABC, abstractmethod
class Shape(ABC):
    @abstractmethod
    def area(self):
        pass    
Shape() ## TypeError: Can't instantiate abstract class Shape with abstract methods area

Case 2 - 
Child class cannot be instantiated without overriding each method of parent class
Eg: 
from abc import ABC, abstractmethod

class Shape(ABC):
    @abstractmethod
    def area(self):
        pass

class Rectangle(Shape):
    def __init__(self, len, bre):
        self.len = len
        self.bre = bre
    
Rectangle(10,20) # TypeError: Can't instantiate abstract class Rectangle with abstract methods area

Case 3 - when everything is defined properly
from abc import ABC, abstractmethod, ABCMeta

class Shape(metaclass = ABCMeta):
    def __init__(self, le, bre):
        pass
    @abstractmethod
    def area(self):
        pass

class Rectangle(Shape):
    def __init__(self, le, bre):
        self.le = le
        self.bre = bre
    def area(self):
        return self.le * self.bre
    
print(Rectangle(10,20).area()) ## 200

Key Takeaways of Abstract base class
===========================
Abstract Base Classes (ABCs) ensure that derived classes implement particular methods from the base class at instantiation time.
Using ABCs can help avoid bugs and make class hierarchies easier to maintain.

An abstract class can have both abstract and non-abstract methods. When we subclass an abstract class it is not necessary to override non-abstract methods.


What Named Tuples are good for: Very widely used in Google code base
-------------------------------
Namedtuples can be a great alternative to defining a class manually.

What’s a namedtuple and what makes it so special? A good way to think about namedtuples is to view them as an extension of the built-in tuple data type.

Python’s tuples are a simple data structure for grouping arbitrary objects. Tuples are also immutable—they cannot be modified once they’ve been created.

One downside of plain tuples is that the data you store in them can only be pulled out by accessing it through integer indexes. You can’t give names to individual properties stored in a tuple. This can impact code readability.

Namedtuples aim to solve these two problems.

First of all, namedtuples are immutable containers, just like regular tuples. Once you store data in top-level attribute on a namedtuple, you can’t modify it by updating the attribute. All attributes on a namedtuple object follow the “write once, read many” principle.

Each object stored in them can be accessed through a unique (human-readable) identifier. This frees you from having to remember integer indexes, or resorting to workarounds like defining integer constants as mnemonics for your indexes.

from collections import namedtuple
Car = namedtuple('Car', 'color milage')

In the namedtuple factory takes first argument as the name of the class and next arguments as the instance variables of the class.

Why are we passing className inside the named tuple as we are already providing it outside before equals? This parameter is referred to as the “typename” in the Python docs. It’s the name of the new class that’s being created by calling the namedtuple function.

Now we have a class created, named as Car and has two instance variables color and milage.

The class name is used in the docstring and the __repr__ implementation that namedtuple automatically generates for us.

If you assign someother name before equals then there will be two references which will be pointing to same class object.
from collections import namedtuple
MatCar = namedtuple('Car', 'color milage')

Now both MatCar and Car are pointing to same class object.
Car ## __main__.Car
MatCar ## __main__.Car

We are passing the instance variable names as the shorthand to create a list, internally it is converted to a list like 
'color milage'.split() # ['color', 'milage']

We can also define the variables as a list
Car = namedtuple('Car', ['color', 'milage'])

We can now create the object of our newly created named tuple class
mycar = Car('red', 21)
mycar.color ## red

Besides accessing the values stored in a namedtuple by their identifiers, you can still access them by their index. That way, namedtuples can be used as a drop-in replacement for regular tuples:
mycar[0] # red
tuple(mycar) # ('red', 21)

Tuple unpacking works as expected
print(*mycar)

Object unpacking works as expected
col, mil = mycar
print(col, mil) # red 21

You’ll even get a nice string representation for your namedtuple object for free, which saves some typing and verbosity:
mycar # Car(color='red', milage=21)

Like tuples, namedtuples are immutable. When you try to overwrite one of their fields, you’ll get an AttributeError exception:
mycar.color = 'green' # AttributeError: can't set attribute
ast.literal_eval
A good way to view them is to think that namedtuples are a memory-efficient shortcut to defining an immutable class in Python manually.

Subclassing Namedtuples
========================
We can extend named tuple class and add methods, variables to it that way.
class MyClassWithMethods(Car):
    def hexcolor(self):
        if self.color == 'red':
            return '#ff0000'
        else:
            return '#000000'

MyClassWithMethods('red', 21).hexcolor()

However, this might be a little clunky. It might be worth doing if you want a class with immutable properties, but it’s also easy to shoot yourself in the foot here.

For example, adding a new immutable field is tricky because of how namedtuples are structured internally. The easiest way to create hierarchies of namedtuples is to use the base tuple’s _fields property:

Car = namedtuple('Car', 'color milage')
ElectricCar = namedtuple('ElectricCar', Car._fields + ('charge',))
ElectricCar('red',21,45.2) # ElectricCar(color='red', milage=21, charge=45.2)

Like _field property(variable), named tuple provides few more helper methods which can be useful.
It must be noted that all the properties and methods start with _ underscore character, _ normally signals that the variable or method is private but here it is used as public interface. _ The helpers were named that way to avoid naming collisions with user-defined tuple fields.

_asdict() helper method. It returns the contents of a namedtuple as a dictionary:
mycar._asdict() # OrderedDict([('color', 'red'), ('milage', 21)]) 
(### More about OrderedDict is described below)

This is great for avoiding typos in the field names when generating JSON-output, for example:
import json
json.dumps(mycar._asdict()) ## '{"color": "red", "milage": 21}'

Another useful helper is the _replace() function. It creates a (shallow) copy of a tuple and allows you to selectively replace some of its fields:
mycar._replace(color = 'blue') # Car(color='blue', milage=21)

Lastly, the _make() classmethod can be used to create new instances of a namedtuple from a sequence or iterable.
Car._make(['white', 12]) # Car(color='white', milage=12)

When to use Named Tuples:
-------------------------
Using namedtuples over unstructured tuples and dicts can also make my coworkers’ lives easier because they make the data being passed around “self-documenting” (to a degree).

On the other hand, I try not to use namedtuples for their own sake if they don’t help me write “cleaner” and more maintainable code. Like many other techniques shown in this book, sometimes there can be too much of a good thing.

However, if you use them with care, namedtuples can undoubtedly make your Python code better and more expressive.


Google Quants Finance Example - https://github.com/google/tf-quant-finance/blob/13336f99bd1cab56f783f38d320dddf3da00e7cc/tf_quant_finance/experimental/instruments/rates_common.py 
InterestRateMarket = collections.namedtuple(
    'InterestRateMarket',
    [
        # Instance of class RateCurve. The curve used for computing the forward
        # expectation of Libor rate.
        'reference_curve',
        # Instance of class RateCurve. The curve used for discounting cashflows.
        'discount_curve',
        # Scalar of real dtype containing the past fixing of libor rate
        'libor_rate',
        # Scalar of real dtype containing the past fixing of swap rate
        'swap_rate',
        # Instance of class VolatiltyCube. Market implied black volatilities.
        'volatility_curve'
    ])
InterestRateMarket.__new__.__defaults__ = (None, None, None, None, None)

Example to create a dataframe using namedtuple
Point = namedtuple('Point', 'x y')
DataFrame([Point(0,0), Point(0,1), Point(0,2)])

##### DataClass
----------------
Similar to NamedTuple we have dataclass which acts as a code generator and is a convenient way to create classes.
Unlike NamedTuple they are not immutable by default and have several features similar to NamedTuples.
Details - https://realpython.com/python-data-classes/

from dataclasses import dataclass
from typing import Any
@dataclass
class Car:
    color:str
    milage: Any # If you don't want to specify datatype that is completely fine

audi = Car('black', 11.1)
audi

You can also define a dataclass similar to namedtuple
from dataclasses import make_dataclass
NewCar = make_dataclass('NewCar', ['color', 'milage'])
benz = NewCar('White', 9)
benz

Like namedtuple internally many dunders like __repr__, __eq__, __hash__ will be generated automatically for you.
Hence it saves you a lot of time while writing a class and you can also customize features based on your requirement.

It can also be used to create dataframe in pandas.
from dataclasses import make_dataclass
Point = make_dataclass('Point', [('x', int), ('y', int)])
pd.DataFrame([Point(0, 0), Point(0,1), Point(0,2)])


### What is OrderedDict?
The OrderedDict is a subclass of dict object in Python. The only difference between OrderedDict and dict is that, in OrderedDict, it maintains the orders of keys as inserted. In the dict, the ordering may or may not  happen. It is part of collection module.
(SINCE PYTHON 3.6 insertion order in dict are preserved)

import collections
md = {'a':10, 'b':20, 'd':40, 'c':30, 'z':21}
od = collections.OrderedDict()
od['a'] = 10 
od['c'] = 20
od['b'] = 30
print(od) ## It is list of tuples - rderedDict([('a', 10), ('c', 20), ('b', 30)])
md # dict - {'a': 10, 'b': 20, 'd': 40, 'c': 30, 'z': 21}

import collections
md = {'a':10, 'b':20, 'd':40, 'c':30, 'z':21}
od = collections.OrderedDict()
od['a'] = 10 
od['c'] = 20
od['b'] = 30
print(od) ## It is list of tuples - OrderedDict([('a', 10), ('c', 20), ('b', 30)])
print(md) # dict - {'a': 10, 'b': 20, 'd': 40, 'c': 30, 'z': 21}

for item in md.items():
    print (item)
    
for item in od.items():
    print (item)


Key Takeaways from Named Tuples:
---------------------------------
* collection.namedtuple is a memory-efficient shortcut to manually define an immutable class in Python.
* Namedtuples can help clean up your code by enforcing an easier-to-understand structure on your data.
* Namedtuples provide a few useful helper methods that all start with a single underscore, but are part of the public interface. It’s okay to use them.

Class vs Instance Variable Pitfalls:
====================================
Key Takeaways
--------------
Class variables are for data shared by all instances of a class. They belong to a class, not a specific instance and are shared among all instances of a class.
Instance variables are for data that is unique to each instance. They belong to individual object instances and are not shared among the other instances of a class. Each instance variable gets a unique backing store specific to the instance.
Because class variables can be “shadowed” by instance variables of the same name, it’s easy to (accidentally) override class variables in a way that introduces bugs and odd behavior.

Eg: Increment the number when an object is created for a class
class CountObject:
    countO = 0
    def __init__(self):
        self.__class__.countO += 1
    def __repr__(self):
        return (f'{self.__class__.__name__}')
print (CountObject().countO)
print (CountObject().countO)
print (CountObject().countO)
print (CountObject().countO)
print (CountObject().countO)

Instance, Class and Static Methods Demystified:
================================================

class MyClass:
    def method(self):
        return 'instance method called', self
    @classmethod
    def classmethod(cls):
        return 'class method called', cls
    @staticmethod
    def staticmethod():
        return 'static method called'

obj = MyClass()

Instance Methods:
-----------------
Instance methods can also access the class itself through the self.__class__ attribute. This means instance methods can also modify class state.

Calling instance method
print(obj.method()) # ('instance method called', <__main__.MyClass object at 0x1081af7b8>)

MyClass was set up in such a way that each method’s implementation returns a tuple containing information we can use to trace what’s going on and which parts of the class or object that method can access.
This confirms that, in this case, the instance method called method has access to the object instance (printed as <MyClass instance>) via the self argument.
When the method is called, Python replaces the self argument with the instance object, obj. We could ignore the syntactic sugar provided by the obj.method() dot-call syntax and pass the instance object manually to get the same result:

print(MyClass.method(obj)) # ('instance method called', <__main__.MyClass object at 0x1080db7f0>)
Instance methods can also access the class itself through the self.__class__ attribute. This makes instance methods powerful in terms of access restrictions—they can freely modify state on the object instance and on the class itself.


Class Methods:
--------------
The class method only has access to this cls argument, it can’t modify object instance state. That would require access to self. However, class methods can still modify class state that applies across all instances of the class. 

print(obj.classmethod()) # ('class method called', <class '__main__.MyClass'>)

Calling classmethod() showed us that it doesn’t have access to the <MyClass instance> object, but only to the <class MyClass> object, representing the class itself (everything in Python is an object, even classes themselves).

Notice how Python automatically passes the class as the first argument to the function when we call MyClass.classmethod(). Calling a method in Python through the dot syntax triggers this behavior. The self parameter on instance methods works the same way.

Please note that naming these parameters self and cls is just a convention. You could just as easily name them the_object and the_class and get the same result. All that matters is that they’re positioned first in the parameter list for that particular method.


Static Methods:
This type of method doesn’t take a self or a cls parameter, although, of course, it can be made to accept an arbitrary number of other parameters.
As a result, a static method cannot modify object state or class state. Static methods are restricted in what data they can access—they’re primarily a way to namespace your methods.

print(obj.staticmethod()) #static method called

Did you see how we called staticmethod() on the object and were able to do so successfully? Some developers are surprised when they learn that it’s possible to call a static method on an object instance.

Behind the scenes, Python simply enforces the access restrictions by not passing in the self or the cls argument when a static method gets called using the dot syntax.

This confirms that static methods can neither access the object instance state nor the class state. They work like regular functions but belong to the class’ (and every instance’s) namespace.

Now, let’s take a look at what happens when we attempt to call these methods on the class itself, without creating an object instance beforehand:


print(MyClass.staticmethod()) # static method called
print(MyClass.classmethod()) # ('class method called', <class '__main__.MyClass'>)
print(MyClass.method()) # TypeError: method() missing 1 required positional argument: 'self'

We were able to call classmethod() and staticmethod() just fine, but attempting to call the instance method method() failed with a TypeError.

This is to be expected. This time we didn’t create an object instance and tried calling an instance 
function directly on the class blueprint itself. This means there is no way for Python to populate the self argument and therefore the call fails with a TypeError exception.

Class Method:
-------------
Below example shows how a class method can help to maintain the codebase, even if in future if the name of the class changes the internal code is not required to be changed as it is not dependent on self but cls.
For this we write factory methods which are cls methods
class Pizza:
    def __init__(self, ingredients):
        self.ingredients = ingredients
    def __repr__(self):
        return f'{__class__.__name__} {self.ingredients!r}'

    @classmethod
    def margherita(cls):
        return cls(['mozzarella', 'tomatoes'])
                    
    @classmethod
    def prosciutto(cls):
        return cls(['mozzarella', 'tomatoes', 'ham'])
    
Pizza.margherita()
Pizza.prosciutto()

Static Method:
It provides a way to use utils methods in class which are not part of instance or class.
This helps linking between object oriented and procedural programming styles.
It is easier to test Static methods as it is not dependent on class object or instance object.
Practical examples of static methods are when we want to parse some object like parsejson, formatdate.

import math
class Pizza:
    def __init__(self, ingredients, r):
        self.ingredients = ingredients
        self.r = r
    def __repr__(self):
        return f'{__class__.__name__} {self.ingredients!r} {self.r!r}'
    
    def area(self):
        return self.circle_area(self.r)
    
    @staticmethod
    def circle_area(r):
        return r ** 2 * math.pi
    
Pizza(['panner', 'farmhouse',], 4).area()

Google Quants Finance Example: https://github.com/google/tf-quant-finance/blob/742d370c4ff2d0a15eb2bdda2751711cba97262c/api_docs/tf_quant_finance/experimental/instruments/InterestRateMarket.md 

InterestRateMarket(reference_curve, discount_curve, libor_rate, swap_rate, volatility_curve)
@staticmethod
tf_quant_finance.experimental.instruments.InterestRateMarket(
    _cls, reference_curve=None, discount_curve=None, libor_rate=None,
    swap_rate=None, volatility_curve=None
)


Key Takeaways from instance, class and static methods
======================================================
Instance methods need a class instance and can access the instance through self.
Class methods don’t need a class instance. They can’t access the instance (self) but they have access to the class itself via cls.
Static methods don’t have access to cls or self. They work like regular functions but belong to the class’ namespace.
Static and class methods communicate and (to a certain degree) enforce developer intent about class design. This can have definite maintenance benefits.

Magic variables uses:
Before init the if __name__ block of code runs
class Pizza:
    def __init__(self):
        print(f'{__class__}')
        print(__name__)
      
    if __name__ == '__main__':
        print('Never eat pizza')
Pizza()
Output:
Never eat pizza
<class '__main__.Pizza'>
__main__
<__main__.Pizza at 0x10a1566d8>


Common DataStructures in Python:
=================================
dict - Your Go To Dictionary
----
Dictionaries are also often called maps, hashmaps, lookup tables, or associative arrays. They allow for the efficient lookup, insertion, and deletion of any object associated with a given key.

Dictionary can be defined in two ways:
1. curly brances expression # d = {'a':2, 'b':4, 'c':8}
2. dictionary comprehension # {i:i*i for i in np.arange(1,5)}

PyCon 2017 - https://www.youtube.com/watch?v=npw4s1QTmPg - Raymond Hettinger Modern Python Dictionaries

Even for class, behind each instance is an instance dictionary
class UserProperty:
    def __init__(self,p1,p2,p3):
        self.p1 = p1
        self.p2 = p2
        self.p3 = p3
    def __repr__(self):
        return f'{self.__class__.__name__}({self.p1!r},{self.p2!r},{self.p3!r})'

cols = UserProperty('blue','red','green')
langs = UserProperty('Java','python', 'kdb')

for prop in [cols,langs]:
    print(vars(prop)) ## vars() returns the __dict__ attribute of the given object.

Output:
{'p1': 'blue', 'p2': 'red', 'p3': 'green'}
{'p1': 'Java', 'p2': 'python', 'p3': 'kdb'}

Since python 3.6 dict keys are ordered i.e insertion order is preserved.

What is hashtable? Something that reduces the search space by cutting into smaller buckets. This is how we search for our seat in cinema hall.

identity implies equality - meaning if two references are pointing to the same object that means the object is equal to itself. It is true except for NaN
Hence, hashtable follow below rule and does not allow NaN
RULE - If two objects are equal then their hash values are equal as well.  Hence python follow, IF TWO OBJECTS HAVE UNEQUAL HASHES, THEN THE OBJECT MUST BE UNEQUAL AS WELL.
Hence for matching the keys faster:
def fastmatch(key, targetkey):
    if key is targetkey: return True             ## Fast
    if key.hash != targetkey.hash: return False  ## Fast
    return key == targetkey                      ## Slow

If these two 'if' are removed then python speed will reduce to half.

Dictionaries are highly optimized and underlie many parts of the language, for example class attributes and variables in a stack frame are both stored internally in dictionaries.

Python dictionaries are based on a well-tested and finely tuned hash table implementation that provides the performance characteristics you’d expect: O(1) time complexity for lookup, insert, update, and delete operations in the average case.

OrderedDict:
Insertion order of dict keys are preserved in ordered dict which is also avaiable in dict from 3.6
It is present in collections module.
import collections 
d = collections.OrderedDict(one=1, two=2, three=3)
print(d)
print(d['two'])
## Output
OrderedDict([('one', 1), ('two', 2), ('three', 3)])
2

collections.defaultDict
------------------------
Returns default values for missing keys
Defaultdict is a container like dictionaries present in the module collections. Defaultdict is a sub-class of the dict class that returns a dictionary-like object. The functionality of both dictionaries and defualtdict are almost same except for the fact that defualtdict never raises a KeyError. It provides a default value for the key that does not exists.
from collections import defaultdict

def def_value(): 
    return "Not Present"
      
# Defining the dict 
d = defaultdict(def_value) 
d["a"] = 1
d["b"] = 2
  
print(d["a"]) 
print(d["b"]) 
print(d["c"]) ## Not Present

collections.ChainMap:
---------------------
The collections.ChainMap data structure groups multiple dictionaries into a single mapping. Lookups search the underlying mappings one by one until a key is found. Insertions, updates, and deletions only affect the first mapping added to the chain.

from collections import ChainMap
d1 = {'one':1, 'two':2}
d2 = {'tre':3, 'for':4}
chain = ChainMap(d1,d2)
print(chain)
chain['tre']
#chain['three'] KeyError: 'three'
chain['five'] = 5
print(chain)
#del chain['tre'] # KeyError: "Key not found in the first mapping: 'tre'"
del chain['five']
print(chain)
Output:
ChainMap({'one': 1, 'two': 2}, {'tre': 3, 'for': 4})
ChainMap({'one': 1, 'two': 2, 'five': 5}, {'tre': 3, 'for': 4})
ChainMap({'one': 1, 'two': 2}, {'tre': 3, 'for': 4})

ChainMap searches each collection in the chain from left to right until it finds the key (or fails)

type.MappingProxyTypes:
-----------------------
“types.MappingProxyType – A Wrapper for Making Read-Only Dictionaries

MappingProxyType is a wrapper around a standard dictionary that provides a read-only view into the wrapped dictionary’s data.7 This class was added in Python 3.3, and it can be used to create immutable proxy versions of dictionaries.

For example, this can be helpful if you’d like to return a dictionary carrying internal state from a class or module, while discouraging write access to this object. Using MappingProxyType allows you to put these restrictions in place without first having to create a full copy of the dictionary.

>>> from types import MappingProxyType
>>> writable = {'one': 1, 'two': 2}
>>> read_only = MappingProxyType(writable)

# The proxy is read-only:
>>> read_only['one']
1
>>> read_only['one'] = 23
TypeError:
"'mappingproxy' object does not support item assignment"

# Updates to the original are reflected in the proxy:
>>> writable['one'] = 42
>>> read_only
mappingproxy({'one': 42, 'two': 2})

Key Takeaways for dictionaries
==============================
Dictionaries are the central data structure in Python.
The built-in dict type will be “good enough” most of the time.
Specialized implementations, like read-only or ordered dicts, are available in the Python standard library.

Array Data Structure:
---------------------
How do arrays work, and what are they used for?
Arrays consist of fixed-size data records that allow each element to be efficiently located based on its index.
Because arrays store information in adjoining blocks of memory, they’re considered contiguous data structures (as opposed to linked datas structure like linked lists, for example.)

LIST – Mutable Dynamic Arrays
-----------------------------
Lists are a part of the core Python language.8 Despite their name, Python’s lists are implemented as dynamic arrays behind the scenes. This means a list allows elements to be added or removed, and the list will automatically adjust the backing store that holds these elements by allocating or releasing memory.
Python lists can hold arbitrary elements—“everything” is an object in Python, including functions. Therefore, you can mix and match different kinds of data types and store them all in a single list.
This can be a powerful feature, but the downside is that supporting multiple data types at the same time means that data is generally less tightly packed. And as a result, the whole structure takes up more space.

np.array are similar to list which are mutable and can take heterogeneous objects as input.

TUPLE – Immutable Containers
-----------------------------
Just like lists, tuples are also a part of the Python core language.9 Unlike lists, however, Python’s tuple objects are immutable. This means elements can’t be added or removed dynamically—all elements in a tuple must be defined at creation time.
Just like lists, tuples can hold elements of arbitrary data types. Having this flexibility is powerful, but again, it also means that data is less tightly packed than it would be in a typed array.

array.array – Basic Typed Arrays
---------------------------------
Python’s array module provides space-efficient storage of basic C-style data types like bytes, 32-bit integers, floating point numbers, and so on.

Arrays created with the array.array class are mutable and behave similarly to lists, except for one important difference—they are “typed arrays” constrained to a single data type.10

Because of this constraint, array.array objects with many elements are more space-efficient than lists and tuples. The elements stored in them are tightly packed, and this can be useful if you need to store many elements of the same type.

Also, arrays support many of the same methods as regular lists, and you might be able to use them as a “drop-in replacement” without requiring other changes to your application code.
Arrays are mutable in nature

Eg:
import array
a = array.array('f',(1,2,3.0))
print(a) # array('f', [1.0, 2.0, 3.0])
print(a[2]) # 3.0
a[1] = 200
print(a) # array('f', [1.0, 200.0, 3.0])

str - Immutable array of Unicode characters
--------------------------------------------
Python 3.x uses str objects to store textual data as immutable sequences of Unicode characters.11 Practically speaking, that means a str is an immutable array of characters. Oddly enough, it’s also a recursive data structure—each character in a string is a str object of length 1 itself.

String objects are space-efficient because they’re tightly packed and they specialize in a single data type. If you’re storing Unicode text, you should use them. Because strings are immutable in Python, modifying a string requires creating a modified copy. The closest equivalent to a “mutable string” is storing individual characters inside a list.

>>> arr = 'abcd'
>>> arr[1]
'b'
>>> arr
'abcd'
## Strings are immutable
>>> arr[1] = 'e' ## TypeError: 'str' object does not support item assignment
>>> del arr[1] ## TypeError: 'str' object doesn't support item deletion

Strings can be unpacked into a list to get a mutable representation.
ls = list('abcd')
>>> ls
['a', 'b', 'c', 'd']
>>> ''.join(ls)
'abcd'
>>> mutable_string = ''.join(ls)
>>> mutable_string
'abcd'
>>> mutable_string[1] = 'e' ## TypeError: 'str' object does not support item assignment

## Strings are recursive data structure
>>> type('abc')
<class 'str'>
>>> type('a')
<class 'str'>
>>> type('abc'[1])
<class 'str'>

bytes - Immutable array of single bytes:
Bytes objects are immutable sequences of single bytes (integers in the range of 0 <= x <= 255).12 Conceptually, they’re  similar to str objects and we can also think of them as array of bytes.
Like strings, bytes have their own literal syntax for creating objects and they’re space-efficient. Bytes objects are immutable, but unlike strings, there’s a dedicated “mutable byte array” data type called bytearray that they can be unpacked into. 

>>> arr
b'\x00\x01\x02'
>>> arr = b'\x00\x01\x02'

bytes are immutable
>>> arr[1] = 23 ## TypeError: 'bytes' object does not support item assignment
>>> del arr[1] ## TypeError: 'bytes' object doesn't support item deletion

bytearray - Mutable arrays of single bytes
-------------------------------------------
The bytearray type is a mutable sequence of integers in the range 0 <= x <= 255.13 They’re closely related to bytes objects with the main difference being that bytearrays can be modified freely—you can overwrite elements, remove existing elements, or add new ones. The bytearray object will grow and shrink accordingly.

Bytearrays can be converted back into immutable bytes objects but this involves copying the stored data in full—a slow operation taking O(n) time.

>>> arr = bytearray((1,2,3,4))
>>> arr[1] ## 2
>>> arr ## bytearray(b'\x01\x02\x03\x04')

bytearrays are mutable in nature
>>> arr[1] = 121
>>> arr ## bytearray(b'\x01y\x03\x04')
>>> arr[1] ## 121
>>> del arr[1]
>>> arr.append(242)
>>> arr[-1] ## 242

Byte array can hold values in the range 0-255
>>> arr[1] = 'hello' ## TypeError: an integer is required
>>> arr[1] = 300 ## ValueError: byte must be in range(0, 256)

## Byte array can be converted into bytes
>>> arr ## bytearray(b'\x01\x03\x04\xf2')
>>> bytes(arr) ## b'\x01\x03\x04\xf2'

Key takeaways from list, tuple, array, bytes, bytearray:
-----------------------------------------------------------
You need to store arbitrary objects, potentially with mixed data types? Use a list or a tuple, depending on whether you want an immutable data structure or not.

You have numeric (integer or floating point) data and tight packing and performance is important? Try out array.array and see if it does everything you need. Also, consider going beyond the standard library and try out packages like NumPy or Pandas.

You have textual data represented as Unicode characters? Use Python’s built-in str. If you need a “mutable string,” use a list of characters.

You want to store a contiguous block of bytes? Use the immutable bytes type, or bytearray if you need a mutable data structure.

In most cases, I like to start out with a simple list. I’ll only specialize later on if performance or storage space becomes an issue. Most of the time, using a general-purpose array data structure like list gives you the fastest development speed and the most programming convenience.

dict - Simple Data Objects:
---------------------------
Python dictionaries store an arbitrary number of objects, each identified by a unique key. Dictionaries are also often called maps or associative arrays and allow for the efficient lookup, insertion, and deletion of any object associated with a given key.
Data objects created using dictionaries are mutable, and there’s little protection against misspelled field names, as fields can be added and removed freely at any time. Both of these properties can introduce surprising bugs, and there’s always a trade-off to be made between convenience and error resilience.

car1 = {
    'color': 'red',
    'mileage': 3812.4,
    'automatic': True,
}
car2 = {
    'color': 'blue',
    'mileage': 40231,
    'automatic': False,
}

# Dicts have a nice repr:
>>> car2
{'color': 'blue', 'automatic': False, 'mileage': 40231}

# Get mileage:
>>> car2['mileage']
40231

# Dicts are mutable:
>>> car2['mileage'] = 12
>>> car2['windshield'] = 'broken

>>> car2
{'windshield': 'broken', 'color': 'blue',
 'automatic': False, 'mileage': 12}

# No protection against wrong field names,
# or missing/extra fields:
car3 = {
    'colr': 'green',
    'automatic': False,
    'windshield': 'broken',
}

tuple - Immutable group of objects
----------------------------------
Tuples are immutable—they cannot be modified once they’ve been created.
Performance-wise, tuples take up slightly less memory than lists in CPython,17 and they’re also faster to construct.

A potential downside of plain tuples is that the data you store in them can only be pulled out by 
accessing it through integer indexes. You can’t give names to individual properties stored in a tuple. This can impact code readability.(You can always use named tuples)

# Fields: color, mileage, automatic
>>> car1 = ('red', 3812.4, True)
>>> car2 = ('blue', 40231.0, False)

# Tuple instances have a nice repr:
>>> car1
('red', 3812.4, True)
>>> car2
('blue', 40231.0, False)

# Get mileage:
>>> car2[1]
40231.0

# Tuples are immutable:
>>> car2[1] = 12
TypeError:
"'tuple' object does not support item assignment"

# No protection against missing/extra fields
# or a wrong order:
>>> car3 = (3431.5, 'green', True, 'silver')

Writing a Custom Class, More work more control:
-----------------------------------------------
Classes allow you to define reusable “blueprints” for data objects to ensure each object provides the same set of fields.

Using regular Python classes as record data types is feasible, but it also takes manual work to get the convenience features of other implementations. For example, adding new fields to the __init__ constructor is verbose and takes time.

Also, the default string representation for objects instantiated from custom classes is not very helpful. To fix that you may have to add your own __repr__ method,18 which again is usually quite verbose and must be updated every time you add a new field.

Fields stored on classes are mutable, and new fields can be added freely, which you may or may not like. It’s possible to provide more access control and to create read-only fields using the @property decorator,19 but once again, this requires writing more glue code.

Writing a custom class is a great option whenever you’d like to add business logic and behavior to your record objects using methods. However, this means that these objects are technically no longer plain data objects.

class Car:
    def __init__(self, color, milage, automatic):
        self.color = color
        self.milage = milage
        self.automatic = automatic
    def __repr__(self):
        return (f'{self.__class__.__name__} ({self.color!r} {self.milage!r} {self.automatic!r})')
    
car1 = Car('Red', 21, True)
car2 = Car('Blue', 23, False)

print(car2.milage) # 23
print(car1) # Car ('Red' 21 True)

## Classes are Mutable
car1.windshield = 'broken'
print(car1.windshield) ## broken

## You need to update __repr__ for every new field
print(car1) # Car ('Red' 21 True) 

collections.namedtuple - Convenient Data Objects
-------------------------------------------------
The namedtuple class available in Python 2.6+ provides an extension of the built-in tuple data type.20 Similar to defining a custom class, using namedtuple allows you to define reusable “blueprints” for your records that ensure the correct field names are used.

Namedtuples are immutable, just like regular tuples. This means you cannot add new fields or modify existing fields after the namedtuple instance was created.

Besides that, namedtuples are, well… named tuples. Each object stored in them can be accessed through a unique identifier. This frees you from having to remember integer indexes, or resort to workarounds like defining integer constants as mnemonics for your indexes.

Namedtuple objects are implemented as regular Python classes internally. When it comes to memory usage, they are also “better” than regular classes and just as memory efficient as regular tuples:

from collections import namedtuple
from sys import getsizeof

print((namedtuple('Point', 'x y z')(1,2,3))) # Point(x=1, y=2, z=3)
print(getsizeof((namedtuple('Point', 'x y z')(1,2,3)))) # 72
print(getsizeof((1,2,3))) # 72

Namedtuples can be an easy way to clean up your code and make it more readable by enforcing a better structure for your data.

I find that going from ad-hoc data types, like dictionaries with a fixed format, to namedtuples helps me express the intent of my code more clearly. Often when I apply this refactoring, I magically come up with a better solution for the problem I’m facing.

Using namedtuples over regular (unstructured) tuples and dicts can also make my coworkers’ lives easier: Namedtuples make the data that’s being passed around “self-documenting”, at least to a degree.

from collections import namedtuple
Car = namedtuple('Car', 'color milage automatic')

car1 = Car('Blue', 21,'False')

# Instance have a nice representation
car1 # Car(color='Blue', milage=21, automatic='False')

# Accessing Fields
car1.milage # 21

# Fields are immutable
car1.milage = 12 # AttributeError: can't set attribute
car1.windshield = 'broken' # AttributeError: 'Car' object has no attribute 'windshield'

typing.NamedTuple – Improved Namedtuples
=========================================
This class added in Python 3.6 is the younger sibling of the namedtuple class in the collections module.21 It is very similar to namedtuple, the main difference being an updated syntax for defining new record types and added support for type hints.

Please note that type annotations are not enforced without a separate type-checking tool like mypy.22 But even without tool support, they can provide useful hints for other programmers (or be terribly confusing if the type hints become out-of-date.)

from typing import NamedTuple

class Car(NamedTuple):
    color:str
    milage:float
    automatic:bool
        
car1 = Car('red', 21.1, True)

## Instance have a nicer representation
car1 # Car(color='red', milage=21.1, automatic=True)

## Accessing Fields
car1.milage # 21.1

## Fields are immutable
#car1.milage = 22.3 # AttributeError: can't set attribute
#car1.windshield = 'broken' # AttributeError: 'Car' object has no attribute 'windshield'

## Type annotations are not enforced without
## a separate type checking tool like mypy
Car('Green', 'NOT_A_FLOAT', False) ## Car(color='Green', milage='NOT_A_FLOAT', automatic=False)

struct.Struct – Serialized C Structs
=====================================
The struct.Struct class23 converts between Python values and C structs serialized into Python bytes objects. For example, it can be used to handle binary data stored in files or coming in from network connections.

Structs are defined using a format strings-like mini language that allows you to define the arrangement of various C data types like char, int, and long, as well as their unsigned variants.

Serialized structs are seldom used to represent data objects meant to be handled purely inside Python code. They’re intended primarily as a data exchange format, rather than as a way of holding data in memory that’s only used by Python code.

In some cases, packing primitive data into structs may use less memory than keeping it in other data types. However, in most cases that would be quite an advanced (and probably unnecessary) optimization.

from struct import Struct
MyStruct = Struct('i?f')
data = MyStruct.pack(23, False, 42.1)

# All you get is blob of data
print(data) # b'\x17\x00\x00\x00\x00\x00\x00\x00ff(B'

# Data blobs can be unpacked again using unpack
MyStruct.unpack(data) # (23, False, 42.099998474121094)

types.SimpleNamespace – Fancy Attribute Access
===============================================
Here’s one more “esoteric” choice for implementing data objects in Python: types.SimpleNamespace.24 This class was added in Python 3.3 and it provides attribute access to its namespace.

This means SimpleNamespace instances expose all of their keys as class attributes. This means you can use obj.key “dotted” attribute access instead of the obj['key'] square-brackets indexing syntax that’s used by regular dicts. All instances also include a meaningful __repr__ by default.

As its name proclaims, SimpleNamespace is simple! It’s basically a glorified dictionary that allows attribute access and prints nicely. Attributes can be added, modified, and deleted freely.

from types import SimpleNamespace

car1 = SimpleNamespace(color = 'red', milage = '21.1', automatic = False)
## Default Representation
print (car1) # namespace(automatic=False, color='red', milage='21.1')

## Instance support attribute access and are mutable
print (car1.color) # 'red'
car1.atomatic = True
print(car1.automatic) # False
car1.windshield = 'broken'
print(car1) #namespace(automatic=False, color='red', milage='21.1', windshield='broken')

Key Takeaways from dict, custom class, tuple, namedtuple, NamedTuple, Struct, SimpleNamespace:
----------------------------------------------------------------------------------------------
You only have a few (2-3) fields: Using a plain tuple object may be okay if the field order is easy to remember or field names are superfluous. For example, think of an (x, y, z) point in 3D space.

You need immutable fields: In this case, plain tuples, collections.namedtuple, and typing.NamedTuple would all make good options for implementing this type of data object.

You need to lock down field names to avoid typos: collections.namedtuple and typing.NamedTuple are your friends here.

You want to keep things simple: A plain dictionary object might be a good choice due to the convenient syntax that closely resembles JSON.

You need full control over your data structure: It’s time to write a custom class with @property setters and getters.

You need to add behavior (methods) to the object: You should write a custom class, either from scratch or by extending collections.namedtuple or typing.NamedTuple.

You need to pack data tightly to serialize it to disk or to send it over the network: Time to read up on struct.Struct because this is a great use case for it.

If you’re looking for a safe default choice, my general recommendation for implementing a plain record, struct, or data object in Python would be to use collections.namedtuple in Python 2.x and its younger sibling, typing.NamedTuple in Python 3.

@property decorator:
--------------------
Whenever we write a custom class over which we want to have complete control we should use @property decorator.
It provides the benifits of data hiding and abstraction through setters and getters.

In Python, getters and setters are not the same as those in other object-oriented programming languages. Basically, the main purpose of using getters and setters in object-oriented programs is to ensure data encapsulation. Private variables in python are not actually hidden fields like in other object oriented languages. Getters and Setters in python are often used when:

We use getters & setters to add validation logic around getting and setting a value.
To avoid direct access of a class field i.e. private variables cannot be accessed directly or modified by external user.

We also have property method which provides the same benifits but is not highly used.

Example using property function:
class Geek:
    def __init__(self):
        self._age = 0
    
    def get_age(self):
        return self._age
    
    def set_age(self, age):
        self._age = age
        
    def del_age(self):
        del self._age
        
    age = property(get_age, set_age, del_age)
    
g = Geek()
g.age = 21
g.get_age()

Example using property decorator
class Geek:
    def __init__(self):
        self._age = 0
    @property
    def age(self):
        return self._age
    @age.setter
    def age(self, age):
        self._age = age
    
g = Geek()
g.age = 21
g.age

NOTE  - WHILE CALLING g.age YOU ARE NOT CALLING METHOD BUT VARIABLE.
ALSO NOTE - @property  GET METHOS USUALLY RETURNS PRIVATE VARIABLE(WHICH STARTS WITH _)

Google example of property methods - https://github.com/google/tf-quant-finance/blob/master/tf_quant_finance/experimental/instruments/interest_rate_swap.py

  @property
  def term(self):
    return tf.cast(self._start_date.days_until(self._maturity_date),
                   dtype=self._dtype) / 365.

method call - rate_term = fra_proto.floating_rate_term.term

Sets and MultiSets:
-------------------
Set
====
A set is an unordered collection of objects that does not allow duplicate elements. Typically, sets are used to quickly test a value for membership in the set, to insert or delete new values from a set, and to compute the union or intersection of two sets.

In a “proper” set implementation, membership tests are expected to run in fast O(1) time. Union, intersection, difference, and subset operations should take O(n) time on average. 

Just like dictionaries, sets get special treatment in Python and have some syntactic sugar that makes them easy to create. For example, the curly-braces set expression syntax and set comprehensions allow you to conveniently define new set instances

But be careful: To create an empty set you’ll need to call the set() constructor. Using empty curly-braces {} is ambiguous and will create an empty dictionary instead.

“The set type is mutable and allows for the dynamic insertion and deletion of elements.

Python’s sets are backed by the dict data type and share the same performance characteristics. Any hashable object can be stored in a set.

frozenset - 
==========
The frozenset class implements an immutable version of set that cannot be changed after it has been constructed.28 Frozensets are static and only allow query operations on their elements (no inserts or deletions.) Because frozensets are static and hashable, they can be used as dictionary keys or as elements of another set, something that isn’t possible with regular (mutable) set objects.

vowels = frozenset({'a','e','i','o','u'})
vowels.add('p')

## frozensets can be used as dict key
d = {frozenset({1,2,3}):'hello'}
d # {frozenset({1, 2, 3}): 'hello'}

collections.Counter: - Multisets
--------------------
The collections.Counter class in the Python standard library implements a multiset (or bag) type that allows elements in the set to have more than one occurrence.29

This is useful if you need to keep track of not only if an element is part of a set, but also how many times it is included in the set.
from collections import Counter
inventory = Counter()
loot = {'sword': 1, 'bread': 3}
print(type(loot)) # <class 'dict'>
inventory.update(loot)
print(inventory) # Counter({'bread': 3, 'sword': 1})
print(type(inventory)) # <class 'collections.Counter'>
more_loot = {'apple':1, 'sword':2}
inventory.update(more_loot)
print(inventory) # Counter({'sword': 3, 'bread': 3, 'apple': 1})

Here’s a caveat for the Counter class: You’ll want to be careful when counting the number of elements in a Counter object. Calling len() returns the number of unique elements in the multiset, whereas the total number of elements can be retrieved using the sum function:

print(len(inventory)) # 3
print(sum(inventory.values())) # 7

Key Takeaways on set frozenset and collection.Counter
======================================================
Sets are another useful and commonly used data structure included with Python and its standard library.
Use the built-in set type when looking for a mutable set.
frozenset objects are hashable and can be used as dictionary or set keys.
collections.Counter implements multiset or “bag” data structures.

Stacks and Queues:
------------------
Stack - LIFO
Queue - FIFO

A stack is a collection of objects that supports fast last-in, first-out (LIFO) semantics for inserts and 
deletes. Unlike lists or arrays, stacks typically don’t allow for random access to the objects they contain. The insert and delete operations are also often called push and pop.

Stacks and queues are similar. They’re both linear collections of items, and the difference lies in the order that the items are accessed:

With a queue, you remove the item least recently added (first-in, first-out or FIFO); but with a stack, you remove the item most recently added (last-in, first-out or LIFO).

Performance-wise, a proper stack implementation is expected to take O(1) time for insert and delete operations.

Stacks have a wide range of uses in algorithms, for example, in language parsing and runtime memory management (“call stack”). A short and beautiful algorithm using a stack is depth-first search (DFS) on a tree or graph data structure.

Python ships with several stack implementations that each have slightly different characteristics.

list - Simple, Built in Stack ## Equivalent to ArrayList in Java
-----------------------------
Python’s built-in list type makes a decent stack data structure as it supports push and pop operations in amortized O(1) time.30

Python’s lists are implemented as dynamic arrays internally, which means they occasionally need to resize the storage space for elements stored in them when elements are added or removed. The list over-allocates its backing storage so that not every push or pop requires resizing, and as a result, you get an amortized O(1) time complexity for these operations.

The downside is that this makes their performance less consistent than the stable O(1) inserts and deletes provided by a linked list based implementation (like collections.deque, see below). On the other hand, lists do provide fast O(1) time random access to elements on the stack, and this can be an added benefit.

Here’s an important performance caveat you should be aware of when using lists as stacks:

To get the amortized O(1) performance for inserts and deletes, new items must be added to the end of the list with the append() method and removed again from the end using pop(). For optimum performance, stacks based on Python lists should grow towards higher indexes and shrink towards lower ones.

Adding and removing from the front is much slower and takes O(n) time, as the existing elements must be shifted around to make room for the new element. This is a performance antipattern that you should avoid as much as possible.

s = []
s.append('ramesh')
s.append('suresh')
s.append('chandu')
print(s) # ['ramesh', 'suresh', 'chandu']

s.pop() # 'chandu'
s.pop() # 'suresh'
s.pop() # 'ramesh'

s.pop() # IndexError: pop from empty list

collections.deque – Fast & Robust Stacks -- LinkedList 
========================================================
The deque class implements a double-ended queue that supports adding and removing elements from either end in O(1) time (non-amortized). Because deques support adding and removing elements from either end equally well, they can serve both as queues and as stacks.31

Python’s deque objects are implemented as doubly-linked lists which gives them excellent and consistent performance for inserting and deleting elements, but poor O(n) performance for randomly accessing elements in the middle of a stack

Overall, collections.deque is a great choice if you’re looking for a stack data structure in Python’s standard library that has the performance characteristics of a linked-list implementation.

from collections import deque
s = deque()
s.append('ramesh')
s.append('suresh')
s.append('chandu')
print(s) # ['ramesh', 'suresh', 'chandu']

print(s.pop()) # 'chandu'
print(s.pop()) # 'suresh'
print(s.pop()) # 'ramesh'

s.pop() # IndexError: pop from an empty deque

queue.LifoQueue – Locking Semantics for Parallel Computing
==========================================================
This stack implementation in the Python standard library is synchronized and provides locking semantics to support multiple concurrent producers and consumers.33

Besides LifoQueue, the queue module contains several other classes that implement multi-producer/multi-consumer queues that are useful for parallel computing.

Depending on your use case, the locking semantics might be helpful, or they might just incur unneeded overhead. In this case you’d be better off with using a list or a deque as a general-purpose stack.

from queue import LifoQueue
s = LifoQueue()
s.put('ramesh')
s.put('suresh')
s.put('chandu')
print(s) # ['ramesh', 'suresh', 'chandu']

print(s.get()) # 'chandu'
print(s.get()) # 'suresh'
print(s.get()) # 'ramesh'

s.get() # # Blocks / waits forever...

Comparing Stack Implementations in Python
==========================================
“As you’ve seen, Python ships with several implementations for a stack data structure. All of them have slightly different characteristics, as well as performance and usage trade-offs.

If you’re not looking for parallel processing support (or don’t want to handle locking and unlocking manually), your choice comes down to the built-in list type or collections.deque. The difference lies in the data structure used behind the scenes and overall ease of use:

list is backed by a dynamic array which makes it great for fast random access, but requires occasional resizing when elements are added or removed. The list over-allocates its backing storage so that not every push or pop requires resizing, and you get an amortized O(1) time complexity for these operations. But you do need to be careful to only insert and remove items “from the right side” using append() and pop(). Otherwise, performance slows down to O(n).

“collections.deque is backed by a doubly-linked list which optimizes appends and deletes at both ends and provides consistent O(1) performance for these operations. Not only is its performance more stable, the deque class is also easier to use because you don’t have to worry about adding or removing items from “the wrong end.”

In summary, I believe that collections.deque is an excellent choice for implementing a stack (LIFO queue) in Python.

Key Takeaways from list, deque and LifoQueue
=============================================
Python ships with several stack implementations that have slightly different performance and usage characteristics.
collections.deque provides a safe and fast general-purpose stack implementation.
The built-in list type can be used as a stack, but be careful to only append and remove items with append() and pop() in order to avoid slow performance.

Queue FIFO:
==========
A queue is a collection of objects that supports fast first-in, first-out (FIFO) semantics for inserts and deletes. The insert and delete operations are sometimes called enqueue and dequeue. Unlike lists or arrays, queues typically don’t allow for random access to the objects they contain.

Analogy - New items (water molecules, ping-pong balls, …) are put in at one end and travel to the other where you or someone else removes them again. While the items are in the queue (a solid metal pipe) you can’t get at them. The only way to interact with the items in the queue is to add new items at the back (enqueue) or to remove items at the front (dequeue) of the pipe.

With a queue, you remove the item least recently added (first-in, first-out or FIFO); but with a stack, you remove the item most recently added (last-in, first-out or LIFO).

Performance-wise, a proper queue implementation is expected to take O(1) time for insert and delete operations. These are the two main operations performed on a queue, and in a correct implementation, they should be fast.

Queues have a wide range of applications in algorithms and often help solve scheduling and parallel programming problems. A short and beautiful algorithm using a queue is breadth-first search (BFS) on a tree or graph data structure.

“list — Terribly Sloooow Queues”
==================================
It’s possible to use a regular list as a queue but this is not ideal from a performance perspective.34 Lists are quite slow for this purpose because inserting or deleting an element at the beginning requires shifting all of the other elements by one, requiring O(n) time.

Therefore, I would not recommend using a list as a makeshift queue in Python (unless you’re only dealing with a small number of elements).

“>>> q = []
>>> q.append('eat')
>>> q.append('sleep')
>>> q.append('code')

>>> q
['eat', 'sleep', 'code']

# Careful: This is slow!
>>> q.pop(0)
'eat”

collections.deque – Fast & Robust Queues
=========================================
The deque class implements a double-ended queue that supports adding and removing elements from either end in O(1) time (non-amortized). Because deques support adding and removing elements from either end equally well, they can serve both as queues and as stacks.35

Python’s deque objects are implemented as doubly-linked lists.36 This gives them excellent and consistent performance for inserting and deleting elements, but poor O(n) performance for randomly accessing elements in the middle of the stack.

As a result, collections.deque is a great default choice if you’re looking for a queue data structure in Python’s standard library.

>>> from collections import deque
>>> q = deque()
>>> q.append('eat')
>>> q.append('sleep')
>>> q.append('code')

>>> q
deque(['eat', 'sleep', 'code'])

>>> q.popleft()
'eat'
>>> q.popleft()
'sleep'
>>> q.popleft()
'code'

>>> q.popleft()
IndexError: "pop from an empty deque

“multiprocessing.Queue – Shared Job Queues

This is a shared job queue implementation that allows queued items to be processed in parallel by multiple concurrent workers.38 Process-based parallelization is popular in CPython due to the global interpreter lock (GIL) that prevents some forms of parallel execution on a single interpreter process.

As a specialized queue implementation meant for sharing data between processes, multiprocessing.Queue makes it easy to distribute work across multiple processes in order to work around the GIL limitations. This type of queue can store and transfer any pickle-able object across process boundaries.

>>> from multiprocessing import Queue
>>> q = Queue()
>>> q.put('eat')
>>> q.put('sleep')
>>> q.put('code')

>>> q
<multiprocessing.queues.Queue object at 0x1081c12b0>

“>>> q.get()
'eat'
>>> q.get()
'sleep'
>>> q.get()
'code'

>>> q.get()
# Blocks / waits forever...

Key Takeaways on queue using list, collections.deque, queue.Queue, multiprocessing.Queue
==========================================================================================
Python includes several queue implementations as part of the core language and its standard library.
list objects can be used as queues, but this is generally not recommended due to slow performance.
If you’re not looking for parallel processing support, the implementation offered by collections.deque is an excellent default choice for implementing a FIFO queue data structure in Python. It provides the performance characteristics you’d expect from a good queue implementation and can also be used as a stack (LIFO Queue).

Priority Queues:
-----------------
“A priority queue is a container data structure that manages a set of records with totally-ordered39 keys (for example, a numeric weight value) to provide quick access to the record with the smallest or largest key in the set.

You can think of a priority queue as a modified queue: instead of retrieving the next element by insertion time, it retrieves the highest-priority element. The priority of individual elements is decided by the ordering applied to their keys.

Priority queues are commonly used for dealing with scheduling problems, for example, to give precedence to tasks with higher urgency.

Think about the job of an operating system task scheduler:

Ideally, high-priority tasks on the system (e.g., playing a real-time game) should take precedence over lower-priority tasks (e.g., downloading updates in the background). By organizing pending tasks in a priority queue that uses the task urgency as the key, the task scheduler can quickly select the highest-priority tasks and allow them to run first.

list – Maintaining a Manually Sorted Queue
--------------------------------------------
You can use a sorted list to quickly identify and delete the smallest or largest element. The downside is that inserting new elements into a list is a slow O(n) operation.

While the insertion point can be found in O(log n) time using bisect.insort40 in the standard library, this is always dominated by the slow insertion step.

Maintaining the order by appending to the list and re-sorting also takes at least O(n log n) time. Another downside is that you must manually take care of re-sorting the list when new elements are inserted. It’s easy to introduce bugs by missing this step, and the burden is always on you, the developer.

Therefore, I believe that sorted lists are only suitable as priority queues when there will be few insertions.

q = []
q.append((2, 'code'))
“q.append((1, 'eat'))
q.append((3, 'sleep'))

# NOTE: Remember to re-sort every time
#       a new element is inserted, or use
#       bisect.insort().
q.sort(reverse=True)

while q:
    next_item = q.pop()
    print(next_item)

# Result:
#   (1, 'eat')
#   (2, 'code')
#   (3, 'sleep')

heapq – List-Based Binary Heaps

This is a binary heap implementation usually backed by a plain list, and it supports insertion and extraction of the smallest element in O(log n) time.41

This module is a good choice for implementing priority queues in Python. Since heapq technically only provides a min-heap implementation, extra steps must be taken to ensure sort stability and other features typically expected from a “practical” priority queue.

import heapq
q = []

heapq.heappush(q, (2, 'code'))
heapq.heappush(q, (1, 'eat'))
heapq.heappush(q, (3, 'sleep'))

while q:
    next_item = heapq.heappop(q)
    print(next_item)

# Result:
#   (1, 'eat')
#   (2, 'code')
#   (3, 'sleep')

queue.PriorityQueue – Beautiful Priority Queues
================================================
This priority queue implementation uses heapq internally and shares the same time and space complexities.43

The difference is that PriorityQueue is synchronized and provides locking semantics to support multiple concurrent producers and consumers.

“Depending on your use case, this might be helpful—or just slow your program down slightly. In any case, you might prefer the class-based interface provided by PriorityQueue over using the function-based interface provided by heapq.

from queue import PriorityQueue

q = PriorityQueue()

q.put((2, 'code'))
q.put((1, 'eat'))
q.put((3, 'sleep'))

while not q.empty():
    next_item = q.get()
    print(next_item)

# Result:
#   (1, 'eat')
#   (2, 'code')
#   (3, 'sleep')

Key Takeaways on priorityQueue with list, heapq and queue.ProrityQueue
===============================================
Python includes several priority queue implementations for you to use.
queue.PriorityQueue stands out from the pack with a nice object-oriented interface and a name that clearly states its intent. It should be your preferred choice.
If you’d like to avoid the locking overhead of queue.PriorityQueue, using the heapq module directly is also a good option.

Loops and Iterations:
----------------------
Writing pythonic loops:
------------------------
The range type represents an immutable sequence of numbers. Its advantage over a regular list is that it always takes the same small amount of memory. Range objects don’t actually store the individual values representing the number sequence—instead, they function as iterators and calculate the sequence values on the fly.

for-loops are really “for-each” loops that can iterate directly over items from a container or sequence, without having to look them up by index.

Eg: To get index and value of item from a list
l = ['a','b','c']

for index, item in enumerate(l):
    print(f'index {index}, item {item}')

Comprehending Comprehensions:
-----------------------------
The key to understanding list comprehensions is that they’re just for-loops over a collection but expressed in a more terse and compact syntax.

Template for list comprehension
values = [expression for item in collection]

List comprehensions can filter values based on some arbitrary condition that decides whether or not the resulting value becomes a part of the output list.

even_squares = [x * x for x in range(10) if x % 2 == 0]

values = [expression for item in collection if condition]

Similarly there are set and dict comprehension as well
{ x * x for x in range(-9, 10) } # set comprehension
{ x: x * x for x in range(5) } # dict comprehension

slice operator can be used to reverse a list but it is good to stick with list.reverse() or reversed().
slice operator can be used to clear a list and make it empty but using clear() is better.

New object with the same iterator elements can be created using slice operator as shallow copy.
lis = [1,2,3]
orig_list = lis   ## New reference pointing to same object
print(id(lis), id(orig_list))

new_list = lis[:]
print(id(lis), id(new_list)) ## New object is created

print(orig_list is lis) ## True
print(new_list is lis)  ## False

The slice operator is not only useful for selecting sublists of elements within a list. It can also be used to clear, reverse, and copy lists.
But be careful—this functionality borders on the arcane for many Python developers. Using it might make your code less maintainable for everyone else on your team.

Beautiful Iterators:
---------------------
It facinates us when we can run loop over an iterator like list directly using for loop over it like
for i in [1,2,3]:
    print(i)

Same thing can be done with our custom class as well, we just need to use two dunders for it in our class.
__iter__ and __next__
Objects that support the __iter__ and __next__ dunder methods automatically work with for-in loops.

To start with the implementation, we’ll first define and flesh out the Repeater class.
class RepeaterIterator:
    def __init__(self, source):
        self.source = source
    def __next__(self):
        print(id(self.source.value)) ## Called Repeatedly
        return self.source.value

class Repeater:
    def __init__(self, value):
        self.value = value
    def __iter__(self):
        print('Repeater', id(self)) ## Called Once
        return RepeaterIterator(self)


On first inspection, Repeater looks like a bog-standard Python class. But notice how it also includes the __iter__ dunder method.

What’s the RepeaterIterator object we’re creating and returning from __iter__? It’s a helper class we also need to define for our for-in iteration example to work:

Again, RepeaterIterator looks like a straightforward Python class, but you might want to take note of the following two things:

In the __init__ method, we link each RepeaterIterator instance to the Repeater object that created it. That way we can hold onto the “source” object that’s being iterated over.

In RepeaterIterator.__next__, we reach back into the “source” Repeater instance and return the value associated with it.

In this code example, Repeater and RepeaterIterator are working together to support Python’s iterator protocol. The two dunder methods we defined, __iter__ and __next__, are the keys to making a Python object iterable.

Now we have a working iterator which can do wonders by running in infinite loop.
repeater = Repeater('hello')
for i in repeater:
    print(i)

How to for-in loops work in python?
It internally used while loop.
repeater = Repeater('hello')
iterator = repeater.__iter__()
while True:
    item = iterator.__next__()
    print(item)

As you can see, the for-in was just syntactic sugar for a simple while loop:

It first prepared the repeater object for iteration by calling its __iter__ method. This returned the actual iterator object.
After that, the loop repeatedly called the iterator object’s __next__ method to retrieve values from it.

Because there’s never more than one element “in flight,” this approach is highly memory-efficient. Our Repeater class provides an infinite sequence of elements and we can iterate over it just fine. Emulating the same thing with a Python list would be impossible—there’s no way we could create a list with an infinite number of elements in the first place. This makes iterators a very powerful concept.

On more abstract terms, iterators provide a common interface that allows you to process every element of a container while being completely isolated from the container’s internal structure.

Whether you’re dealing with a list of elements, a dictionary, an infinite sequence like the one provided by our Repeater class, or another sequence type—all of that is just an implementation detail. Every single one of these objects can be traversed in the same way with the power of iterators.

And as you’ve seen, there’s nothing special about for-in loops in Python. If you peek behind the curtain, it all comes down to calling the right dunder methods at the right time.

repeater = Repeater('Ramesh')
iterator = iter(repeater)
print(next(iterator))
print(next(iterator))

This gives the same result—an infinite stream of hellos. Every time you call next(), the iterator hands out the same greeting again.

By the way, I took the opportunity here to replace the calls to __iter__ and __next__ with calls to Python’s built-in functions, iter() and next().

Internally, these built-ins invoke the same dunder methods, but they make this code a little prettier and easier to read by providing a clean “facade” to the iterator protocol.

Python offers these facades for other functionality as well. For example, len(x) is a shortcut for calling x.__len__. Similarly, calling iter(x) invokes x.__iter__ and calling next(x) invokes x.__next__.

Generally, it’s a good idea to use the built-in facade functions rather than directly accessing the dunder methods implementing a protocol. It just makes the code a little easier to read.

We can place the above example in a single class
class Repeater:
    def __init__(self, value):
        self.value = value
    def __iter__(self):
        return self
    def __next__(self):
        return self.value

Next thing we need to do it put a stop condition so that the loop in not infinite, how can we do that?
how does list do it?
Let check list using iter and next:
l = list([1,2,3])
iterator = iter(l)
print(next(iterator)) # 1
print(next(iterator)) # 2
print(next(iterator)) # 3
print(next(iterator)) # StopIteration

So now we know what list does,after exhausting all the elements it throws StopIteration Exeception so that the next dunder can be stopped.
Aha! It raises a StopIteration exception to signal we’ve exhausted all of the available values in the iterator.

That’s right: Iterators use exceptions to structure control flow. To signal the end of iteration, a Python iterator simply raises the built-in StopIteration exception.

“If I keep requesting more values from the iterator, it’ll keep raising StopIteration exceptions to signal that there are no more values available to iterate over:

>>> next(iterator)
StopIteration
>>> next(iterator)
StopIteration

Python iterators normally can’t be “reset”—once they’re exhausted they’re supposed to raise StopIteration every time next() is called on them. To iterate anew you’ll need to request a fresh iterator object with the iter() function.

Now, let's write BoundedRepeater class then stops iterating after a set of numbers.
class BoundRepeater:
    def __init__(self, value, max_repeat):
        self.value = value
        self.max_repeat = max_repeat
        self.count = 0
    def __iter__(self):
        return self
    def __next__(self):
        if self.count >= self.max_repeat:
            raise StopIteration
        self.count += 1
        return self.value
br = BoundRepeater('Chandu', 3)
for i in br:
    print(i)

If we rewrite this last for-in loop example to take away some of the syntactic sugar, we end up with the following expanded code snippet:
repeater = BoundRepeater('Chandu',3)
iterator = iter()
while True:
    try:
        item = next(iterator)
    except StopIteration:
        break
    print(item)

Every time next() is called in this loop, we check for a StopIteration exception and break the while loop if necessary.

Being able to write a three-line for-in loop instead of an eight-line while loop is quite a nice improvement. It makes the code easier to read and more maintainable. And this is another reason why iterators in Python are such a powerful tool.

Generators:
-----------
By looking at the class of iterator it seems for every iterator class we will need to write same piece of code again in again in different classes. Is there any solution to that, yes there is - GENERATORS

Generators look like regular functions but instead of using the return statement, they use yield to pass data back to the caller.

what we have done in iterator class initially to write endless loop of 'hello' can be done in generator as:
def repeater(value):
    while True:
        yield value

We can run the infinite loop as
for i in repeater('Hi'):
    print(i)

Now, how do these generators work? They look like normal functions, but their behavior is quite different. For starters, calling a generator function doesn’t even run the function. It merely creates and returns a generator object:
repeater('Hi') # <generator object repeater at 0x102e740c0>

The code in the generator function only executes when next() is called on the generator object:
generator_obj = repeater('Hi') # <generator object repeater at 0x102e740c0>
next(generator_obj) # 'Hi'
next(generator_obj) # 'Hi'

If you read the code of the repeater function again, it looks like the yield keyword in there somehow stops this generator function in mid-execution and then resumes it at a later point in time:
def repeater(value):
    while True:
        yield value

And that’s quite a fitting mental model for what happens here. You see, when a return statement is invoked inside a function, it permanently passes control back to the caller of the function. When a yield is invoked, it also passes control back to the caller of the function—but it only does so temporarily.

Whereas a return statement disposes of a function’s local state, a yield statement suspends the function and retains its local state. In practical terms, this means local variables and the execution state of the generator function are only stashed away temporarily and not thrown out completely. Execution can be resumed at any time by calling next() on the generator:

This makes generators fully compatible with the iterator protocol. For this reason, I like to think of them primarily as syntactic sugar for implementing iterators.

You’ll find that for most types of iterators, writing a generator function will be easier and more readable than defining a long-winded class-based iterator.

Generators That Stop Generating
-------------------------------
Remember, in our class-based iterator we were able to signal the end of iteration by manually raising a StopIteration exception. Because generators are fully compatible with class-based iterators, that’s still what happens behind the scenes.

Thankfully, as programmers we get to work with a nicer interface this time around. Generators stop generating values as soon as control flow returns from the generator function by any means other than a yield statement. This means you no longer have to worry about raising StopIteration at all!

def repeat_thre_times(value):
    yield value
    yield value
    yield value
    
for i in repeat_thre_times('Hi'):
    print(i)

Notice how this generator function doesn’t include any kind of loop. In fact it’s dead simple and only consists of three yield statements. If a yield temporarily suspends execution of the function and passes back a value to the caller, what will happen when we reach the end of this generator? Let’s find out

As you may have expected, this generator stopped producing new values after three iterations. We can assume that it did so by raising a StopIteration exception when execution reached the end of the function. But to be sure, let’s confirm that with another experiment:

iterator = repeat_thre_times('Hi')
next(iterator) # 'Hi'
next(iterator) # 'Hi'
next(iterator) # 'Hi'
next(iterator) # 'StopIteration'
next(iterator) # 'StopIteration'

This iterator behaved just like we expected. As soon as we reach the end of the generator function, it keeps raising StopIteration to signal that it has no more values to provide.

Let’s come back to another example from the iterators chapter. The BoundedIterator class implemented an iterator that would only repeat a value a set number of times:

Why don’t we try to re-implement this BoundedRepeater class as a generator function. Here’s my first take on it:

def bounded_iterator(value, max_repeat):
    count = 0
    while True:
        if count >= max_repeat:
            return
        count += 1
        yield value

I intentionally made the while loop in this function a little unwieldy. I wanted to demonstrate how invoking a return statement from a generator causes iteration to stop with a StopIteration exception. We’ll soon clean up and simplify this generator function some more, but first let’s try out what we’ve got so far:

for i in bounded_iterator('Maghu', 3):
    print(i)

“Great! Now we have a generator that stops producing values after a configurable number of repetitions. It uses the yield statement to pass back values until it finally hits the return statement and iteration stops.

Like I promised you, we can further simplify this generator. We’ll take advantage of the fact that Python adds an implicit return None statement to the end of every function. This is what our final implementation looks like:

def bound_iterator(value, max_repeat):
    for i in range(max_repeat):
        yield i

for i in bounded_iterator('Manushya', 3):
    print(i)

As you just saw, generators help “abstract away” most of the boilerplate code otherwise needed when writing class-based iterators. They can make your life as a programmer much easier and allow you to write cleaner, shorter, and more maintainable iterators. Generator functions are a great feature in Python, and you shouldn’t hesitate to use them in your own programs.

Key Takeaways of Generators:
----------------------------
Generator functions are syntactic sugar for writing objects that support the iterator protocol. Generators abstract away much of the boilerplate code needed when writing class-based iterators.
The yield statement allows you to temporarily suspend execution of a generator function and to pass back values from it.
Generators start raising StopIteration exceptions after control flow leaves the generator function by any means other than a yield statement.

Generator Expressions: (Expressions is the fancy name for Comprehensions)
-------------------------
Since we were able to write a bound_iterator function yielding value specified number of times, can't we reduce it to generator comprehension; yes, we can!

iterator = ('Hellos' for _ in range(3))
for i in iterator:
    print (i)

Shorthand for above expression will be:
for i in ('Hellos' for _ in range(3)):
    print (i)

There’s one small caveat though: Once a generator expression has been consumed, it can’t be restarted or reused. So in some cases there is an advantage to using generator functions or class-based iterators

Generator Expressions vs List Comprehensions:
---------------------------------------------
“As you can tell, generator expressions are somewhat similar to list comprehensions:

>>> listcomp = ['Hello' for i in range(3)]
>>> genexpr = ('Hello' for i in range(3))
Unlike list comprehensions, however, generator expressions don’t construct list objects. Instead, they generate values “just in time” like a class-based iterator or generator function would.

All you get by assigning a generator expression to a variable is an iterable “generator object”:

>>> listcomp
['Hello', 'Hello', 'Hello']

>>> genexpr
<generator object <genexpr> at 0x1036c3200>
To access the values produced by the generator expression, you need to call next() on it, just like you would with any other iterator:

>>> next(genexpr)
'Hello'
>>> next(genexpr)
'Hello'
>>> next(genexpr)
'Hello'
>>> next(genexpr)
StopIteration

Alternatively, you can also call the list() function on a generator expression to construct a list object holding all generated values:

>>> genexpr = ('Hello' for i in range(3))
>>> list(genexpr)
['Hello', 'Hello', 'Hello']
Of course, this was just a toy example to show how you can “convert” a generator expression (or any other iterator for that matter) into a list. 

Just like with list comprehensions, this gives you a “cookie-cutter pattern” you can apply to many generator functions in order to transform them into concise generator expressions.

Filtering values in generator expressions:
-------------------------------------------
print even numbers
for x in (i for i in range(10) if i%2==0):
    print(x)

Template - genexpr = (expression for item in collection if condition)

If we are passing generator expression to a function then we can drop ()
sum(i for i in range(10) if i%2==0) # 20 everything is done in single ()

Generator expression nesting:
-----------------------------
we can have multiple loops/conditions in a single generator expression.

for i in ((x, y, z) for x in range(1,10) if x%7==0 for y in range(10,20) if y%8 == 0 for z in range(20,30) if z%9==0):
    print (i)

“And this is where I’d like to place a big caveat:

Please don’t write deeply nested generator expressions like that. They can be very difficult to maintain in the long run.

This is one of those “the dose makes the poison” situations where a beautiful and simple tool can be overused to create hard to read and difficult to debug programs.

Just like with list comprehensions, I personally try to stay away from any generator expression that includes more than two levels of nesting.

Generator expressions are a helpful and Pythonic tool in your toolbox, but that doesn’t mean they should be used for every single problem you’re facing. For complex iterators, it’s often better to write a generator function or even a class-based iterator.

If you need to use nested generators and complex filtering conditions, it’s usually better to factor out sub-generators (so you can name them) and then to chain them together again at the top level called Iterator Chains.

Key Takeaways on Generator Expression and Generator Nesting
============================================================
Generator expressions are similar to list comprehensions. However, they don’t construct list objects. Instead, generator expressions generate values “just in time” like a class-based iterator or generator function would.
Once a generator expression has been consumed, it can’t be restarted or reused.
Generator expressions are best for implementing simple “ad hoc” iterators. For complex iterators, it’s better to write a generator function or a class-based iterator.


ITERATOR CHAINS:
-----------------
If you take advantage of Python’s generator functions and generator expressions, you’ll be building concise and powerful iterator chains in no time.

Generators and generator expressions are syntactic sugar for writing iterators in Python. They abstract away much of the boilerplate code needed when writing class-based iterators.

While a regular function produces a single return value, generators produce a sequence of results. You could say they generate a stream of values over the course of their lifetime.

Generators can be “connected” to each other in order to build efficient data processing algorithms that work like a pipeline.

Eg: Passing one generator to another:
def integers():
    for i in range(1,9):
        yield i

def squared(seq):
    for i in seq:
        yield i*i

list(squared(integers())) # Converting the output to list

And we can keep on adding new building blocks to this pipeline. Data flows in one direction only, and each processing step is shielded from the others via a well-defined interface.

This is similar to how pipelines work in Unix. We chain together a sequence of processes so that the output of each process feeds directly as input to the next one.

Why don’t we add another step to our pipeline that negates each value and then passes it on to the next processing step in the chain:

def integers():
    for i in range(1,9):
        yield i

def squared(seq):
    for i in seq:
        yield i*i

def neg(seq):
    for i in seq:
        yield -i

list(neg(squared(integers()))) # [-1, -4, -9, -16, -25, -36, -49, -64]

My favorite thing about chaining generators is that the data processing happens one element at a time. There’s no buffering between the processing steps in the chain:

The integers generator yields a single value, let’s say 3.
This “activates” the squared generator, which processes the value and passes it on to the next stage as 3 × 3 = 9
The square number yielded by the squared generator gets fed immediately into the negated generator, which modifies it to -9 and yields it again.

Now, let's use generator expression for these generators chaining
Eg: 
integers = range(1,9)
squared = (i*i for i in integers)
neg = (-i for i in squared)
list(neg)

The only downside to using generator expressions is that they can’t be configured with function arguments, and you can’t reuse the same generator expression multiple times in the same processing pipeline.

But of course, you could mix-and-match generator expressions and regular generators freely in building these pipelines. This will help improve readability with complex pipelines.

Key Takeaways Generators Iterating Chains:
==========================================
Generators can be chained together to form highly efficient and maintainable data processing pipelines.
Chained generators process each element going through the chain individually.
Generator expressions can be used to write concise pipeline definitions, but this can impact readability.

DICTIONARIES:
=============
Python’s dictionaries have a get() method on them which supports a “default” parameter that can be used as a fallback value:

It support EAFP principle - Easy to ask for forgiveness than permission.

When get() is called, it checks if the given key exists in the dictionary. If it does, the value for the key is returned. If it does not exist, then the value of the default parameter is returned instead.

emp = {
    382: 'Ramesh',
    961: 'Suresh',
    420: 'Chandu',
}
def greetings(userid):
    return f'Hi {emp.get(userid, "there")}!'
print(greetings(382))   # Hi Ramesh!
print(greetings(1111))  # Hi there!

NOT A GOOD IDEA TO WRITE CODE LIKE BELOW:
def greeting(userid):
    if userid in name_for_userid:
        return 'Hi %s!' % name_for_userid[userid]
    else:
        return 'Hi there!

OR 

def greeting(userid):
    try:
        return 'Hi %s!' % name_for_userid[userid]
    except KeyError:
        return 'Hi there

Key Takeaways from basics of dictionaries
------------------------------------------
Avoid explicit key in dict checks when testing for membership.
EAFP-style exception handling or using the built-in get() method is preferable.
In some cases, the collections.defaultdict class from the standard library can also be helpful.

Sorting Dictionaries for Fun and Profit
----------------------------------------
To compare two tuples, Python compares the items stored at index zero first. If they differ, this defines the outcome of the comparison. If they’re equal, the next two items at index one are compared, and so on.

Dict are internally stored as list of tuples where each key/value pair is called an item and is a tuple.
Like:
d = {'a':20, 'c':10, 'b':14}
sorted(d.items()) # [('a', 20), ('b', 14), ('c', 10)]
In above example the elements of dict are sorted by key.
If you want to sort them by value, you can go with
sorted(d.items(), key=lambda x:x[1]) # [('c', 10), ('b', 14), ('a', 20)]

In fact, the concept is so common that Python’s standard library includes the operator module. This module implements some of the most frequently used key funcs as plug-and-play building blocks, like operator.itemgetter and operator.attrgetter.

Here’s an example of how you might replace the lambda-based index lookup in the first example with operator.itemgetter:

from operator import itemgetter
sorted(d.items(), key=itemgetter(1)) # [('c', 10), ('b', 14), ('a', 20)]

Using the operator module might communicate your code’s intent more clearly in some cases. On the other hand, using a simple lambda expression might be just as readable and more explicit. In this particular case, I actually prefer the lambda expression.

Another benefit of using lambdas as a custom key func is that you get to control the sort order in much finer detail. For example, you could sort a dictionary based on the absolute numeric value of each value stored in it:
sorted(d.items(), key=lambda x:abs(x[1])) # [('c', 10), ('b', 14), ('a', 20)]

If you need to reverse the sort order so that larger values go first, you can use the reverse=True keyword argument when calling sorted():

sorted(d.items(), key=lambda x:x[1], reverse=True) # [('a', 20), ('b', 14), ('c', 10)]

Like I said earlier, it’s totally worth spending some time getting a good grip on how key funcs work in Python. They provide you with a ton of flexibility and can often save you from writing code to transform one data structure into another.

Key Takeaways
--------------------------
When creating sorted “views” of dictionaries and other collections, you can influence the sort order with a key func.
Key funcs are an important concept in Python. The most frequently used ones were even added to the operator module in the standard library.
Functions are first-class citizens in Python. This is a powerful feature you’ll find used everywhere in the language.

Emulating Switch/Case Statements With Dicts
----------------------------------------------
Python doesn’t have switch/case statements so it’s sometimes necessary to write long if…elif…else chains as a workaround. In this chapter you’ll discover a trick you can use to emulate switch/case statements in Python with dictionaries and first-class functions. Sound exciting? Great—here we go!

Imagine we had the following if-chain in our program:

>>> if cond == 'cond_a':
...     handle_a()
... elif cond == 'cond_b':
...     handle_b()
... else:
...     handle_default()
Of course, with only three different conditions, this isn’t too horrible yet. But just imagine if we had ten or more elif branches in this statement. Things would start to look a little different. I consider long if-chains to be a code smell that makes programs more difficult to read and maintain.

One way to deal with long if…elif…else statements is to replace them with dictionary lookup tables that emulate the behavior of switch/case statements.

The idea here is to leverage the fact that Python has first-class functions. This means they can be passed as arguments to other functions, returned as values from other functions, and assigned to variables and stored in data structures.

For example, we can define a function and then store it in a list for later access:

>>> def myfunc(a, b):
...     return a + b
...
>>> funcs = [myfunc]
>>> funcs[0]
<function myfunc at 0x107012230>
The syntax for calling this function works as you’d intuitively expect—we simply use an index into the list and then use the “()” call syntax for calling the function and passing arguments to it:

>>> funcs[0](2, 3)
5
Now, how are we going to use first-class functions to cut our chained if-statement back to size? The core idea here is to define a dictionary that maps lookup keys for the input conditions to functions that will carry out the intended operations:

>>> func_dict = {
...     'cond_a': handle_a,
...     'cond_b': handle_b
... }
Instead of filtering through the if-statement, checking each condition as we go along, we can do a dictionary key lookup to get the handler function and then call it:

>>> cond = 'cond_a'
>>> func_dict[cond]()
This implementation already sort-of works, at least as long as cond can be found in the dictionary. If it’s not in there, we’ll get a KeyError exception.

So let’s look for a way to support a default case that would match the original else branch. Luckily all Python dicts have a get() method on them that returns the value for a given key, or a default value if the key can’t be found. This is exactly what we need here:

>>> func_dict.get(cond, handle_default)()
This code snippet might look syntactically odd at first, but when you break it down, it works exactly like the earlier example. Again, we’re using Python’s first-class functions to pass handle_default to the get()-lookup as a fallback value. That way, if the condition can’t be found in the dictionary, we avoid raising a KeyError and call the default handler function instead.

Let’s take a look at a more complete example for using dictionary lookups and first-class functions to replace if-chains. After reading through the following example, you’ll be able to see the pattern needed to transform certain kinds of if-statements to a dictionary-based dispatch.

We’re going to write another function with an if-chain that we’ll then transform. The function takes a string opcode like

def dispatch_if(operator,x,y):
    if operator == 'add':
        return x + y
    if operator == 'sub':
        return x - y
    if operator == 'mul':
        return x * y
    if operator == 'div':
        return x / y
    
print(dispatch_if('mul',10,15)) # 150
print(dispatch_if('add',10,15)) # 25
print(dispatch_if('unknown',10,15)) # None

Please note that the 'unknown' case works because Python adds an implicit return None statement to the end of any function.

So far so good. Let’s transform the original dispatch_if() into a new function which uses a dictionary to map opcodes to arithmetic operations with first-class functions.

def dispatch_dict(operator, x, y):
    return {
        'add': lambda: x + y,
        'sub': lambda: x - y,
        'mul': lambda: x * y,
        'div': lambda: x / y,
    }.get(operator, lambda:None)()

print(dispatch_dict('add', 10, 15))
print(dispatch_dict('mul', 10, 15))
print(dispatch_if('unknown', 10, 15))

“There are a couple of ways this code could be further improved if it was real “production-grade” code.

First of all, every time we call dispatch_dict(), it creates a temporary dictionary and a bunch of lambdas for the opcode lookup. This isn’t ideal from a performance perspective. For code that needs to be fast, it makes more sense to create the dictionary once as a constant and then to reference it when the function is called. We don’t want to recreate the dictionary every time we need to do a lookup.

Second, if we really wanted to do some simple arithmetic like x + y, then we’d be better off using Python’s built-in operator module instead of the lambda functions used in the example. The operator module provides implementations for all of Python’s operators, for example operator.mul, operator.div, and so on. This is a minor point, though. I intentionally used lambdas in this example to make it more generic. This should help you apply the pattern in other situations as well.

Well, now you’ve got another tool in your bag of tricks that you can use to simplify some of your if-chains should they get unwieldy. Just remember—this technique won’t apply in every situation and sometimes you’ll be better off with a plain if-statement.

Key Takeaways on Usage of Dicts instead of ifs
===============================================
Python doesn’t have a switch/case statement. But in some cases you can avoid long if-chains with a dictionary-based dispatch table.
Once again Python’s first-class functions prove to be a powerful tool. But with great power comes great responsibility.

The Craziest Dict Expression in the West
========================================
{True: 'yes', 1: 'no', 1.0: 'maybe'} # {True: 'maybe'}

Why the output is as {True: 'maybe'}?
Because True == 1 == 1.0

Python treats bool as a subclass of int. This is the case in Python 2 and Python 3:

“The Boolean type is a subtype of the integer type, and Boolean values behave like the values 0 and 1, respectively, in almost all contexts, the exception being that when converted to a string, the strings ‘False’ or ‘True’ are returned, respectively.

Why do we still get True as the key here? Shouldn’t the key also change to 1.0 at the end, due to the repeated assignments?
After some mode research in the CPython interpreter source code, I learned that Python’s dictionaries don’t update the key object itself when a new value is associated with it:

>>> ys = {1.0: 'no'}
>>> ys[True] = 'yes'
>>> ys
{1.0: 'yes'}
Of course this makes sense as a performance optimization—if the keys are considered identical, then why spend time updating the original? In the last example you saw that the initial True object is never replaced as the key. Therefore, the dictionary’s string representation still prints the key as True (instead of 1 or 1.0.)

“With what we know now, it looks like the values in the resulting dict are getting overwritten only because they compare as equal. However, it turns out that this effect isn’t caused by the __eq__ equality check alone, either.

Python dictionaries are backed by a hash table data structure. When I first saw this surprising dictionary expression, my hunch was that this behavior had something to do with hash collisions.

You see, a hash table internally stores the keys it contains in different “buckets” according to each key’s hash value. The hash value is derived from the key as a numeric value of a fixed length that uniquely identifies the key.

This allows for fast lookups. It’s much quicker to search for a key’s numeric hash value in a lookup table instead of comparing the full key object against all other keys and checking for equality.

However, the way hash values are typically calculated isn’t perfect. And eventually, two or more keys that are actually different will have the same derived hash value, and they will end up in the same lookup table bucket.

“If two keys have the same hash value, that’s called a hash collision, and it’s a special case that the hash table’s algorithms for inserting and finding elements need to handle.

Based on that assessment, it’s fairly likely that hashing has something to do with the surprising result we got from our dictionary expression. So let’s find out if the keys’ hash values also play a role here.

I’m defining the following class as our little detective tool:

class AlwaysEquals:
    def __eq__(self, other):
        return True
    def __hash__(self):
        return id(self)

“This class is special in two ways.

First, because its __eq__ dunder method always returns True, all instances of this class will pretend they’re equal to any other object:

>>> AlwaysEquals() == AlwaysEquals()
True
>>> AlwaysEquals() == 42
True
>>> AlwaysEquals() == 'waaat?'
True

“And second, each AlwaysEquals instance will also return a unique hash value generated by the built-in id() function:

>>> objects = [AlwaysEquals(),
               AlwaysEquals(),
               AlwaysEquals()]
>>> [hash(obj) for obj in objects]
[4574298968, 4574287912, 4574287072]

In CPython, id() returns the address of the object in memory, which is guaranteed to be unique.

With this class we can now create objects that pretend to be equal to any other object but have a unique hash value associated with them. That’ll allow us to test if dictionary keys are overwritten based on their equality comparison result alone.

And, as you can see, the keys in the next example are not getting overwritten, even though they always compare as equal:

AlwaysEquals() == AlwaysEquals() # True

{AlwaysEquals():'yes', AlwaysEquals():'no'}
{<__main__.AlwaysEquals at 0x103e4f550>: 'yes',
 <__main__.AlwaysEquals at 0x103e4f198>: 'no'}

 a = SameHash()
b = SameHash()
print(a) # <__main__.SameHash object at 0x10327c470>
print(b) # <__main__.SameHash object at 0x10327c080>
a == b
print(hash(a)) # 1
print(hash(b)) # 1

{a: 'a', b:'b'}
{<__main__.SameHash at 0x10327c630>: 'a',
 <__main__.SameHash at 0x10327c5c0>: 'b'}

As this example shows, the “keys get overwritten” effect isn’t caused by hash value collisions alone either.

Dictionaries check for equality and compare the hash value to determine if two keys are the same. Let’s try and summarize the findings of our investigation:

The {True: 'yes', 1: 'no', 1.0: 'maybe'} dictionary expression evaluates to {True: 'maybe'} because the keys True, 1, and 1.0 all compare as equal, and they all have the same hash value:

## check equality
print(True == 1 == 1.0) # True

## check hash
print(hash(True)) # 1
print(hash(1)) # 1
print(hash(1.0)) # 1

Perhaps not-so-surprising anymore, that’s how we ended up with this result as the dictionary’s final state:

>>> {True: 'yes', 1: 'no', 1.0: 'maybe'}
{True: 'maybe'}

## Coming back to the AlwaysEquals Class, if the hash is equal and the 
## key value is equal then key remains the same and the value overrides
class AlwaysEquals:
    def __eq__(self, other):
        return True
    def __hash__(self):
        return 1

{AlwaysEquals():'yes', AlwaysEquals():'no'} # {<__main__.AlwaysEquals at 0x10327c470>: 'no'}

Key Takeaways on Dictionaries Internals
=========================================
Dictionaries treat keys as identical if their __eq__ comparison result says they’re equal and their hash values are the same.
Unexpected dictionary key collisions can and will lead to surprising results.

So Many Ways to Merge Dictionaries
====================================
If we have two dicts and we want to create a new dict y merging these dicts then we can use following ways:
using update():
---------------
xs = {'a':1, 'b':2,}
ys = {'b':2, 'c':3,}
zs = {}
zs.update(xs)
zs.update(ys)
zs # {'a': 1, 'b': 2, 'c': 3}

internally update function is like below:
def update(dict1, dict2):
    for key, value in dict2.items():
        dict1[key] = value

You could expand this chain of update() calls for as long as you like in order to merge any number of dictionaries into one. 

using unpacking:
----------------
dict(xs, **ys) ## {'a': 1, 'b': 2, 'c': 3}

Just like making repeated update() calls, this approach only works for merging two dictionaries and cannot be generalized to combine an arbitrary number of dictionaries in one step.

pretier way of unpacking since v-3.5 (This one should be preferred)
==========================================================
zs = {**xs, **ys} # {'a': 1, 'b': 2, 'c': 3}

The right-hand side takes priority, and a value in ys overrides any existing value under the same key in xs. This becomes clear when we look at the dictionary that results from the merge operation:

Key Takeaways from merging dictionaries:
-----------------------------------------
* In Python 3.5 and above you can use the **-operator to merge multiple dictionary objects into one with a single expression, overwriting existing keys left-to-right.
* To stay compatible with older versions of Python, you might want to use the built-in dictionary update() method instead.

“Have you ever tried hunting down a bug in one of your programs by sprinkling a bunch of debug “print” statements to trace the execution flow? Or maybe you needed to generate a log message to print some configuration settings…

I have—and I’ve often been frustrated with how difficult some data structures are to read in Python when they’re printed as text strings. For example, here’s a simple dictionary. Printed in an interpreter session, the key order is arbitrary, and there’s no indentation to the resulting string:

Luckily there are some easy-to-use alternatives to a straight up to-str conversion that give a more readable result. One option is using Python’s built-in json module. You can use json.dumps() to pretty-print Python dicts with nicer formatting:

import json
json.dumps(zs, indent=4, sort_keys=True)

These settings result in a nicely indented string representation that also normalizes the order of the dictionary keys for better legibility.

While this looks nice and readable, it isn’t a perfect solution. Printing dictionaries with the json module only works with dicts that contain primitive types—you’ll run into trouble trying to print a dictionary that contains a non-primitive data type, like a function:

“>>> json.dumps({all: 'yup'})
TypeError: "keys must be a string"
Another downside of using json.dumps() is that it can’t stringify complex data types, like sets:

>>> mapping['d'] = {1, 2, 3}
>>> json.dumps(mapping)
TypeError: "set([1, 2, 3]) is not JSON serializable"
Also, you might run into trouble with how Unicode text is represented—in some cases you won’t be able to take the output from json.dumps and copy and paste it into a Python interpreter session to reconstruct the original dictionary object.

The classical solution to pretty-printing objects in Python is the built-in pprint module. Here’s an example:

import pprint
pprint.pprint(zs)

You can see that pprint is able to print data types like sets, and it also prints the dictionary keys in a reproducible order. Compared to the standard string representation for dictionaries, what we get here is much easier on the eyes.

“However, compared to json.dumps(), it doesn’t represent nested structures as well visually. Depending on the circumstances, this can be an advantage or a disadvantage. I occasionally use json.dumps() to print dictionaries because of the improved readability and formatting, but only if I’m sure they’re free of non-primitive data types.

Key Takeaways from pretty printing Dicts:
-------------------------------------------
The default to-string conversion for dictionary objects in Python can be difficult to read.
The pprint and json module are “higher-fidelity” options built into the Python standard library.
Be careful with using json.dumps() and non-primitive keys and values as this will trigger a TypeError.

Python Productivity techniques:
-------------------------------
Exploring Python modules and objects:
-------------------------------------
If you want to get the details of all the classes, methods, function in module; you can use dir() to get the details of the module.
Let say you want to understand details of datetime.

>>> import datetime
>>> dir(datetime)

Since everything is an object in python therefore not only modules but same technique works for classes and DataStructures.

>>> dir(datetime.date)

Sometimes calling dir() on an object will result in too much information—on a complex module or class you’ll get a long printout that’s difficult to read quickly. Here’s a little trick you can use to filter down the list of attributes to just the ones you’re interested in:

import datetime
[_ for _ in dir(datetime) if 'date' in _.lower()] # ['date', 'datetime', 'datetime_CAPI']

help()
------
help() is another useful function which helps you get complete documentation of an object in python.
help(datetime)
It's similar to man of Unix

Help on module datetime:

NAME
    datetime - Fast implementation of the datetime type.

MODULE REFERENCE
    https://docs.python.org/3.7/library/datetime
    
    The following documentation is automatically generated from the Python
    source files.  It may be incomplete, incorrect or include features that
    are considered implementation detail and may vary between Python
    implementations.  When in doubt, consult the module reference at the
    location listed above.

You can call help() on arbitrary Python objects, including other built-in functions and your own Python classes. The Python interpreter automatically generates this documentation from the attributes on an object and its docstring (if available.) 

help(datetime.date)
Help on class date in module datetime:

class date(builtins.object)
 |  date(year, month, day) --> date object
 |  
 |  Methods defined here:
 |  
 |  __add__(self, value, /)
 |      Return self+value.

Key Takeaways on dir() and help()
=================================
Use the built-in dir() function to interactively explore Python modules and classes from an interpreter session.
The help() built-in lets you browse through the documentation right from your interpreter (hit q to exit.)


Peeking Behind the Bytecode Curtain
------------------------------------
When the CPython interpreter executes your program, it first translates it into a sequence of bytecode instructions. Bytecode is an intermediate language for the Python virtual machine that’s used as a performance optimization.

Instead of directly executing the human-readable source code, compact numeric codes, constants, and references are used that represent the result of compiler parsing and semantic analysis.

This saves time and memory for repeated executions of programs or parts of programs. For example, the bytecode resulting from this compilation step is cached on disk in .pyc and .pyo files so that executing the same Python file is faster the second time around.



How below function is converted internally?

def greet(name):
    return 'Hello, ' + name + '!'
>>> greet('Guido')
'Hello, Guido!

Remember how I said that CPython first translates our source code into an intermediate language before it “runs” it? Well, if that’s true, we should be able to see the results of this compilation step. And we can.

Each function has a __code__ attribute (in Python 3) that we can use to get at the virtual machine instructions, constants, and variables used by our greet function:

>>> greet.__code__.co_code
b'd\x01|\x00\x17\x00d\x02\x17\x00S\x00'
>>> greet.__code__.co_consts
(None, 'Hello, ', '!')
>>> greet.__code__.co_varnames
('name',)
You can see co_consts contains parts of the greeting string our function assembles. Constants and code are kept separate to save memory space. Constants are, well, constant—meaning they can never be modified and are used interchangeably in multiple places.

So instead of repeating the actual constant values in the co_code instruction stream, Python stores constants separately in a lookup table. The instruction stream can then refer to a constant with an index into the lookup table. The same is true for variables stored in the co_varnames field.

I hope this general concept is starting to become more clear. But looking at the co_code instruction stream still makes me feel a little queasy. This intermediate language is clearly meant to be easy to work with for the Python virtual machine, not humans. After all, that’s what the text-based source code is for.

“The developers working on CPython realized that too. So they gave us another tool called a disassembler to make inspecting the bytecode easier.

Python’s bytecode disassembler lives in the dis module that’s part of the standard library. So we can just import it and call dis.dis() on our greet function to get a slightly easier-to-read representation of its bytecode:

>>> import dis
>>> dis.dis(greet)
  2           0 LOAD_CONST           1 ('Hello, ')
              2 LOAD_FAST            0 (name)
              4 BINARY_ADD
              6 LOAD_CONST           2 ('!')
              8 BINARY_ADD
             10 RETURN_VALUE
The main thing disassembling did was split up the instruction stream and give each opcode in it a human-readable name like LOAD_CONST.

You can also see how constant and variable references are now interleaved with the bytecode and printed in full to spare us the mental gymnastics of a co_const or co_varnames table lookup. Neat!

Looking at the human-readable opcodes, we can begin to understand how CPython represents and executes the 'Hello, ' + name + '!' expression in the original greet() function.

It first retrieves the constant at index 1 ('Hello, ') and puts it on the stack. It then loads the contents of the name variable and also puts them on the stack.

The stack is the data structure used as internal working storage for the virtual machine. There are different classes of virtual machines and one of them is called a stack machine. CPython’s virtual machine is an implementation of such a stack machine. If the whole thing is named after the stack, you can imagine what a central role this data structure plays.

What’s interesting about a stack as an abstract data structure is that, at the bare minimum, it only supports two operations: push and pop. Push adds a value to the top of the stack and pop removes and returns the topmost value. Unlike an array, there’s no way to access elements “below” the top level.


Key Takeaways on Virtual Machine and it's Internal workings
============================================================
CPython executes programs by first translating them into an intermediate bytecode and then running the bytecode on a stack-based virtual machine.
You can use the built-in dis module to peek behind the scenes and inspect the bytecode.


PandasOfficialDoc
-----------------

conda activate myenv
jupyter notebook

pandas: powerful Python data analysis toolkit Release 1.2.3
------------------------------------------------------------

In pandas a data table is called a DataFrame.

pandas supports the integration with many file formats or data sources out of the box (csv, excel, sql, json, parquet,. . . ). Importing data from each of these data sources is provided by function with the prefix read_*. Similarly, the to_* methods are used to store data.

There is no need to loop over all rows of your data table to do calculations. Data manipulations on a column work elementwise. Adding a column to a DataFrame based on existing data in other columns is straightforward.

Split-apply-combine approach:
Basic statistics (mean, median, min, max, counts...) are easily calculable. These or custom aggregations can be applied on the entire data set, a sliding window of the data or grouped by categories. The latter is also known as the split-apply-combine approach.

Change the structure of your data table in multiple ways. You can melt() your data table from wide to long/tidy form or pivot() from long to wide format. With aggregations built-in, a pivot table is created with a single command.

Multiple tables can be concatenated both column wise as row wise and database-like join/merge operations are provided to combine multiple tables of data.

pandas has great support for time series and has an extensive set of tools for working with dates, times, and time-indexed data.

Data sets do not only contain numerical data. pandas provides a wide range of functions to clean textual data and extract useful information from it.

Handling Import Errors:
If you encounter an ImportError, it usually means that Python couldn’t find pandas in the list of available libraries. Python internally has a list of directories it searches through, to find packages. You can obtain these directories with:
import sys 
sys.path

Dependencies 
Package             Minimum supported version
setuptools              24.2.0
NumPy                   1.16.5
python-dateutil         2.7.3
pytz                    2017.3

Package Overview:
-----------------
pandas is a Python package providing fast, flexible, and expressive data structures designed to make working with “relational” or “labeled” data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real-world data analysis in Python. Additionally, it has the broader goal of becoming the most powerful and flexible open source data analysis/manipulation tool available in any language. It is already well on its way towards this goal.
pandas is well suited for many different kinds of data:
• Tabular data with heterogeneously-typed columns, as in a SQL table or Excel spreadsheet
• Ordered and unordered (not necessarily fixed-frequency) time series data.
• Arbitrary matrix data (homogeneously typed or heterogeneous) with row and column labels
• Any other form of observational / statistical data sets. The data need not be labeled at all to be placed into a pandas data structure.

The two primary data structures of pandas, Series (1-dimensional) and DataFrame (2-dimensional), handle the vast majority of typical use cases in finance, statistics, social science, and many areas of engineering. pandas is built on top of NumPy and is intended to integrate well within a scientific computing environment with many other 3rd party libraries.

Here are just a few of the things that pandas does well:
Easy handling of missing data (represented as NaN) in floating point as well as non-floating point data 
Size mutability: columns can be inserted and deleted from DataFrame and higher dimensional objects
Automatic and explicit data alignment: objects can be explicitly aligned to a set of labels,or the user can simply ignore the labels and let Series, DataFrame, etc. automatically align the data for you in computations.
Powerful, flexible group by functionality to perform split-apply-combine operations on data sets, for both aggregating and transforming data
Make it easy to convert ragged, differently-indexed data in other Python and NumPy data structures into DataFrame objects
Intelligent label-based slicing, fancy indexing, and subsetting of large data sets Intuitive merging and joining data sets
Flexible reshaping and pivoting of data sets
Hierarchical labeling of axes (possible to have multiple labels per tick)
Robust IO tools for loading data from flat files (CSV and delimited), Excel files, databases, and saving / loading data from the ultrafast HDF5 format
Time series-specific functionality: date range generation and frequency conversion, moving window statistics, date shifting, and lagging.

Many of these principles are here to address the shortcomings frequently experienced using other languages / scientific research environments. For data scientists, working with data is typically divided into multiple stages: munging and cleaning data, analyzing / modeling it, then organizing the results of the analysis into a form suitable for plotting or tabular display. pandas is the ideal tool for all of these tasks.
Some other notes
• pandas is fast. Many of the low-level algorithmic bits have been extensively tweaked in Cython code. However, as with anything else generalization usually sacrifices performance. So if you focus on one feature for your application you may be able to create a faster specialized tool.
• pandas is a dependency of statsmodels, making it an important part of the statistical computing ecosystem in Python.
• pandas has been used extensively in production in financial applications.

DataStructures
df = DataFrame({
    'Dimensions':[1,2],
    'Name':['Series', 'DataFrame'],
    'Description':['1D labeled homogeneously-typed array',
                  'General 2D labeled, size-mutable tabular structure with potentially heterogeneously-typed column']
})
df


Dimensions	Name	    Description
	1	    Series	    1D labeled homogeneously-typed array
	2	    DataFrame	General 2D labeled, size-mutable tabular structure  with
                         potentially heterogeneously-typed column

Why more than one data structure?
The best way to think about the pandas data structures is as flexible containers for lower dimensional data. For example, DataFrame is a container for Series, and Series is a container for scalars. We would like to be able to insert and remove objects from these containers in a dictionary-like fashion.
Also, we would like sensible default behaviors for the common API functions which take into account the typical orientation of time series and cross-sectional data sets. When using the N-dimensional array (ndarrays) to store 2- and 3-dimensional data, a burden is placed on the user to consider the orientation of the data set when writing functions; axes are considered more or less equivalent (except when C- or Fortran-contiguousness matters for performance). In pandas, the axes are intended to lend more semantic meaning to the data; i.e., for a particular data set, there is likely to be a “right” way to orient the data. The goal, then, is to reduce the amount of mental effort required to code up data transformations in downstream functions.
For example, with tabular data (DataFrame) it is more semantically helpful to think of the index (the rows) and the columns rather than axis 0 and axis 1. Iterating through the columns of the DataFrame thus results in more readable code:

for col in df.columns:
    series = df[col]
    # do something with series

Mutability and copying of data
All pandas data structures are value-mutable (the values they contain can be altered) but not always size-mutable. The length of a Series cannot be changed, but, for example, columns can be inserted into a DataFrame. However, the vast majority of methods produce new objects and leave the input data untouched. In general we like to favor immutability where sensible.

1.4.3 Getting started tutorials
WHAT KIND OF DATA DOES PANDAS HANDLE?
---------------------------------------
I want to start using pandas
import pandas as pd

To load the pandas package and start working with it, import the package. The community agreed alias for pandas is pd, so loading pandas as pd is assumed standard practice for all of the pandas documentation.

pandas data table representation
I want to store passenger data of the Titanic. For a number of passengers, I know the name (characters), age (integers) and sex (male/female) data.

df = pd.DataFrame({
    'Names' : ['Ramesh Kumar', 'Suresh Kumar', 'Rupesh Kumar'],
    'Age' : [22, 35, 58],
    'Sex' : ['male', 'male', 'female']
})
df

To manually store data in a table, create a DataFrame. When using a Python dictionary of lists, the dictionary keys will be used as column headers and the values in each list as columns of the DataFrame.

A DataFrame is a 2-dimensional data structure that can store data of different types (including characters, integers, floating point values, categorical data and more) in columns. It is similar to a spreadsheet, a SQL table 

• The table has 3 columns, each of them with a column label. The column labels are respectively Name, Age and Sex.
• The columnName consists of textual data with each value a string,the column Age are numbers and the column Sex is textual data.

EACH COLUMN IN A DATAFRAME IS A SERIES
I’m just interested in working with the data in the column Age
df['Age]

When selecting a single column of a pandas DataFrame, the result is a pandas Series. To select the column, use the column label in between square brackets [].

Note: If you are familiar to Python dictionaries, the selection of a single column is very similar to selection of dictionary values based on the key.

You can create a Series from scratch as well:

ages = pd.Series([22, 35, 58])
ages

0    22
1    35
2    58
dtype: int64

A pandas Series has no column labels, as it is just a single column of a DataFrame. A Series does have row labels.

Do something with a DataFrame or Series
I want to know the maximum Age of the passengers
We can do this on the DataFrame by selecting the Age column and applying max():
df['Age'].max() # DataFrame
ages.max() # Series

As illustrated by the max() method, you can do things with a DataFrame or Series. pandas provides a lot of functionalities, each of them a method you can apply to a DataFrame or Series. As methods are functions, do not forget to use parentheses ().
I’m interested in some basic statistics of the numerical data of my data table
df.describe()

The describe() method provides a quick overview of the numerical data in a DataFrame. As the Name and Sex columns are textual data, these are by default not taken into account by the describe() method.
Many pandas operations return a DataFrame or a Series. The describe() method is an example of a pandas operation returning a pandas Series.

Bullet Points:
--------------
• Import the package, aka import pandas as pd
• A table of data is stored as a pandas DataFrame
• Each column in a DataFrame is a Series
• You can do things by applying a method to a DataFrame or Series

This tutorial uses the Titanic data set, stored as CSV. The data consists of the following data columns: 
• PassengerId: Id of every passenger.
• Survived: This feature have value 0 and 1. 0 for not survived and 1 for survived.
• Pclass: There are 3 classes: Class 1, Class 2 and Class 3.
• Name: Name of passenger.
• Sex: Gender of passenger.
• Age: Age of passenger.
• SibSp: Indication that passenger have siblings and spouse. 
• Parch: Whether a passenger is alone or have family.
• Ticket: Ticket number of passenger. 
• Fare: Indicating the fare.
• Cabin: The cabin of passenger.
• Embarked: The embarked category.

HOW DO I READ AND WRITE TABULAR DATA?
--------------------------------------
I want to analyze the Titanic passenger data, available as a CSV file.
titanic = pd.read_csv('data/titanic.csv')
titanic

pandas provides the read_csv() function to read data stored as a csv file into a pandas DataFrame. pandas supports many different file formats or data sources out of the box (csv, excel, sql, json, parquet, . . . ), each of them with the prefix read_*.
Make sure to always have a check on the data after reading in the data. When displaying a DataFrame, the first and last 5 rows will be shown by default:

I want to see the first 8 rows of a pandas DataFrame.
titanic.head(8)
To see the first N rows of a DataFrame, use the head() method with the required number of rows (in this case 8) as argument.

Interested in the last N rows instead? pandas also provides a tail() method. For example, titanic.tail(10) will return the last 10 rows of the DataFrame.

A check on how pandas interpreted each of the column data types can be done by requesting the pandas dtypes attribute:
titanic.dtypes
PassengerId      int64
Pclass           int64
Name            object
Sex             object
Age            float64

For each of the columns, the used data type is enlisted. The data types in this DataFrame are integers (int64), floats (float64) and strings (object).

NOTE: 
When asking for the dtypes, no brackets are used! dtypes is an attribute of a DataFrame and Series. Attributes of DataFrame or Series do not need brackets. Attributes represent a characteristic of a DataFrame/Series, whereas a method (which requires brackets) do something with the DataFrame/Series.

My colleague requested the Titanic data as a spreadsheet.
titanic.to_excel("titanic.xlsx", sheet_name="passengers", index=False)

Whereas read_* functions are used to read data to pandas, the to_* methods are used to store data. The to_excel() method stores the data as an excel file. In the example here, the sheet_name is named passengers instead of the default Sheet1. By setting index=False the row index labels are not saved in the spreadsheet.
The equivalent read function read_excel() will reload the data to a DataFrame:
titanic = pd.read_excel("titanic.xlsx", sheet_name="passengers")

I’m interested in a technical summary of a DataFrame

titanic.info()

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   PassengerId  891 non-null    int64  
 1   Survived     891 non-null    int64  
 2   Pclass       891 non-null    int64  
 3   Name         891 non-null    object

dtypes: float64(2), int64(4), object(5)
memory usage: 36.0+ KB

The method info() provides technical information about a DataFrame, so let’s explain the output in more detail:
In [1]: import pandas as pd

This tutorial uses the Titanic data set, stored as CSV. The data consists of the following data columns:
• PassengerId: Id of every passenger.
It is indeed a DataFrame.
There are 891 entries, i.e. 891 rows.
Each row has a row label (aka the index) with values ranging from 0 to 890.
The table has 12 columns. Most columns have a value for each of the rows (all 891 values are non-null). Some columns do have missing values and less than 891 non-null values.
The columns Name, Sex, Cabin and Embarked consists of textual data (strings, aka object). The other columns are numerical data with some of them whole numbers (aka integer) and others are real numbers (aka float).
* The kind of data (characters, integers,. . . ) in the different columns are summarized by listing the dtypes. The approximate amount of RAM used to hold the DataFrame is provided as well.
* Getting data in to pandas from many different file formats or data sources is supported by read_* functions. Exporting data out of pandas is provided by different to_* methods.
* The head/tail/info methods and the dtypes attribute are convenient for a first check.

HOW DO I SELECT A SUBSET OF A TABLE?
HOW DO I SELECT A SUBSET OF A DATAFRAME?
HOW DO I SELECT SPECIFIC COLUMNS FROM A DATAFRAME?
I’m interested in the age of the Titanic passengers.
ages = titanic['Age']
ages.head()

To select a single column, use square brackets [] with the column name of the column of interest.
Each column in a DataFrame is a Series. As a single column is selected, the returned object is a pandas Series.
We can verify this by checking the type of the output:
type(titanic['Age']) # pandas.core.series.Series

And have a look at the shape of the output:
titanic['Age'].shape # (891,)
titanic.shape # (891, 12)

DataFrame.shape is an attribute of a pandas Series and DataFrame containing the number of rows and columns: (nrows, ncolumns). A pandas Series is 1-dimensional and only the number of rows is returned.

I’m interested in the age and sex of the Titanic passengers.
age_sex = titanic[['Age', 'Sex']]
age_sex.head()
To select multiple columns, use a list of column names within the selection brackets [].
Note: The inner square brackets define a Python list with columnnames,whereas the outer brackets are used to select the data from a pandas DataFrame as seen in the previous example.

The returned data type is a pandas DataFrame:
age_sex = titanic[['Age', 'Sex']]
age_sex.head()
type(age_sex) # pandas.core.frame.DataFrame
age_sex.shape # (891, 2)
The selection returned a DataFrame with 891 rows and 2 columns. Remember, a DataFrame is 2-dimensional with both a row and column dimension.

How do I filter specific rows from a DataFrame?
I’m interested in the passengers older than 35 years.
above_35 = titanic[titanic['Age'] > 35]
above_35.head()

To select rows based on a conditional expression, use a condition inside the selection brackets []. The condition inside the selection brackets 
titanic["Age"] > 35
checks for which rows the Age column has a value greater than 35
(titanic['Age']>35).head() 

0    False
1     True
2     True
3    False
4    False
Name: Age, dtype: bool

The output of the conditional expression (>, but also ==, !=, <, <=,. . . would work) is actually a pandas Series of boolean values (either True or False) with the same number of rows as the original DataFrame. Such a Series of boolean values can be used to filter the DataFrame by putting it in between the selection brackets []. Only rows for which the value is True will be selected.

We know from before that the original Titanic DataFrame consists of 418 rows. Let’s have a look at the number of rows which satisfy the condition by checking the shape attribute of the resulting DataFrame above_35:
above_35.shape # (105, 11)

I’m interested in the Titanic passengers from cabin class 2 and 3.
class_23 = titanic[titanic['Pclass'].isin([2,3])]
class_23.head()

Similar to the conditional expression, the isin() conditional function returns a True for each row the values are in the provided list. To filter the rows based on such a function, use the conditional function inside the selection brackets []. In this case, the condition inside the selection brackets titanic["Pclass"].isin([2, 3]) checks for which rows the Pclass column is either 2 or 3.
The above is equivalent to filtering by rows for which the class is either 2 or 3 and combining the two statements with an | (or) operator:

class_32 = titanic[(titanic['Pclass'] == 2 ) | (titanic['Pclass'] == 3 )]

Note: When combining multiple conditional statements, each condition must be surrounded by parentheses (). Moreover, you can not use or/and but need to use the or operator | and the and operator &.

I want to work with passenger data for which the age is known.
age_not_na = titanic[titanic['Age'].notna()]
age_not_na

The notna() conditional function returns a True for each row the values are not an Null value. As such, this can be combined with the selection brackets [] to filter the data table.
You might wonder what actually changed, as the first 5 lines are still the same values. One way to verify is to check if the shape has changed:
age_not_na = titanic[titanic['Age'].notna()]
age_not_na.shape # (714, 12)

How do I select specific rows and columns from a DataFrame?
I’m interested in the names of the passengers older than 35 years.
adult_names = titanic.loc[titanic['Age']>35, 'Name']
adult_names.head()

In this case, a subset of both rows and columns is made in one go and just using selection brackets [] is not sufficient anymore. The loc/iloc operators are required in front of the selection brackets []. WHEN USING LOC/ILOC, THE PART BEFORE THE COMMA IS THE ROWS YOU WANT, AND THE PART AFTER THE COMMA IS THE COLUMNS YOU WANT TO SELECT.
When using the column names, row labels or a condition expression, use the loc operator in front of the selection brackets []. For both the part before and after the comma, you can use a single label, a list of labels, a slice of labels, a conditional expression or a colon. USING A COLON SPECIFIES YOU WANT TO SELECT ALL ROWS OR COLUMNS.

I’m interested in rows 10 till 25 and columns 3 to 5.
titanic.iloc[9:25, 2:5]

Again, a subset of both rows and columns is made in one go and just using selection brackets [] is not sufficient anymore. When specifically interested in certain rows and/or columns based on their position in the table, use the iloc operator in front of the selection brackets [].
When selecting specific rows and/or columns with loc or iloc, new values can be assigned to the selected data. For example, to assign the name anonymous to the first 3 elements of the third column:
titanic.iloc[:3,2] = 'anonymous'
titanic

Bullet Points for Indexing:
• When selecting subsets of data, square brackets [] are used.
• Inside these brackets, you can use a single column/row label, a list of column/row labels, a slice of labels, a conditional expression or a colon.
• Select specific rows and/or columns using loc when using the row and column names
• Select specific rows and/or columns using iloc when using the positions in the table
• You can assign new values to a selection based on loc/iloc.

For this tutorial, air quality data about 𝑁𝑂2 is used, made available by openaq and using the py-openaq package. The air_quality_no2.csv data set provides 𝑁𝑂2 values for the measurement stations FR04014, BETR801 and London Westminster in respectively Paris, Antwerp and London.
air_quality = pd.read_csv("data/air_quality_no2.csv", index_col=0, parse_dates=True)
air_quality.head()

	                station_antwerp	    station_paris	station_london
datetime			
2019-05-07 02:00:00	    NaN	                NaN	            23.0
2019-05-07 03:00:00	    50.5	            25.0	        19.0
2019-05-07 04:00:00	    45.0	            27.7	        19.0
2019-05-07 05:00:00	    NaN	                50.4	        16.0
2019-05-07 06:00:00	    NaN	                61.9        	NaN

air_quality.info()
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 1035 entries, 2019-05-07 02:00:00 to 2019-06-21 02:00:00
Data columns (total 3 columns):
 #   Column           Non-Null Count  Dtype  
---  ------           --------------  -----  
 0   station_antwerp  95 non-null     float64
 1   station_paris    1004 non-null   float64
 2   station_london   969 non-null    float64
dtypes: float64(3)
memory usage: 32.3 KB

Note: The usage of the index_col and parse_dates parameters of the read_csv function to define the first (0th) column as index of the resulting DataFrame and convert the dates in the column to Timestamp objects, respectively.

HOW TO CREATE PLOTS IN PANDAS?
------------------------------
I want a quick visual check of the data.
air_quality.plot()

With a DataFrame, pandas creates by default one 'line plot' for each of the columns with numeric data.

I want to plot only the columns of the data table with the data from Paris.
air_quality['station_paris'].plot()

To plot a specific column, use the selection method of the subset data in combination with the plot() method. Hence, the plot() method works on both Series and DataFrame.

air_quality.plot.scatter(x='station_london', y='station_paris', alpha=0.7)
# alpha make scatters solid as it reaches 1.

Apart from the default line plot when using the plot function, a number of alternatives are available to plot data. Let’s use some standard Python to get an overview of the available plot methods:
code:
[
    method_name
    for method_name in dir(air_quality.plot)
    if not method_name.startswith("_")
]
['area',
 'bar',
 'barh',
 'box',
 'density',
 'hexbin',
 'hist',
 'kde',
 'line',
 'pie',
 'scatter']

Note: In many development environments as well as IPython and Jupyter Notebook, use the TAB button to get an overview of the available methods, for example air_quality.plot. + TAB.

One of the options is DataFrame.plot.box(), which refers to a boxplot. The box method is applicable on the air quality example data:
air_quality.plot.box()
air_quality.describe() # to understand the boxplot.

I want each of the columns in a separate subplot.
axs = air_quality.plot.area(figsize=(12,6) , subplots=True)

Separate subplots for each of the data columns are supported by the subplots argument of the plot functions. The builtin options available in each of the pandas plot functions that are worthwhile to have a look.

I want to further customize, extend or save the resulting plot.
fig, axs = plt.subplots(figsize = (12, 4))
air_quality.plot.area(ax=axs)
axs.set_ylabel('NO$_2$ Concentration')
fig.savefig("no2_concentration.png")

Each of the plot objects created by pandas is a matplotlib object. As Matplotlib provides plenty of options to customize plots, making the link between pandas and Matplotlib explicit enables all the power of matplotlib to the plot. This strategy is applied in the previous example:

fig, axs = plt.subplots(figsize=(12, 4)) ## Create an empty matplotlib Figure and Axes
air_quality.plot.area(ax=axs) ## Use pandas to put the area plot on the prepared Figure/Axes
axs.set_ylabel("NO$_2$ concentration") ## Do any matplotlib customization you like
fig.savefig("no2_concentrations.png") ## Save the Figure/Axes using the existing matplotlib method.

Bullet Points on plots:
• The .plot.* methods are applicable on both Series and DataFrames
• By default, each of the columns is plotted as a different element (line, boxplot,. .) • Any plot created by pandas is a Matplotlib object.

HOW TO CREATE NEW COLUMNS DERIVED FROM EXISTING COLUMNS?
--------------------------------------------------------
I WANT TO EXPRESS THE 𝑁𝑂2 CONCENTRATION OF THE STATION IN LONDON IN mg/m3

(If we assume temperature of 25 degrees Celsius and pressure of 1013 hPa, the conversion factor is 1.882)

air_quality['london_mg_per_cubic'] = air_quality['station_london']*1.882
air_quality.head()

To create a new column, use the [] brackets with the new column name at the left side of the assignment.
Note: The calculation of the values is done element_wise.This means all values in the given column are multiplied by the value 1.882 at once. You do not need to use a loop to iterate each of the rows!

I WANT TO CHECK THE RATIO OF THE VALUES IN PARIS VERSUS ANTWERP AND SAVE THE RESULT IN A NEW COLUMN
air_quality['ratio_paris_antwerp'] = (
    air_quality['station_paris'] / air_quality['station_antwerp']
)
air_quality.head()

The calculation is again element-wise, so the / is applied for the values in each row.
Also other mathematical operators (+, -, *, /) or logical operators (<, >, =,. . . ) work element wise.

I WANT TO RENAME THE DATA COLUMNS TO THE CORRESPONDING STATION IDENTIFIERS USED BY openAQ
air_quality_renamed = air_quality.rename(
    columns={
        'station_antwerp': 'BETR801',
        "station_paris": "FR04014",
        "station_london": "London Westminster",
    }
)
air_quality_renamed.head()

The rename() function can be used for both row labels and column labels. Provide a dictionary with the keys as the current names and the values as the new names to update the corresponding names.

The mapping should not be restricted to fixed names only, but can be a mapping function as well. For example, converting the column names to lowercase letters can be done using a function as well:

air_quality_renamed = air_quality_renamed.rename(columns=str.lower)
air_quality_renamed.head()

REMEMBER
* Create a new column by assigning the output to the DataFrame with a new column name in between the [].
* Operations are element-wise, no need to loop over rows.
* Use rename with a dictionary or function to rename row labels or column names.

HOW TO CALCULATE SUMMARY STATISTICS?
This tutorial uses the Titanic data set, stored as CSV. The data consists of the following data columns: • PassengerId: Id of every passenger.
• Survived: This feature have value 0 and 1. 0 for not survived and 1 for survived.
• Pclass: There are 3 classes: Class 1, Class 2 and Class 3.
• Name: Name of passenger.
• Sex: Gender of passenger.
• Age: Age of passenger.
• SibSp: Indication that passenger have siblings and spouse. • Parch: Whether a passenger is alone or have family.
• Ticket: Ticket number of passenger. • Fare: Indicating the fare.
• Cabin: The cabin of passenger.
• Embarked: The embarked category.

titanic = pd.read_csv('data/titanic.csv')
titanic.head()

AGGREGATING STATISTICS
-----------------------
What is the average age of the Titanic passengers?
titanic['Age'].mean() # 29.699

Different statistics are available and can be applied to columns with numerical data. Operations in general exclude missing data and operate across rows by default.

What is the median age and ticket fare price of the Titanic passengers?
titanic[['Age', 'Fare']].median()
Age     28.0000
Fare    14.4542
dtype: float64

The statistic applied to multiple columns of a DataFrame is calculated for each numeric column.
The aggregating statistic can be calculated for multiple columns at the same time. Remember the describe function.
titanic[['Age', 'Fare']].describe()

Instead of the predefined statistics, specific combinations of aggregating statistics for given columns can be defined using the DataFrame.agg() method:
titanic.agg({
    'Age':['min', 'max', 'median', 'skew', 'mean'],
    'Fare':['min', 'max', 'median', 'skew', 'mean', 'std'],
})

AGGREGATING STATISTICS GROUPED BY CATEGORY
-------------------------------------------
What is the average age for male versus female Titanic passengers?
titanic[['Sex','Age']].groupby('Sex').mean()
	        Age
Sex	
female	27.915709
male	30.726645

As our interest is the average age for each gender, a subselection on these two columns is made first: titanic[[ "Sex", "Age"]]. Next, the groupby() method is applied on the Sex column to make a group per category. The average age for each gender is calculated and returned.
Calculating a given statistic (e.g. mean age) for each category in a column (e.g. male/female in the Sex column) is a common pattern. The groupby method is used to support this type of operations. More general, this fits in the more general split-apply-combine pattern:
• Split the data into groups
• Apply a function to each group independently 
• Combine the results into a data structure

Split - Sex in Male and Female
Apply - Mean function on each group splitted
Combine - Combbine the result and present

The apply and combine steps are typically done together in pandas.

In the previous example, we explicitly selected the 2 columns first. If not, the mean method is applied to each column containing numerical columns:
titanic.groupby('Sex').mean()


It does not make much sense to get the average value of the Pclass. if we are only interested in the average age for each gender, the selection of columns (rectangular brackets [] as usual) is supported on the grouped data as well:
titanic.groupby('Sex')['Age'].mean()

Why option is Faster?
%timeit titanic.groupby('Sex')['Age'].mean() # 801 µs ± 45.4 µs
%timeit titanic[['Sex','Age']].groupby('Sex').mean() # 24.1 ms ± 282 µs

Note: ThePclass column contains numerical data butactually represents 3 categories(orfactors) respectively the labels ‘1’, ‘2’ and ‘3’. Calculating statistics on these does not make much sense. Therefore, pandas provides a Categorical data type to handle this type of data.

What is the mean ticket fare price for each of the sex and cabin class combinations?
titanic.groupby(['Sex', 'Pclass'])['Fare'].mean()
Grouping can be done by multiple columns at the same time. Provide the column names as a list to the groupby() method.

COUNT NUMBER OF RECORDS BY CATEGORY
What is the number of passengers in each of the cabin classes?
titanic.Pclass.value_counts()
The function is a shortcut, as it is actually a groupby operation in combination with counting of the number of records within each group:
titanic.groupby('Pclass')['Pclass'].count()

Why one is faster?
%timeit titanic.Pclass.value_counts() # 389 µs ± 44.5 µs per loop
%timeit titanic.groupby('Pclass')['Pclass'].count() # 530 µs ± 38.6 µs per loop

Note: Both size and count can be used in combination with groupby.Where as size includes NaN values and just provides the number of rows (size of the table), count excludes the missing values. In the value_counts method, use the dropna argument to include or exclude the NaN value

REMEMBER
* Aggregation statistics can be calculated on entire columns or rows
* groupby provides the power of the split-apply-combine pattern
* value_counts is a convenient shortcut to count the number of entries in each category of a variable.  

HOW TO RESHAPE THE LAYOUT OF TABLES?
------------------------------------
This tutorial uses air quality data about 𝑁𝑂2 and Particulate matter less than 2.5 micrometers, made available by openaq and using the py-openaq package. The air_quality_long.csv data set provides 𝑁 𝑂2 and 𝑃 𝑀25 values for the measurement stations FR04014, BETR801 and London Westminster in respectively Paris, Antwerp and London.
The air-quality data set has the following columns:
• city: city where the sensor is used, either Paris, Antwerp or London
• country: country where the sensor is used, either FR, BE or GB
• location: the id of the sensor, either FR04014, BETR801 or London Westminster
• parameter: the parameter measured by the sensor, either 𝑁 𝑂2 or Particulate matter • value: the measured value
• unit: the unit of the measured parameter, in this case ‘μg/m3’
and the index of the DataFrame is datetime, the datetime of the measurement.

Note: The air-quality data is provided in a so-called long format data representation with each observation on a separate row and each variable a separate column of the data table. The long/narrow format is also known as the tidy data format.

tidy data - https://www.jstatsoft.org/article/view/v059i10
(A huge amount of effort is spent cleaning data to get it ready for analysis, but there has been little research on how to make data cleaning as easy and effective as possible. This paper tackles a small, but important, component of data cleaning: data tidying. Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets. This structure also makes it easier to develop tidy tools for data analysis, tools that both input and output tidy datasets. The advantages of a consistent data structure and matching tools are demonstrated with a case study free from mundane data manipulation chores.)

air_quality = pd.read_csv('data/air_quality_long.csv', index_col='date.utc', parse_dates=True) # date.utc is a column in dataset
air_quality.head()

How to reshape the layout of tables? 

SORT TABLE ROWS

I want to sort the Titanic data according to the age of the passengers.
titanic.sort_values(by='Age').head() ## KDB equivalent - `age xasc select from ('',(,)",")0:`titanic.csv

I want to sort the Titanic data according to the cabin class and age in descending order.
titanic.sort_values(by=['Pclass','Age'], ascending=False)

With Series.sort_values(), the rows in the table are sorted according to the defined column(s). The index will follow the row order.

LONG TO WIDE TABLE FORMAT
-------------------------
Let’s use a small subset of the air quality data set. We focus on 𝑁 𝑂2 data and only use the first two measurements of each location (i.e. the head of each group). The subset of data will be called no2_subset

 # filter for no2 data only
no2 = air_quality[air_quality['parameter'] == 'no2']
no2.head()

# use 2 measurements (head) for each location (groupby)
no2_subset = no2.sort_index().groupby(['location']).head(2)
no2_subset

I want the values for the three stations(location) as separate columns next to each other
no2_subset.pivot(columns='location', values='value') ## OR
no2_subset.pivot(columns='location')['value']

Which one is faster?
%timeit no2_subset.pivot(columns='location', values='value') # 2.34 ms ± 306 µs per loop
%timeit no2_subset.pivot(columns='location')['value'] # 5.6 ms ± 204 µs per loop

The pivot() function is purely reshaping of the data: a single value for each index/column combination is required.

As pandas support plotting of multiple columns out of the box, the conversion from long to wide table format enables the plotting of the different time series at the same time.
no2 = air_quality[air_quality['parameter'] == 'no2']
no2.head()
no2.pivot(columns='location', values='value').plot(figsize = (12, 6))

Note: When the index parameter is not defined, the existing index (rowlabels) is used.

PIVOT TABLE
I want the mean concentrations for 𝑁𝑂2 and 𝑃𝑀2.5 in each of the stations(locations) in table form.
air_quality = pd.read_csv('data/air_quality_long.csv', index_col='date.utc', parse_dates=True) # date.utc is a column in dataset

air_quality.pivot_table(columns='parameter', values='value', index='location', aggfunc='mean')

Difference between pivot() and pivot_table()?
In the case of pivot(), the data is only rearranged. When multiple values need to be aggregated (in this specific case, the values on different time steps) pivot_table() can be used, providing an aggregation function (e.g. mean) on how to combine these values.

Pivot table is a well known concept in spreadsheet software. When interested in summary columns for each variable separately as well, put the margin parameter to True:
air_quality.pivot_table(columns='parameter', values='value', 
                        index='location', aggfunc='mean', margins=True,)

Note: In case you are wondering, pivot_table() is indeed directly linked to groupby().The same result can be derived by grouping on both parameter and location:
air_quality.groupby(['parameter', 'location']).mean()

WIDE TO LONG FORMAT
no2_pivoted = no2.pivot(columns="location", values='value').reset_index()
no2_pivoted.head()

I want to collect all air quality 𝑁𝑂2 measurements in a single column (long format)
no2_pivoted.melt(id_vars='date.utc')

The pandas.melt() method on a DataFrame converts the data table from wide format to long format. The column headers become the variable names in a newly created column.

The solution is the short version on how to apply pandas.melt(). The method will melt all columns NOT mentioned in id_vars together into two columns: A column with the column header names and a column with the values itself. The latter column gets by default the name value.

The pandas.melt() method can be defined in more detail
no2 = no2_pivoted.melt( id_vars='date.utc', value_vars=['BETR801','FR04014','London Westminster',], value_name='NO2', var_name='id_location', )
no2.head()

The result in the same, but in more detail defined:
• value_vars  - defines explicitly which columns to melt together
• value_name  - provides a custom column name for the values column instead of the default column name value
• var_name  - provides a custom column name for the column collecting the column header names. Otherwise it takes the index name or a default variable

Hence, the arguments value_name and var_name are just user-defined names for the two generated columns. The columns to melt are defined by id_vars and value_vars.

REMEMBER
* Sorting by one or more columns is supported by sort_values
* The pivot function is purely restructuring of the data, pivot_table supports aggregations
* The reverse of pivot (long to wide format) is melt (wide to long format)

HOW TO COMBINE DATA FROM MULTIPLE TABLES?
------------------------------------------
DataSets:
air_quality_no2 = pd.read_csv("data/air_quality_no2_long.csv", parse_dates=True)
air_quality_no2.head()
air_quality_no2 = air_quality_no2[[ 'date.utc', 'location',
                                  'parameter', 'value', ]]
air_quality_no2.head()

The air_quality_pm25_long.csv data set provides 𝑃𝑀25 values for the measurement stations FR04014, BETR801 and London Westminster in respectively Paris, Antwerp and London.
air_quality_pm25 = pd.read_csv("data/air_quality_pm25_long.csv",parse_dates=True)
air_quality_pm25.head()
air_quality_pm25 = air_quality_pm25[[ 'date.utc', 'location',
                                  'parameter', 'value', ]]
air_quality_pm25.head()

How to combine data from multiple tables? 
CONCATENATING OBJECTS
I want to combine the measurements of 𝑁𝑂2 and 𝑃𝑀25, two tables with a similar structure, in a single table

air_quality = pd.concat([air_quality_pm25, air_quality_no2], axis=0)
air_quality

The concat() function performs concatenation operations of multiple tables along one of the axis (row-wise or column-wise).
By default concatenation is along axis 0, so the resulting table combines the rows of the input tables. Let’s check the shape of the original and the concatenated tables to verify the operation:

print('Shape of the ``air_quality_pm25`` table', air_quality_pm25.shape)
print('Shape of the ``air_quality_no2`` table', air_quality_no2.shape)
print('Shape of resulting ``air_quality`` table', air_quality.shape)

Shape of the ``air_quality_pm25`` table (1110, 4)
Shape of the ``air_quality_no2`` table (2068, 4)
Shape of resulting ``air_quality`` table (3178, 4)

Hence, the resulting table has 3178 = 1110 + 2068 rows

Note: The axis argument will return in a number of pandas methods that can be applied along an axis. A DataFrame has two corresponding axes: the first running vertically downwards across rows (axis 0), and the second running horizontally across columns (axis 1). Most operations like concatenation or summary statistics are by default across rows (axis 0), but can be applied across columns as well.

Sorting the table on the datetime information illustrates also the combination of both tables, with the parameter column defining the origin of the table (either no2 from table air_quality_no2 or pm25 from table air_quality_pm25):

air_quality.sort_values('date.utc')

In this specific example, the parameter column provided by the data ensures that each of the original tables can be identified. This is not always the case. the concat function provides a convenient solution with the keys argument, adding an additional (hierarchical) row index. For example:

air_quality_ = pd.concat([air_quality_pm25, air_quality_no2], keys=["PM25", 'NO2',])
air_quality_

Note: The existence of multiple row/column indices at the same time has not been mentioned within these tutorials. Hierarchical indexing or MultiIndex is an advanced and powerful pandas feature to analyze higher dimensional data.
Multi-indexing is out of scope for this pandas introduction. For the moment, remember that the func- tion reset_index can be used to convert any level of an index to a column, e.g. air_quality. reset_index(level=0)

air_quality_ = pd.concat([air_quality_pm25, air_quality_no2], keys=["PM25", 'NO2',])
air_quality_
air_quality_.reset_index(level=0)

JOIN TABLES USING A COMMON IDENTIFIER

We have to Add the station coordinates, provided by the stations metadata table, to the corresponding rows in the measurements table.

Note: The stations used in this example (FR04014,BETR801 and LondonWestminster)are just three entries enlisted in the metadata table. We only want to add the coordinates of these three to the measurements table, each on the corresponding rows of the air_quality table.
stations_coord = pd.read_csv("data/air_quality_stations.csv")
stations_coord.head()

air_quality.head()

air_quality = pd.merge(air_quality, stations_coord, how='left', on='location')
air_quality.head()

Using the merge() function, for each of the rows in the air_quality table, the corresponding coordinates are added from the air_quality_stations_coord table. Both tables have the column location in common which is used as a key to combine the information. By choosing the left join, only the locations available in the air_quality (left) table, i.e. FR04014, BETR801 and London Westminster, end up in the resulting table. The merge function supports multiple join options similar to database-style operations.

I want to Add the parameter full description and name, provided by the parameters metadata table, to the measurements table

air_quality_parameters = pd.read_csv("data/air_quality_parameters.csv")
air_quality_parameters.head()

air_quality.head()

air_quality = pd.merge(air_quality, air_quality_parameters, how='left',
                      left_on='parameter', right_on='id')
air_quality.head()

Compared to the previous example, there is no common column name. However, the parameter column in the air_quality table and the id column in the air_quality_parameters_name both provide the measured variable in a common format. The left_on and right_on arguments are used here (instead of just on) to make the link between the two tables.
pandas supports also inner, outer, and right joins.

REMEMBER
* Multiple tables can be concatenated both column-wise and row-wise using the concat function.
* For database-like merging/joining of tables, use the merge function.

HOW TO HANDLE TIME SERIES DATA WITH EASE?
=========================================
For this tutorial, air quality data about 𝑁𝑂2 and Particulate matter less than 2.5 micrometers is used, made available by openaq and downloaded using the py-openaq package. The air_quality_no2_long.csv" data set provides 𝑁 𝑂2 values for the measurement stations FR04014, BETR801 and London Westminster in respectively Paris, Antwerp and London.

air_quality = pd.read_csv("data/air_quality_no2_long.csv")
air_quality.rename(columns = {'date.utc': 'datetime'})
air_quality.head()
air_quality.city.unique()

air_quality.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 2068 entries, 0 to 2067
Data columns (total 7 columns):
 #   Column     Non-Null Count  Dtype  
---  ------     --------------  -----  
 0   city       2068 non-null   object 
 1   country    2068 non-null   object 
 2   datetime   2068 non-null   object

Currently, datetime is object type.

Using pandas datetime properties
---------------------------------
I want to work with the dates in the column datetime as datetime objects instead of plain text.

air_quality['datetime'] = pd.to_datetime(air_quality.datetime)
air_quality.datetime.dtype # datetime64[ns, UTC]

Initially, the values in datetime are character strings and do not provide any datetime operations (e.g. extract the year, day of the week,. . . ). By applying the to_datetime function, pandas interprets the strings and convert these to datetime (i.e. datetime64[ns, UTC]) objects. In pandas we call these datetime objects similar to datetime. datetime from the standard library as pandas.Timestamp.

Note: Asmanydatasetsdocontaindatetimeinformationinoneofthecolumns,pandasinputfunctionlikepandas. read_csv() and pandas.read_json() can do the transformation to dates when reading the data using the parse_dates parameter with a list of the columns to read as Timestamp:
pd.read_csv("../data/air_quality_no2_long.csv", parse_dates=["datetime"])

Why are these pandas.Timestamp objects useful? 
Let’s illustrate the added value with some example cases. What is the start and end date of the time series data set we are working with?
air_quality.datetime.min(), air_quality.datetime.max()
(Timestamp('2019-05-07 01:00:00+0000', tz='UTC'),
 Timestamp('2019-06-21 00:00:00+0000', tz='UTC'))

 Using pandas.Timestamp for datetimes enables us to calculate with date information and make them comparable. Hence, we can use this to get the length of our time series:
 air_quality.datetime.max() -  air_quality.datetime.min()
 # Timedelta('44 days 23:00:00')

 The result is a pandas.Timedelta object, similar to datetime.timedelta from the standard Python library and defining a time duration.

 I want to add a new column to the DataFrame containing only the month of the measurement
air_quality['month'] = air_quality.datetime.dt.month
air_quality.head()

By using Timestamp objects for dates, a lot of time-related properties are provided by pandas. For example the month, but also year, weekofyear, quarter,. . . All of these properties are accessible by the dt accessor.

What is the average 𝑁𝑂2 concentration for each day of the week for each of the measurement locations?
air_quality.groupby([air_quality.datetime.dt.weekday, 'location'])['value'].mean()

Here, we want to calculate a given statistic (e.g. mean 𝑁𝑂2) for each weekday and for each measurement location. To group on weekdays, we use the datetime property weekday (with Monday=0 and Sunday=6) of pandas Timestamp, which is also accessible by the dt accessor. The grouping on both locations and weekdays can be done to split the calculation of the mean on each of these combinations.

Plot the typical 𝑁𝑂2 pattern during the day of our time series of all stations together. In other words, what is the average value for each hour of the day?

fig, axs = plt.subplots(figsize=(12,4))
air_quality.groupby(air_quality.datetime.dt.hour)['value'].mean().plot(
    kind='bar', rot=270, ax=axs)
plt.xlabel('Hour of the day')
plt.ylabel('$NO_2 (μg/m^3)$')
fig.savefig('hourlyPollution.png')

Similar to the previous case, we want to calculate a given statistic (e.g. mean 𝑁𝑂2) for each hour of the day and we can use the split-apply-combine approach again. For this case, we use the datetime property hour of pandas Timestamp, which is also accessible by the dt accessor.

Datetime as index
------------------
In the tutorial on reshaping, pivot() was introduced to reshape the data table with each of the measurements locations as a separate column:
no_2 = air_quality.pivot(index='datetime', columns='location', values='value')
no_2.head()

Note: By pivoting the data, the datetime information became the index of the table. In general, setting a column as an index can be achieved by the set_index function.

Working with a datetime index (i.e. DatetimeIndex) provides powerful functionalities. For example, we do not need the dt accessor to get the time series properties, but have these properties available on the index directly:
no_2.index.year, no_2.index.month_name

Some other advantages are the convenient subsetting of time period or the adapted time scale on plots. Let’s apply this on our data.

I want to Create a plot of the 𝑁𝑂2 values in the different stations from the 20th of May till the end of 21st of May
no_2['2019-05-20':'2019-05-21'].plot(figsize=(12,6))

By providing a string that parses to a datetime, a specific subset of the data can be selected on a DatetimeIndex.

RESAMPLE A TIME SERIES TO ANOTHER FREQUENCY
--------------------------------------------
Aggregate the current hourly(currently the dataset consists of hourly values) time series values to the monthly maximum value in each of the stations.

monthly_max = no_2.resample('M').max()
monthly_max

A very powerful method on time series data with a datetime index, is the ability to resample() time series to another frequency (e.g., converting secondly data into 5-minutely data).
The resample() method is similar to a groupby operation:
• it provides a time-based grouping, by using a string (e.g. M, 5H,. . . ) that defines the target frequency 
• it requires an aggregation function such as mean, max,. . .

no_2.resample('6H').mean().plot(figsize=(12,6))

An overview of the aliases used to define time series frequencies is given in the offset aliases overview table. When defined, the frequency of the time series is provided by the freq attribute:

monthly_max.index.freq # <MonthEnd>
monthly_max.index.freqstr # 'M'

I want to Make a plot of the daily mean 𝑁𝑂2 value in each of the stations.
no_2.resample('D').mean().plot(style='-o', figsize=(12,6))

REMEMBER
* Valid date strings can be converted to datetime objects using to_datetime function or as part of read functions.
* Datetime objects in pandas support calculations, logical operations and convenient date-related properties using the dt accessor.
* A DatetimeIndex contains these date-related properties and supports convenient slicing.
* Resample is a powerful method to change the frequency of a time series.

HOW TO MANIPULATE TEXTUAL DATA?
This tutorial uses the Titanic data set, stored as CSV.
titanic = pd.read_csv("data/titanic.csv")

Make all name characters lowercase.
To make each of the strings in the Name column lowercase, select the Name column, add the str accessor and apply the lower method. As such, each of the strings is converted element-wise.

titanic.Name.str.lower()

Create a new column Surname that contains the surname of the passengers by extracting the part before the comma.

titanic.Name.str.split(',')

Using the Series.str.split() method, each of the values is returned as a list of 2 elements. The first element is the part before the comma and the second element is the part after the comma.

titanic['Surname'] = titanic.Name.str.split(',').str.get(0)
titanic.head()

As we are only interested in the first part representing the surname (element 0), we can again use the str accessor and apply Series.str.get() to extract the relevant part. Indeed, these string functions can be concatenated to combine multiple functions at once!

I want to Extract the passenger data about the countesses on board of the Titanic.

titanic[titanic.Name.str.contains('Countess')]

The string method Series.str.contains() checks for each of the values in the column Name if the string contains the word Countess and returns for each of the values True (Countess is part of the name) or False (Countess is not part of the name). This output can be used to subselect the data using conditional (boolean) indexing introduced in the subsetting of data tutorial. As there was only one countess on the Titanic, we get one row as a result.

Note: More powerful extractions on strings are supported, as the Series.str.contains() and Series. str.extract() methods accept regular expressions,

Which passenger of the Titanic has the longest name?
To get the longest name we first have to get the lengths of each of the names in the Name column. By using pandas string methods, the Series.str.len() function is applied to each of the names individually (element-wise).

titanic.Name.str.len().idxmax()

Next, we need to get the corresponding location, preferably the index label, in the table for which the name length is the largest. The idxmax() method does exactly that. It is not a string method and is applied to integers, so no str is used.

titanic.iloc[titanic.Name.str.len().idxmax()]['Name']
titanic.loc[titanic.Name.str.len().idxmax()]['Name']

Based on the index name of the row (307) and the column (Name), we can do a selection using the loc or iloc operator.

In the “Sex” column, replace values of “male” by “M” and values of “female” by “F”.

titanic['Sex_short'] = titanic.Sex.replace({'male':'M', 'female':'F'})
titanic.head(2)

Whereas replace() is not a string method, it provides a convenient way to use mappings or vocabularies to translate certainvalues.Itrequiresadictionarytodefinethemapping{from : to}.

Warning: There is also a replace() method available to replace a specific set of characters. However, when having a mapping of multiple values, this would become:
titanic["Sex_short"] = titanic["Sex"].str.replace("female", "F")
titanic["Sex_short"] = titanic["Sex_short"].str.replace("male", "M")
This would become cumbersome and easily lead to mistakes. Just think (or try out yourself) what would happen if those two statements are applied in the opposite order.

REMEMBER
* String methods are available using the str accessor.
* String methods work element-wise and can be used for conditional indexing.
* The replace method is a convenient method to convert values according to a given dictionary.


=================================Out of the box================================
||Accessors||
=============
Pandas is a widely-used data analysis and manipulation library in Python. It provides numerous functions and methods to work with any type of data. There are also methods that work only with a specific data type. These methods are accessed through 4 accessors.
The accessors extend the capabilities of Pandas and provide specific operations. For instance, extracting the month from the date can be done using the dt accessor.
In this post, we will see various operations with 4 accessors of Pandas which are:
str: String data type
    Eg: Get only the row which have alphabets from a series.
            a = pd.Series(['a',10,'bcd1',20,'efs'])
            a[a.str.isalpha().replace(np.nan, False)]

cat: Categorical data type
    For categorical data it is more efficient to work with categorical datatype than using the object datatype. It makes a significant difference in terms of memory and speed especially when the data has low cardinality(i.e number of categories is low compared to the number of observations)

    cate = pd.Series(['A','B','A','A','B','C'], dtype='category')
    cate.cat.categories # Index(['A', 'B', 'C'], dtype='object')
    cate.cat.rename_categories({'A':1, 'B':2, 'C':3})
    #cate[0]='D' # ValueError: Cannot setitem on a Categorical with a new category, set the categories first
    cate.cat.add_categories('D', inplace=True)
    cate[0]='D'
    cate

dt: Datetime, Timedelta, Period data types
    dts = pd.Series(pd.date_range('2021.01.01', periods=5, freq='10D'))
        dts.dt.day
        dts.dt.year
        dts.dt.month
        dts.dt.date
        dts.dt.hour
        dts.dt.minute
        dts.dt.second
        dts.dt.weekday
        dts.dt.isocalendar().week
        dts.dt.is_month_start

sparse: Sparse data type

=====================================================================================

CHAPTER 2 - USER GUIDE
------------------------
2.1.1 Object creation

Creating a Series by passing a list of values, letting pandas create a default integer index:
s = pd.Series([1,3,5,np.nan,6,8])

pd.date_range() - Return a fixed frequency DatetimeIndex.
    start=None, end=None, periods=None, freq=None, tz=None, normalize=False, name=None, closed=None, **kwarg

Creating a DataFrame by passing a NumPy array, with a datetime index and labeled columns:
dates = pd.date_range("20210101", periods=6, freq='2D')
df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list("ABCD"))
df

df2 = pd.DataFrame({
    'A':1.0,
    'B':pd.Timestamp('20210102'),
    'C':pd.Series(1, index=list(range(4)), dtype='float32'),
    'D':np.array([3]*4, dtype='int32'),
    'E':pd.Categorical(['test', 'train', 'test', 'train', ]),
    'F':'foo',
})
df2.info()

The columns of the resulting DataFrame have different dtypes.
df2.dtypes


DataFrame.to_numpy() gives a NumPy representation of the underlying data. Note that this can be an expensive operation when your DataFrame has columns with different data types, which comes down to a fundamental differ- ence between pandas and NumPy: NumPy arrays have one dtype for the entire array, while pandas DataFrames have one dtype per column. When you call DataFrame.to_numpy(), pandas will find the NumPy dtype that can hold all of the dtypes in the DataFrame. This may end up being object, which requires casting every value to a Python object.

For df, our DataFrame of all floating-point values, DataFrame.to_numpy() is fast and doesn’t require copying data.
df.to_numpy()

For df2, the DataFrame with multiple dtypes, DataFrame.to_numpy() is relatively expensive.
df2.to_numpy()
array([[1.0, Timestamp('2021-01-02 00:00:00'), 1.0, 3, 'test', 'foo'],
       [1.0, Timestamp('2021-01-02 00:00:00'), 1.0, 3, 'train', 'foo'],
       [1.0, Timestamp('2021-01-02 00:00:00'), 1.0, 3, 'test', 'foo'],
       [1.0, Timestamp('2021-01-02 00:00:00'), 1.0, 3, 'train', 'foo']],
      dtype=object)

Note: DataFrame.to_numpy()does not include the index orc olumn labels in the output.

2.1.2 Viewing data
-------------------
Transposing your data:
df.T

Sorting by an axis:
df.sort_index(axis=1, ascending=False)

                D	        C	        B	        A
2021-01-01	-0.030514	-0.875529	-0.055480	-0.503135
2021-01-03	0.122823	-0.488960	-1.026299	-0.982295

df.sort_index(ascending=False)

                A	        B	        C	        D
2021-01-11	-0.537115	0.030939	-0.311905	0.598476
2021-01-09	-0.873883	0.678836	-0.255343	-0.826411

Sorting by values:
df.sort_values('B')

2.1.3 Selection
----------------
Note: While standard Python / Numpy expressions for selecting and setting are intuitive and come in handy for interactive work, for production code, we recommend the optimized pandas data access methods, .at, .iat, .loc and .iloc.

Getting
  Selecting a single column, which yields a Series, equivalent to df.A:
  df['A']

 Selecting via [], which slices the rows.
  df[0:3]
  df["20210102":"20210106"]

 Selection by label
 ------------------
 For getting a cross section using a label:
 df.loc[dates[0]]

Selecting on a multi-axis by label:
df[['A','B']] ~ df.loc[:,['A','B']]

Showing label slicing, both endpoints are included:
df.loc['20210101':'20210105',['A','C']]

Reduction in the dimensions of the returned object:
df.loc['2021-01-03',['A','C']]

For getting a scalar value
df.loc[dates[0], 'A'] # -0.5031

For getting fast access to a scalar (equivalent to the prior method):
df.at[dates[0], 'A'] # -0.5031

which one is faster?
%timeit df.loc[dates[0], 'A'] # 30.3 µs ± 1.27
%timeit df.at[dates[0], 'A'] # 23.4 µs ± 5.19 µs

Selection by position(index)
-----------------------------
Select via the position of the passed integers:
df.iloc[3]

By integer slices, acting similar to numpy/Python:
df.iloc[2:5, 2:4]

By lists of integer position locations, similar to the NumPy/Python style:
df.iloc[[1,2,4], [0,2]]

For slicing rows explicitly:
df.iloc[2:5,]

For slicing columns explicitly:
df.iloc[:, 2:4]

For getting a value explicitly:
df.iloc[2,2] # 0.79

For getting fast access to a scalar (equivalent to the prior method):
df.iat[2,2] # 0.790

Boolean indexing
-----------------
Using a single column’s values to select data.
df[df['A']>0]

Selecting values from a DataFrame where a boolean condition is met.
df[df>0]

Using the isin() method for filtering:
df2 = df.copy()
df2['E'] = [ 'one', 'one', 'two', 'three', 'four', 'three', ]
df2[df2.E.isin(['two', 'four'])]

Setting
-------
Setting a new column automatically aligns the data by the indexes.
s1 = pd.Series(np.arange(1,7), index=pd.date_range('20210101', periods=6))
s1

Setting values by label:
df.at[dates[0], 'A'] = 0

Setting values by position:
df.iat[0, 1] = 0

Setting by assigning with a NumPy array:
df['D'] = np.array([3] * len(df))

A where operation with setting.
df2 = df.copy()
df2[df2>0] = -df2

df2 = df.copy()
df2 = df2.where(df2<0,-df2)

2.1.4 Missing data
-------------------
pandas primarily uses the value np.nan to represent missing data. It is by default not included in computations.

Reindexing allows you to change/add/delete the index on a specified axis. This returns a copy of the data.

reindex - assign new index to existing dataframe.

df1 = df.reindex(index=dates[0:4], columns=list(df.columns)+['E'])
df1.loc[dates[0:2], 'E'] = 1

To drop any rows that have missing data.
df.dropna(how = 'any') ~ df.dropna()

which one is faster?
%timeit df.dropna() # 1.59 ms ± 13.5 µs per loop
%timeit df.dropna(how = 'any') # 1.67 ms ± 50.9 µs per loop

%timeit titanic.dropna() # 1.97 ms ± 117 µs per loop
%timeit titanic.dropna(how='any') # 2.2 ms ± 196 µs per loop

Filling missing data.
df1.fillna(value=5) ~ df1.fillna(5)

To get the boolean mask where values are nan.
pd.isna(df1)

2.1.5 Operations
----------------
Stats
Operations in general exclude missing data.
df.mean() # To get mean of each column

Same operation on the other axis:
df.mean(axis=1) ~ df.mean(1) # To get mean of each row
df.mean.mean() # To get a scaler which is mean of complete dataframe

Operating with objects that have different dimensionality and need alignment. In addition, pandas automatically broadcasts along the specified dimension.

s = pd.Series([1,3,5,np.nan,6,8], index=dates).shift(2)
df.sub(s, axis='index')
df.rsub(s, axis='index')

Apply
------
Applying functions to the data:
df.apply(np.cumprod)

df.apply(lambda x: x.max() - x.min())
df.apply(lambda x: x.max() - x.min(), axis=1)

Histogramming:
get the frequency.

s = pd.Series(np.random.randint(0,7,size=10))
s.value_counts()

String Methods
--------------
Series is equipped with a set of string processing methods in the str accessor that make it easy to operate on each element of the array, as in the code snippet below. Note that pattern-matching in str generally uses regular expressions by default (and in some cases always uses them).

s = pd.Series(['A','B','V','CABA',np.nan, 'RAVA'])
s.str.lower()

2.1.6 Merge
------------
Concat
------
pandas provides various facilities for easily combining together Series and DataFrame objects with various kinds of set logic for the indexes and relational algebra functionality in the case of join / merge-type operations.
Concatenating pandas objects together with concat():

df = pd.DataFrame(np.random.randn(10,4))
# let's break it into pieces
pieces = [df[:3], df[3:7], df[7:]]
pd.concat(pieces)

Note: Adding a column to a DataFrame is relatively fast. However, adding a row requires a copy, and may be expensive. We recommend passing a pre-built list of records to the DataFrame constructor instead of building a DataFrame by iteratively appending records to it.

Join
-----
SQL style merges
left = pd.DataFrame({"key": ["foo", "foo"], "lval": [1, 2]})
right = pd.DataFrame({"key": ["foo", "foo"], "rval": [4, 5]})
pd.merge(left, right, on="key")

2.1.7 Grouping
--------------
By “group by” we are referring to a process involving one or more of the following steps: • Splitting the data into groups based on some criteria
• Applying a function to each group independently
• Combining the results into a data structure

df = pd.DataFrame(
    {
    "A": ["foo", "bar", "foo", "bar", "foo", "bar", "foo", "foo"],
    "B": ["one", "one", "two", "three", "two", "two", "one", "three"],
    "C": np.random.randn(8),
    "D": np.random.randn(8),
    }
)
Grouping and then applying the sum() function to the resulting groups.
df.groupby(["A", "B"]).sum()

2.1.8 Reshaping
---------------
tuples = list(zip(*[
    ["bar", "bar", "baz", "baz", "foo", "foo", "qux", "qux"],
    ["one", "two", "one", "two", "one", "two", "one", "two"],
]))

index = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])
df = pd.DataFrame(np.random.randn(8,2), index=index, columns=['A','B'])
df2 = df[:4]
df2

The stack() method “compresses” a level in the DataFrame’s columns.
stacked = df2.stack()
stacked
(It is similar to melt but it maintains index as well)
first  second   
bar    one     A    0.709423
               B   -0.276359
       two     A    0.771991
               B   -0.523291
baz    one     A    0.033710
               B   -0.099103
       two     A   -0.046292
               B    0.984227
dtype: float64

With a “stacked” DataFrame or Series (having a MultiIndex as the index), the inverse operation of stack() is unstack(), which by default unstacks the last level:

stacked.unstack()
                		A	       B
first	second		
bar	    one	        0.709423	-0.276359
        two	        0.771991	-0.523291
baz	    one	        0.033710	-0.099103
        two	       -0.046292	0.984227

stacked.unstack([0,1])
first	bar	baz
second	one	two	one	two
A	0.709423	0.771991	0.033710	-0.046292
B	-0.276359	-0.523291	-0.099103	0.984227

Pivot tables
------------
Reshape - Long to Wide

df = pd.DataFrame({
    "A": ["one", "one", "two", "three"] * 3,
    "B": ["A", "B", "C"] * 4,
    "C": ["foo", "foo", "foo", "bar", "bar", "bar"] * 2,
    "D": np.random.randn(12),
    "E": np.random.randn(12),
})
df.pivot_table(columns=['C'], values='D', index=['A','B'])

2.1.9 Time series
------------------
pandas has simple, powerful, and efficient functionality for performing resampling operations during frequency con- version (e.g., converting secondly data into 5-minutely data). This is extremely common in, but not limited to, financial applications.

rng = pd.date_range('2021/01/01', periods=100, freq='S')
ts = pd.Series(np.random.randint(0,500,len(rng)), index=rng)
ts.resample('5M').mean()

Time zone representation:
rng = pd.date_range('3/6/2020 00:00', periods=5)
ts = pd.Series(np.random.randn(len(rng)), index=rng)
ts_utc = ts.tz_localize('UTC')
Converting to another time zone:
ts_utc.tz_convert('US/Eastern')

Converting between time span representations:
rng = pd.date_range('3/6/2020 00:00', periods=5, freq='M')
ts = pd.Series(np.random.randn(len(rng)), index=rng)
ps = ts.to_period()
ps.to_timestamp()

Converting between period and timestamp enables some convenient arithmetic functions to be used. In the following example, we convert a quarterly frequency with year ending in November to 9am of the end of the month following the quarter end:

prng = pd.period_range('1990Q1', '2000Q4', freq='Q-NOV')
ts = pd.Series(np.random.randn(len(prng)), index=prng)
ts.index = (prng.asfreq('M','e')+1).asfreq('H','s')+9
ts

Get the first date od provided month.
pd.Timestamp('2020-01', freq='M').to_period().to_timestamp()

2.1.10 Categoricals
-------------------
pandas can include categorical data in a DataFrame.

df = pd.DataFrame({
    'id':np.arange(1,7),
    'raw_grade': [ 'a', 'b', 'b', 'a', 'a', 'e', ],
})

Convert the raw grades to a categorical data type.
df['grade']=df['raw_grade'].astype('category')

Rename the categories to more meaningful names (assigning to Series.cat.categories() is in place!).
df['grade'].cat.categories = ['vg', 'g', 'b']

Reorder the categories and simultaneously add the missing categories (methods under Series.cat() return a new Series by default).
df['grade'] = df['grade'].cat.set_categories(['vb','b','m','g','vg'])

Sorting is per order in the categories, not lexical order.
df.sort_values('grade')

Grouping by a categorical column also shows empty categories.
df.groupby('grade').count()

2.1.11 Plotting
----------------
We use the standard convention for referencing the matplotlib API:
ts = pd.Series(np.random.randn(1000), index=pd.date_range(
    '2019.01.01', periods=1000, ))
ts = ts.cumsum()
ts.plot()

On a DataFrame, the plot() method is a convenience to plot all of the columns with labels:
df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, 
                  columns=['A','B','C','D'])
df = df.cumsum()

plt.figure()
df.plot(figsize=(12,6))
plt.legend(loc='best')

2.1.12 Getting data in/out
CSV
Writing to a csv file. - df.to_csv("foo.csv")
Reading from a csv file. - pd.read_csv("foo.csv")

2.2 Intro to data structures
============================
We’ll start with a quick, non-comprehensive overview of the fundamental data structures in pandas to get you started. The fundamental behavior about data types, indexing, and axis labeling / alignment apply across all of the objects.

Here is a basic tenet to keep in mind: DATA ALIGNMENT IS INTRINSIC. The link between labels and data will not be broken unless done so explicitly by you.

===========What is Vectorization ===========================
https://realpython.com/numpy-array-programming/#getting-into-shape-intro-to-numpy-arrays

This practice of replacing explicit loops with array expressions is commonly referred to as vectorization. In general, vectorized array operations will often be one or two (or more) orders of magnitude faster than their pure Python equivalents, with the biggest impact [seen] in any kind of numerical computations.

When looping over an array or any data structure in Python, there’s a lot of overhead involved. Vectorized operations in NumPy delegate the looping internally to highly optimized C and Fortran functions, making for cleaner and faster Python code.

consider a 1-dimensional vector of True and False for which you want to count the number of “False to True” transitions in the sequence:
In simple words we need to count the number of cases in which True comes after False.
Eg:
x = np.array([False, False, True, True,False])
np.count_nonzero(x[:-1]<x[1:]) # 1 - only once True comes after false at position 3

Classic Python:
def get_count(x) -> int:
    count = 0
    for i,j in list(zip(x[:-1], x[1:])):
        if j and not i:
            count += 1
    return count
%timeit get_count(x) # 1.1 ms ± 19.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

numpy:
%timeit np.count_nonzero(x[:-1]<x[1:]) # 1.1 ms ± 19.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

Numpy solution is around 157 times faster.
from timeit import timeit

setup = 'from __main__ import count_transitions, x; import numpy as np'
num = 100
t1 = timeit('count_transitions(x)', setup = setup, number=num)
print(t1)
t2 = timeit('np.count_nonzero(x[:-1]<x[1:])', setup = setup, number=num)
print(t2)
print(f'Speed difference: {(t1/t2)}')

17.00445253200087
0.10794881399851874
Speed difference: 157.52329184676546

Question: Given a stock’s price history as a sequence, and assuming that you are only allowed to make one purchase and one sale, what is the maximum profit that can be obtained? For example, given prices = (20, 18, 14, 17, 20, 21, 15), the max profit would be 7, from buying at 14 and selling at 21.

6 solutions:
p = np.array([20, 18, 14, 17, 20, 21, 15])

def bruteforce(a) -> int:
    max_diff = 0
    for i in p:
        for j in p[1:]:
            t = i - j
            if t > max_diff:
                max_diff = t
    return max_diff

def ordern(a) -> int:
    max_val  = min_val = p[0]
    for i in p[1:]:
        if i>max_val:
            max_val = i
        elif i< min_val:
            min_val = i
    return max_val - min_val

def realordern(p) -> int:
    diff_px = 0
    min_px = p[0]
    for px in p[1:]:
        min_px = min(min_px, px)
        diff_px = max(px-min_px, diff_px)
    return diff_px

def basic(p) -> int:
    return max(p)-min(p)

from timeit import timeit

setup = 'from __main__ import realordern, ordern, p, basic, bruteforce; import numpy as np'
num = 10000
t6 = timeit('np.max(p - np.minimum.accumulate(p))', setup = setup, number=num)
print('numpy accumulate', t6)
t5 = timeit('np.max(p) - np.min(p)', setup = setup, number=num)
print('numpy max min', t5)
t4 = timeit('basic(p)', setup = setup, number=num)
print('python max min', t4)
t1 = timeit('realordern(p)', setup = setup, number=num)
print('solution suggested by realPython', t1)
t2 = timeit('ordern(p)', setup = setup, number=num)
print('My if else solution', t2)
t3 = timeit('bruteforce(p)', setup = setup, number=num)
print('BruteForce way ', t3)
print(min(t1, t2, t3, t4, t5 ,t6))

numpy accumulate 0.13581958799841232
numpy max min 0.10029465600018739
python max min 0.03708621999976458
solution suggested by realPython 0.051151576000847854
My if else solution 0.02522610599771724
BruteForce way  0.21499461300118128
0.02522610599771724

AXES can be confusing term so in short - summing an array for axis=0 collapses the rows of the array with a column-wise computation.

df = DataFrame({
    'a':[10,20],
    'b':[100, 200]
})
print(df)
df.sum(axis=0)

    a    b
0  10  100
1  20  200
a     30
b    300

random.normal(loc=0.0, scale=1.0, size=None)
Draw random samples from a normal (Gaussian) distribution.
The probability density function of the normal distribution, first derived by De Moivre and 200 years later by both Gauss and Laplace independently [2], is often called the bell curve because of its characteristic shape (see the example below).
The normal distributions occurs often in nature. For example, it describes the commonly occurring distribution of samples influenced by a large number of tiny, random disturbances, each with its own unique distribution [2].

http://scipy-lectures.org/intro/numpy/array_object.html#what-are-numpy-and-numpy-arrays

Interactive help for numpy
np.array? --> Interactive help
------------
Docstring:
array(object, dtype=None, *, copy=True, order='K', subok=False, ndmin=0)
Create an array.

Looking for something:
np.lookfor('create a random array')
-----------------------------------
Search results for 'create a random array'
------------------------------------------
numpy.matlib.rand
    Return a matrix of random values with given shape.

np.con*?
-----------
np.concatenate
np.conj
np.conjugate
np.convolve
===================================================================================



2.2.1 Series
=============
Series is a one-dimensional labeled array capable of holding any data type (integers, strings, floating point numbers, Python objects, etc.). The axis labels are collectively referred to as the index. The basic method to create a Series is to call:

s = pd.Series(data, index=index)

Here, data can be many different things: • a Python dict
• an ndarray
• a scalar value (like 5)
The passed index is a list of axis labels. Thus, this separates into a few cases depending on what data is:

From ndarray
If data is an ndarray, index must be the same length as data. If no index is passed, one will be created having values [0, ..., len(data) - 1].

s = pd.Series(np.arange(5), index=['a','b','c','d','e'])
s.index

pd.Series(np.arange(5))

Note: pandas supports non-unique index values. If an operation that does not support duplicate index values is attempted, an exception will be raised at that time. The reason for being lazy is nearly all performance-based (there are many instances in computations, like parts of GroupBy, where the index is not used).

## DUPLICATE INDEX
s = pd.Series(np.arange(5), index=['a','b','a','d','e']) 

From dict
Series can be instantiated from dicts:
s = pd.Series({
    'b':1,
    'a':0,
    'c':2,
})

Note: When the data is a dict, and an index is not passed, the Series index will be ordered by the dict’s insertion order, if you’re using Python version >= 3.6 and pandas version >= 0.23.

pd.Series(s, index=['b','c','d','a'])

Note: NaN (not a number)is the standard missing data marker used in pandas.

From scalar value
If data is a scalar value, an index can be provided. The value will be repeated to match the length of index.
pd.Series(5)
pd.Series(5, index=['b','c','d','a'])

Series is ndarray-like
Series acts very similarly to a ndarray, and is a valid argument to most NumPy functions. However, operations such as slicing will also slice the index.

s[0] is s['a'] ## False
s[0] == s['a'] ## True

print(id(s[0])) # 4949691408
id(s['a']) # 4949692112

s[:3]

s[s>s.median()]

s[[4,3,1]]

np.exp(s)

Like a NumPy array, a pandas Series has a dtype.
s.dtype # dtype('float64')

This is often a NumPy dtype. However, pandas and 3rd-party libraries extend NumPy’s type system in a few places, in which case the dtype would be an ExtensionDtype. Some examples within pandas are Categorical data and Nullable integer data type. 

If you need the actual array backing a Series, use Series.array.
s.array
<PandasArray>
[   0.9662603314256619,  -0.10565599919230215, -0.011807389416360094,
   -0.3584626404849454,   -1.4087244659737785]
Length: 5, dtype: float64

Accessing the array can be useful when you need to do some operation without the index (to disable automatic alignment, for example).
type(s.array) # pandas.core.arrays.numpy_.PandasArray

Series.array will always be an ExtensionArray. Briefly, an ExtensionArray is a thin wrapper around one or more concrete arrays like a numpy.ndarray. pandas knows how to take an ExtensionArray and store it in a Series or a column of a DataFrame.

While Series is ndarray-like, if you need an actual ndarray, then use Series.to_numpy().
s.to_numpy()

Even if the Series is backed by a ExtensionArray, Series.to_numpy() will return a NumPy ndarray.

Series is dict-like
--------------------
A Series is like a fixed-size dict in that you can get and set values by index label:

s['a'] # 0.9662603314256619
'e' in s # True
'f' in s # False

If a label is not contained, an exception is raised:
s['f'] # KeyError: 'f'

Using the get method, a missing label will return None or specified default:
s.get('f')
s.get('f', 10) # 10
s.get('f', np.nan) nan

Vectorized operations and label alignment with Series
------------------------------------------------------
When working with raw NumPy arrays, looping through value-by-value is usually not necessary. The same is true when working with Series in pandas. Series can also be passed into most NumPy methods expecting an ndarray.

s+s
s * 2
np.exp(s)

A key difference between Series and ndarray is that operations between Series automatically align the data based on label. Thus, you can write computations without giving consideration to whether the Series involved have the same labels.

s1 = pd.Series(np.arange(5), index=['a','b','c','d','e'])
print(s1[:-1])
print(s1[1:])
s1[:-1] + s1[1:]
a    0
b    1
c    2
d    3
dtype: int64
b    1
c    2
d    3
e    4
dtype: int64
a    NaN
b    2.0
c    4.0
d    6.0
e    NaN
dtype: float64

a = np.arange(5)
print(a)
a[:-1] + a[1:]

[0 1 2 3 4]
array([1, 3, 5, 7])


The result of an operation between unaligned Series will have the union of the indexes involved. If a label is not found in one Series or the other, the result will be marked as missing NaN. Being able to write code without doing any explicit data alignment grants immense freedom and flexibility in interactive data analysis and research. The integrated data alignment features of the pandas data structures set pandas apart from the majority of related tools for working with labeled data.


Note: In general, we chose to make the default result of operations between differently indexed objects yield the union of the indexes in order to avoid loss of information. Having an index label, though the data is missing, is typically important information as part of a computation. You of course have the option of dropping labels with missing data via the dropna function.

Name attribute
---------------
Series can also have a name attribute:

s1 = pd.Series(np.arange(5), index=['a','b','c','d','e'], name='something')
print(s1)
print(s1.name) # something

The Series name will be assigned automatically in many cases, in particular when taking 1D slices of DataFrame as you will see below.
You can rename a Series with the pandas.Series.rename() method.

s2 = s1.rename('different')
s2.name # 'different'
print(id(s1)) # 4939539792
print(id(s2)) # 4939597952

Note that s1 and s2 refer to different objects.

You can also name an existing series with rename method.
s3 = pd.Series(np.arange(5), index=['a','b','c','d','e'],)
s3.rename('Ramesha') # Name: Ramesha, dtype: int64

2.2.2 DataFrame
----------------
DataFrame is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet or SQL table, or a dict of Series objects. It is generally the most commonly used pandas object. Like Series, DataFrame accepts many different kinds of input:
• Dict of 1D ndarrays, lists, dicts, or Series 
• 2-D numpy.ndarray
• Structured or record ndarray
• A Series
• Another DataFrame

--------------------------------------------------------------------------------
Structured or record ndarray - 

Structured arrays are ndarrays whose datatype is a composition of simpler datatypes organized as a sequence of named fields. For example,

x = np.array([
    ('Rex', 9, 81.0,),
    ('Fido', 3, 27.0,),
], dtype=[('name', 'U10'), ('age','i4'), ('weight', 'f4')]
)
x.dtype

Here x is a one-dimensional array of length two whose datatype is a structure with three fields: 
1. A string of length 10 or less named ‘name’, 
2. a 32-bit integer named ‘age’, and 
3. a 32-bit float named ‘weight’.

If you index x at position 1 you get a structure:
x[1] # ('Fido', 3, 27.)
x[1]['age'] # 3
You can access and modify individual fields of a structured array by indexing with the field name:
x[1]['age'] = 30
x[1] #  ('Fido', 30, 27.)

## Dataframe from structured array
df = DataFrame(x)
df

Structured datatypes are designed to be able to mimic ‘structs’ in the C language, and share a similar memory layout. They are meant for interfacing with C code and for low-level manipulation of structured buffers, for example for interpreting binary blobs. For these purposes they support specialized features such as subarrays, nested datatypes, and unions, and allow control over the memory layout of the structure.

Users looking to manipulate tabular data, such as stored in csv files, may find other pydata projects more suitable, such as xarray, pandas, or DataArray. These provide a high-level interface for tabular data analysis and are better optimized for that use. For instance, the C-struct-like memory layout of structured arrays in numpy can lead to poor cache behavior in comparison
---------------------------------------------------------------------------------

back to dataframe - 
Along with the data, you can optionally pass index (row labels) and columns (column labels) arguments. If you pass an index and / or columns, you are guaranteeing the index and / or columns of the resulting DataFrame. Thus, a dict of Series plus a specific index will discard all data not matching up to the passed index.
If axis labels are not passed, they will be constructed from the input data based on common sense rules.

df = DataFrame({
    'a':[10,20],
    'b':[100, 200]
}, index=['chandu', 'maghu']
)
df

Note: When the data is a dict,and columns is not specified,the DataFrame columns will be ordered by the dict’s insertion order, if you are using Python version >= 3.6 and pandas >= 0.23.

If you want to create a dataframe with a dict but different names:
d = {
    'a':[10,20],
    'b':[100, 200]
}
df = DataFrame(list(d.values()), columns=['Ramesh', 'Suresh'])
df

From dict of Series or dicts
----------------------------
The resulting index will be the union of the indexes of the various Series. If there are any nested dicts, these will first be converted to Series. If no columns are passed, the columns will be the ordered list of dict keys.

d = {
    'one': pd.Series(np.arange(1,4).astype(float), index=['a','b','c']),
    'two': pd.Series(np.arange(1,5).astype(float), index=['a','b','c','d']),
}
df = pd.DataFrame(d)
df

df = pd.DataFrame(d, index=['d','b','a'])

df = pd.DataFrame(d, index=['d','b','a'], columns=['two', 'three'])

The row and column labels can be accessed respectively by accessing the index and columns attributes:

Note: When a particular set of columns is passed along with a dict of data, the passed columns override the keys in the dict.

df.index # Index(['d', 'b', 'a'], dtype='object')
df.columns # Index(['two', 'three'], dtype='object')

From dict of ndarrays / lists
-----------------------------
The ndarrays must all be the same length. If an index is passed, it must clearly also be the same length as the arrays. If no index is passed, the result will be range(n), where n is the array length.

d = {"one": [1.0, 2.0, 3.0, 4.0], "two": [4.0, 3.0, 2.0, 1.0]}
pd.DataFrame(d, index=["a", "b", "c", "d"])

From structured or record array
-------------------------------
This case is handled identically to a dict of arrays.
data = np.zeros((2,), dtype=[("A", "i4"), ("B", "f4"), ("C", "a10")])
data[:] = [(1, 2.0, "Hello"), (2, 3.0, "World")]
pd.DataFrame(data)
pd.DataFrame(data, index=['on', 'tw'])
pd.DataFrame(data, columns=list('cab'.upper()))

Note: DataFrame is not intended to work exactly like a 2-dimensional NumPy ndarray.

From a list of dicts
---------------------
data2 = [{"a": 1, "b": 2}, {"a": 5, "b": 10, "c": 20}]
pd.DataFrame(data2)
pd.DataFrame(data2, index=["first", "second"])
pd.DataFrame(data2, columns=["a", "b"])

From a dict of tuples
---------------------
You can automatically create a MultiIndexed frame by passing a tuples dictionary.
d = {
    ("a", "b"): {("A", "B"): 1, ("A", "C"): 2},
    ("a", "a"): {("A", "C"): 3, ("A", "B"): 4},
    ("a", "c"): {("A", "B"): 5, ("A", "C"): 6},
    ("b", "a"): {("A", "C"): 7, ("A", "B"): 8},
    ("b", "b"): {("A", "D"): 9, ("A", "B"): 10},
}
DataFrame(d)

From a Series
-------------
The result will be a DataFrame with the same index as the input Series, and with one column whose name is the original name of the Series (only if no other column name provided).

From a list of namedtuples
---------------------------
The field names of the first namedtuple in the list determine the columns of the DataFrame. The remaining namedtuples (or tuples) are simply unpacked and their values are fed into the rows of the DataFrame. If any of those tuples is shorter than the first namedtuple then the later columns in the corresponding row are marked as missing values. If any are longer than the first namedtuple, a ValueError is raised.

Point = namedtuple('Point', 'x y')
DataFrame([Point(0,0), Point(0,1), Point(0,2)])

Point = namedtuple('Point3D', 'x y z')
DataFrame([Point(0,0,0), Point(0,1,1), Point(0,1,2)])

From a list of dataclasses
--------------------------
Passing a list of dataclasses is equivalent to passing a list of dictionaries.
Please be aware, that all values in the list should be dataclasses, mixing types in the list would result in a TypeError.

from dataclasses import make_dataclass
Point = make_dataclass('Point', [('x', int), ('y', int)])
pd.DataFrame([Point(0, 0), Point(0,1), Point(0,2)])

Missing data
Much more will be said on this topic in the Missing data section. To construct a DataFrame with missing data, we use np.nan to represent missing values. Alternatively, you may pass a numpy.MaskedArray as the data argument to the DataFrame constructor, and its masked entries will be considered missing.

Alternate constructors
-----------------------
DataFrame.from_dict --
----------------------
DataFrame.from_dict takes a dict of dicts or a dict of array-like sequences and returns a DataFrame. It operates like the DataFrame constructor except for the orient parameter which is 'columns' by default, but which can be set to 'index' in order to use the dict keys as row labels
pd.DataFrame.from_dict(dict([('A', np.arange(1,4)), ('B', np.arange(4,7))]))

If you pass orient='index', the keys will be the row labels. In this case, you can also pass the desired column names:
pd.DataFrame.from_dict(
    dict([('A', np.arange(1,4)), ('B', np.arange(4,7))]),
    orient = 'index',
    columns=['one', 'two', 'three'])

DataFrame.from_records
------------------------
DataFrame.from_records takes a list of tuples or an ndarray with structured dtype. It works analogously to the normal DataFrame constructor, except that the resulting DataFrame index may be a specific field of the structured dtype. For example:
x = np.array([
    ('Rex', 9, 81.0,),
    ('Fido', 3, 27.0,),
], dtype=[('name', 'U10'), ('age','i4'), ('weight', 'f4')]
)

pd.DataFrame.from_records(x)

Column selection, addition, deletion
-------------------------------------
You can treat a DataFrame semantically like a dict of like-indexed Series objects. Getting, setting, and deleting columns works with the same syntax as the analogous dict operations:

d = {
    'one': pd.Series(np.arange(1,4).astype(float), index=['a','b','c']),
    'two': pd.Series(np.arange(1,5).astype(float), index=['a','b','c','d']),
}
df = pd.DataFrame(d)
df['one']   ## Selection
df['three'] = df['one'] * df['two'] ## Column Addition
df['flag'] = df['one'] > 2  ## Column Addition
df

Columns can be deleted or popped like with a dict:
del df['two']   ## permanently deleted
three = df.pop('three')  ## Poped and deleated

When inserting a scalar value, it will naturally be propagated to fill the column:
df['foo'] = 'bar'

When inserting a Series that does not have the same index as the DataFrame, it will be conformed to the DataFrame’s index:
df['one_trunc'] = df["one"][:2]

If the index of the series to be added does not match with df, then np.nan is filled.
s = pd.Series(np.arange(1,4), index=['z','y','x'])
df1 = df
print(df1)
df1['trunc_one'] = s
df1

one	flag	    foo	trunc_one
a	1.0	False	bar	NaN
b	2.0	False	bar	NaN
c	3.0	True	bar	NaN
d	NaN	False	bar	NaN

You can insert raw ndarrays but their length must match the length of the DataFrame’s index.
By default, columns get inserted at the end. The insert function is available to insert at a particular location in the columns:
df.insert(1, 'bar', df["one"])

Assigning new columns in method chains
--------------------------------------
assign - DataFrame has an assign() method that allows you to easily create new columns that are potentially derived from existing columns.
assign always returns a copy of the data, leaving the original DataFrame untouched.
iris = pd.read_csv("data/iris.data")
iris.head()
iris.assign(SepalRation = iris.SepalWidth / iris.SepalLength).head()

In the example above, we inserted a precomputed value. We can also pass in a function of one argument to be evaluated on the DataFrame being assigned to.
iris.assign(SepalRatio=lambda x:(x['SepalWidth']/x['SepalLength'])).head()

Passing a callable, as opposed to an actual value to be inserted, is useful when you don’t have a reference to the DataFrame at hand. This is common when using assign in a chain of operations. For example, we can limit the DataFrame to just those observations with a Sepal Length greater than 5, calculate the ratio, and plot:

iris.query("SepalLength > 5").assign(
    SepalRatio = iris.SepalWidth / iris.SepalLength,
    PetalRatio = lambda x: x.PetalWidth / x.PetalLength
).plot(kind="scatter", x="SepalRatio", y="PetalRatio")

Since a function is passed in, the function is computed on the DataFrame being assigned to. Importantly, this is the DataFrame that’s been filtered to those rows with sepal length greater than 5. The filtering happens first, and then the ratio calculations. This is an example where we didn’t have a reference to the filtered DataFrame available.

The function signature for assign is simply **kwargs. The keys are the column names for the new fields, and the values are either a value to be inserted (for example, a Series or NumPy array), or a function of one argument to be called on the DataFrame. A copy of the original DataFrame is returned, with the new values inserted.
Starting with Python 3.6 the order of **kwargs is preserved. This allows for dependent assignment, where an expression later in **kwargs can refer to a column created earlier in the same assign().

dfa = pd.DataFrame({"A": [1, 2, 3], "B": [4, 5, 6]})
dfa.assign(C=lambda x:x.A+x.B, D=lambda x:x.A+x.C)

In the second expression, x['C'] will refer to the newly created column,that’s equal to dfa['A'] + dfa['B'].

Indexing / selection
---------------------
The basics of indexing are as follows:
Operation                           Syntax                      Result
Select column                       df[col]                     Series
Select row by label                 df.loc[label]               Series
Select row by integer location      df.iloc[loc]                Series
Slice rows                          df[5:10]                    DataFrame
Select rows by boolean vector       df[bool_vec]                DataFrame

Row selection, for example, returns a Series whose index is the columns of the DataFrame:
df.loc["b"]
df.iloc[2]

Data alignment and arithmetic
------------------------------

Data alignment between DataFrame objects automatically align on BOTH THE COLUMNS AND THE INDEX (ROW LABELS). Again, the resulting object will have the union of the column and row labels.
df1 = pd.DataFrame(np.random.randn(10, 4), columns=['A', 'B', 'C', 'D'])
df2 = pd.DataFrame(np.random.randn(7,3), columns=['A', 'B', 'C'])
df1 + df2

When doing an operation between DataFrame and Series, the default behavior is to align the Series index on the DataFrame columns, thus broadcasting row-wise. For example:
df1 - df1.iloc[0]

Operations with scalars are just as you would expect:
df = pd.DataFrame(np.arange(20).reshape(5,4), columns=['A', 'B', 'C', 'D'])
df*5+2
1/df
df**4

Boolean operators work as well:
df1 = pd.DataFrame({'a':[1,0,1], 'b':[0,1,1]}, dtype=bool)
df2 = pd.DataFrame({'a':[0,1,1], 'b':[1,1,0]}, dtype=bool)
df1 & df2
df1 | df2
df1 ^ df2
-df1

Transposing
To transpose, access the T attribute (also the transpose function), similar to an ndarray:
df[:3].T

DataFrame interoperability with NumPy functions
------------------------------------------------
Elementwise NumPy ufuncs (log, exp, sqrt, . . . ) and various other NumPy functions can be used with no issues on Series and DataFrame, assuming the data within are numeric:
np.exp(df)
np.asanyarray(df)

DataFrame is not intended to be a drop-in replacement for ndarray as its indexing semantics and data model are quite different in places from an n-dimensional array.
Series implements __array_ufunc__, which allows it to work with NumPy’s universal functions. The ufunc is applied to the underlying array in a Series.
s = pd.Series(np.arange(1,5).astype(int))
np.exp(s)

Changed in version 0.25.0: When multiple Series are passed to a ufunc, they are aligned before performing the operation.
Like other parts of the library, pandas will automatically align labeled inputs as part of a ufunc with multiple inputs. For example, using numpy.remainder() on two Series with differently ordered labels will align before the operation.
s1 = pd.Series(np.arange(1,4), index=['a', "b", 'c'])
s2 = pd.Series([1,3,5], index=['b', 'a', 'c'])
s1+s2
np.remainder(s1,s2)

As usual, the union of the two indices is taken, and non-overlapping values are filled with missing values.
s1 = pd.Series(np.arange(1,4), index=['a', "b", 'c'])
s2 = pd.Series([1,3,5], index=['b', 'a', 'c'])
s3 = pd.Series(np.arange(2,7,2), index=['b', 'c', 'd'])
s1+s2
np.remainder(s1, s2)
np.remainder(s1, s3)

When a binary ufunc is applied to a Series and Index, the Series implementation takes precedence and a Series is returned.
ser = pd.Series([1,2,3])
idx = pd.Index([4,5,6])
type(np.maximum(ser, idx)) # pandas.core.series.Series

NumPy ufuncs are safe to apply to Series backed by non-ndarray arrays, for example arrays.SparseArray. If possible, the ufunc is applied without converting the underlying data to an ndarray.

Console display
---------------
Very large DataFrames will be truncated to display them in the console. You can also get a summary using info().
However, using to_string will return a string representation of the DataFrame in tabular form, though it won’t always fit the console width:
baseball = pd.read_csv("data/baseball.csv")
baseball
baseball.info()
print(baseball.iloc[-20:,:12].to_string())

Wide DataFrames will be printed across multiple rows by default:
pd.DataFrame(np.random.randn(3,12))

You can change how much to print on a single row by setting the display.width option:
pd.set_option('display.width', 40)
pd.DataFrame(np.random.randn(3, 12))

You can adjust the max width of the individual columns by setting display.max_colwidth
pd.set_option("display.max_colwidth", 30)

You can also disable this feature via the expand_frame_repr option. This will print the table in one block.

DataFrame column attribute access and IPython completion
---------------------------------------------------------
If a DataFrame column label is a valid Python variable name, the column can be accessed like an attribute:
df = pd.DataFrame({"foo1": np.random.randn(5), "foo2": np.random.randn(5)})
df.foo1
The columns are also connected to the IPython completion mechanism so they can be tab-completed:
df.fo<TAB>

2.3 Essential basic functionality
==================================
Here we discuss a lot of the essential functionality common to the pandas data structures. To begin, let’s create some example objects like we did in the 10 minutes to pandas section:
s = pd.Series(np.random.randn(5), index=['a','b','c','d','e'])
index = pd.date_range('1.1.2000', periods=8)
df = pd.DataFrame(np.random.randn(8,3), index=index, columns=['A', 'B', 'C'])

2.3.1 Head and tail
-------------------
To view a small sample of a Series or DataFrame object, use the head() and tail() methods. The default number of elements to display is five, but you may pass a custom number.
long_series = pd.Series(np.random.randn(1000))
long_series.head()
long_series.tail(3)

2.3.2 Attributes and underlying data
-------------------------------------
pandas objects have a number of attributes enabling you to access the metadata • shape: gives the axis dimensions of the object, consistent with ndarray
• Axis labels
– Series: index (only axis)
– DataFrame: index (rows) and columns Note, these attributes can be safely assigned to!
df[:2]
df.columns = [col.lower() for col in df.columns]

pandas objects (Index, Series, DataFrame) can be thought of as containers for arrays, which hold the actual data and do the actual computation. For many types, the underlying array is a numpy.ndarray. However, pandas and 3rd party libraries may extend NumPy’s type system to add support for custom arrays (see dtypes).
To get the actual data inside a Index or Series, use the .array property
s.array
df.index.array

array will always be an ExtensionArray. The exact details of what an ExtensionArray is and why pandas uses them are a bit beyond the scope of this introduction. We will see it in future chapter.
s.to_numpy()
np.asarray(s)

--------------Extension Array-----------------

3.15.6 pandas.api.extensions.ExtensionArray
class pandas.api.extensions.ExtensionArray Abstract base class for custom 1-D array types.
pandas will recognize instances of this class as proper arrays with a custom type and will not attempt to coerce them to objects. They may be stored directly inside a DataFrame or Series.

----------------------------------------------

When the Series or Index is backed by an ExtensionArray, to_numpy() may involve copying data and coercing values. See dtypes for more.
to_numpy() gives some control over the dtype of the resulting numpy.ndarray. For example, consider date- times with timezones. NumPy doesn’t have a dtype to represent timezone-aware datetimes, so there are two possibly useful representations:
1. An object-dtype numpy.ndarray with Timestamp objects, each with the correct tz
2. A datetime64[ns] -dtype numpy.ndarray, where the values have been converted to UTC and the time-
zone discarded
Timezones may be preserved with dtype=object
ser = pd.Series(pd.date_range('2000', periods=2, tz='CET'))
ser # dtype: datetime64[ns, CET]
ser.to_numpy() # dtype=object
ser.to_numpy(dtype=object) # array([Timestamp('2000-01-01 00:00:00+0100', tz='CET', freq='D'),
                           # Timestamp('2000-01-02 00:00:00+0100', tz='CET', freq='D')],
                           # dtype=object)

Or Timezones can be thrown away with dtype='datetime64[ns]'
ser.to_numpy(dtype='datetime64[ns]') # array(['1999-12-31T23:00:00.000000000', '2000-01-01T23:00:00.000000000'],
                                      #dtype='datetime64[ns]')

Getting the “raw data” inside a DataFrame is possibly a bit more complex. When your DataFrame only has a single data type for all the columns, DataFrame.to_numpy() will return the underlying data:
df = pd.DataFrame(np.random.randn(8,3), index=index, columns=['A', 'B', 'C'])
df.columns = [col.lower() for col in df.columns]
df.to_numpy()

If a DataFrame contains homogeneously-typed data, the ndarray can actually be modified in-place, and the changes will be reflected in the data structure. For heterogeneous data (e.g. some of the DataFrame’s columns are not all the same dtype), this will not be the case. The values attribute itself, unlike the axis labels, cannot be assigned to.
Note: When working with heterogeneous data,the dtype of the resulting ndarray will be chosen to accommodate all of the data involved. For example, if strings are involved, the result will be of object dtype. If there are only floats and integers, the resulting array will be of float dtype.

In the past, pandas recommended Series.values or DataFrame.values for extracting the data from a Series or DataFrame. You’ll still find references to these in old code bases and online. Going forward, we recommend avoiding .values and using .array or .to_numpy(). .values has the following drawbacks:
  1. When your Series contains an extension type, it’s unclear whether Series.values returns a NumPy array or the extension array. Series.array will always return an ExtensionArray, and will never copy data. Series.to_numpy() will always return a NumPy array, potentially at the cost of copying / coercing values.
  2. When your DataFrame contains a mixture of data types, DataFrame.values may involve copying data and coercing values to a common dtype, a relatively expensive operation. DataFrame.to_numpy(), being a method, makes it clearer that the returned NumPy array may not be a view on the same data in the DataFrame.
--------------- Extension Types --------------------------

4.7.2 Extension types
pandas defines an interface for implementing data types and arrays that extend NumPy’s type system. pandas itself uses the extension system for some types that aren’t built into NumPy (categorical, period, interval, datetime with timezone).
Libraries can define a custom array and data type. When pandas encounters these objects, they will be handled properly (i.e. not converted to an ndarray of objects). Many methods like pandas.isna() will dispatch to the extension type’s implementation.
If you’re building a library that implements the interface, please publicize it on ecosystem.extensions. The interface consists of two classes.

ExtensionDtype
A pandas.api.extensions.ExtensionDtype is similar to a numpy.dtype object. It describes the data type. Implementors are responsible for a few unique items like the name.
One particularly important item is the type property. This should be the class that is the scalar type for your data. For example, if you were writing an extension array for IP Address data, this might be ipaddress.IPv4Address.
See the extension dtype source for interface definition.
New in version 0.24.0.
pandas.api.extension.ExtensionDtype can be registered to pandas to allow creation via a string dtype name. This allows one to instantiate Series and .astype() with a registered string name, for example 'category' is a registered string accessor for the CategoricalDtype.
See the extension dtype dtypes for more on how to register dtypes. ExtensionArray
This class provides all the array-like functionality. ExtensionArrays are limited to 1 dimension. An ExtensionArray is linked to an ExtensionDtype via the dtype attribute.
pandas makes no restrictions on how an extension array is created via its __new__ or __init__, and puts no restrictions on how you store your data. We do require that your array be convertible to a NumPy array, even if this is relatively expensive (as it is for Categorical).
They may be backed by none, one, or many NumPy arrays. For example, pandas.Categorical is an extension array backed by two arrays, one for codes and one for categories. An array of IPv6 addresses may be backed by a NumPy structured array with two fields, one for the lower 64 bits and one for the upper 64 bits. Or they may be backed by some other storage type, like Python lists.

----------------------------------------------------------

2.3.3 Accelerated operations
-----------------------------
pandas has support for accelerating certain types of binary numerical and boolean operations using the numexpr library and the bottleneck libraries.
These libraries are especially useful when dealing with large data sets, and provide large speedups. numexpr uses smart chunking, caching, and multiple cores. bottleneck is a set of specialized cython routines that are especially fast when dealing with arrays that have nans.
You are highly encouraged to install both libraries.
These are both enabled to be used by default, you can control this by setting the options:
 pd.set_option("compute.use_bottleneck", False)
 pd.set_option("compute.use_numexpr", False)

2.3.4 Flexible binary operations
---------------------------------

With binary operations between pandas data structures, there are two key points of interest:
• Broadcasting behavior between higher- (e.g. DataFrame) and lower-dimensional (e.g. Series) objects.
• Missing data in computations.
We will demonstrate how to manage these issues independently, though they can be handled simultaneously.

Matching / broadcasting behavior
--------------------------------
DataFrame has the methods add(), sub(), mul(), div() and related functions radd(), rsub(), ... for carrying out binary operations. For broadcasting behavior, Series input is of primary interest. Using these functions, you can use to either match on the index or columns via the axis keyword:
df = pd.DataFrame({
    'one':pd.Series(np.arange(1,4), index=['a','b','c',]),
    'two':pd.Series(np.arange(1,5), index=['a','b','c','d',]),
    'three':pd.Series(np.arange(1,4), index=['b','c','d']),
})
print(df)
row = df.iloc[1] # 2, 2, 1
column = df['two'] # 1, 2, 3, 4
print(df.sub(row, axis='columns'))
print(df.sub(row, axis=1))

df.sub(column, axis='index')
df.sub(column, axis=0)

Furthermore you can align a level of a MultiIndexed DataFrame with a Series.
dfmi = df.copy()
dfmi.index = pd.MultiIndex.from_tuples(
    [(1,'a'), (1,'b'), (1,'c'), (2, 'a')],
    names=['first', 'second']
    )
dfmi
column = df['two'] # 1, 2, 3, 4
dfmi.sub(column, axis=0, level=1)
dfmi.sub(column, axis=0, level="second")

Series and Index also support the divmod() builtin. This function takes the floor division and modulo operation at the same time returning a two-tuple of the same type as the left hand side. For example:
s = pd.Series(np.arange(0,31,10))
quo, rem = dm = divmod(s, 3)
print(dm)
print('*'*8)
print(rem)
print('*'*8)
print(quo)

idx = pd.Index(np.arange(0,31,10))
quo, rem = divmod(idx, 3)
quo
rem

We can also do elementwise divmod():
quo, rem = divmod(s, [1,2,3,5])

Missing data / operations with fill values
-------------------------------------------
In Series and DataFrame, the arithmetic functions have the option of inputting a fill_value, namely a value to substitute when at most one of the values at a location are missing. For example, when adding two DataFrame objects, you may wish to treat NaN as 0 unless both DataFrames are missing that value, in which case the result will be NaN (you can later replace NaN with some other value using fillna if you wish).
df1 = DataFrame({
    'a':[1,2,3,],
    'b':[11,22,33]
})

df2 = DataFrame({
    'a':[1,2,3,],
    'b':[11,22,33],
    'c':[10, 20, 30]
})
print(df1 + df2)
df1.add(df2, fill_value=0)
df1.add(df2, fill_value=1)
   a   b   c
0  2  22 NaN
1  4  44 NaN
2  6  66 NaN
a	b	c
0	2	22	10.0
1	4	44	20.0
2	6	66	30.0
	a	b	c
0	2	22	11.0
1	4	44	21.0
2	6	66	31.0

Flexible comparisons
---------------------
Series and DataFrame have the binary comparison methods eq, ne, lt, gt, le, and ge whose behavior is analogous to the binary arithmetic operations described above:
df1.gt(df2)
df1.ne(df2)

These operations produce a pandas object of the same type as the left-hand-side input that is of dtype bool. These boolean objects can be used in indexing operations.

Boolean reductions
-------------------
You can apply the reductions: empty, any(), all(), and bool() to provide a way to summarize a boolean result.
df1.gt(2).all(axis=1)
df1.gt(2).any(axis=1)

You can reduce to a final boolean value.
df1.gt(2).any(axis=1).any() # True

You can test if a pandas object is empty, via the empty property.
df1.empty # False
pd.DataFrame(columns=list('ABC')).empty # True

To evaluate single-element pandas objects in a boolean context, use the method bool():
pd.Series([True]).bool() # True
pd.DataFrame([False]).bool() # False

Warning: You might be tempted to do the following:
if df:
    pass

df1 and df2

These will both raise errors, as you are trying to compare multiple values.:
ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().

Comparing if objects are equivalent
------------------------------------
Often you may find that there is more than one way to compute the same result. As a simple example, consider df + df and df * 2. To test that these two computations produce the same result, given the tools shown above, you might imagine using(df + df == df * 2).all().But in fact,this expression is False:
( (df1 + df1) == (df1 * 2) ).all()

To get a single boolean value use
( (df1 + df1) == (df1 * 2) ).all().all()

Notice that the boolean DataFrame df + df == df * 2 contains some False values! This is because NaNs do not compare as equals:
np.nan == np.nan # False
So, NDFrames (such as Series and DataFrames) have an equals() method for testing equality, with NaNs in corre- sponding locations treated as equal.
pd.Series(np.nan).equals(pd.Series(np.nan)) # True
(df1*2).equals(df1 + df1) # True

Note that the Series or DataFrame index needs to be in the same order for equality to be True:
df1 = DataFrame({'a':['foo', 0, np.nan]})
df2 = DataFrame({'a':[0, 'foo', np.nan]}, index=[1,0,2])

df1.equals(df2) # False
df1.equals(df2.sort_index()) # True

Comparing array-like objects
-----------------------------
You can conveniently perform element-wise comparisons when comparing a pandas data structure with a scalar value:
pd.Series(["foo", "bar", "baz"]) == "foo"
pd.Index(["foo", "bar", "baz"]) == "foo"

pandas also handles element-wise comparisons between different array-like objects of the same length:
pd.Series(["foo", "bar", "baz"]) == pd.Index(["foo", "bar", "qux"])
pd.Series(["foo", "bar", "baz"]) == np.array(["foo", "bar", "qux"]) ## True True False
pd.Series(["foo", "bar", "baz"]).equals( np.array(["foo", "bar", "qux"]) ) # False

Trying to compare Index or Series objects of different lengths will raise a ValueError:
pd.Series(['foo', 'bar', 'baz']) == pd.Series(['foo', 'bar']) ValueError: Series lengths must match to compare
pd.Series(['foo', 'bar', 'baz']) == pd.Series(['foo']) ValueError: Series lengths must match to compare

Note that this is different from the NumPy behavior where a comparison can be broadcast:
np.array([1, 2, 3]) == np.array([2]) #  array([False, True, False])
or it can return False if broadcasting can not be done:
np.array([1, 2, 3]) == np.array([1, 2]) # False

Combining overlapping data sets
--------------------------------
A problem occasionally arising is the combination of two similar data sets where values in one are preferred over the other. An example would be two data series representing a particular economic indicator where one is considered to be of “higher quality”. However, the lower quality series might extend further back in history or have more complete data coverage. As such, we would like to combine two DataFrame objects where missing values in one DataFrame are conditionally filled with like-labeled values from the other DataFrame. The function implementing this operation is combine_first(), which we illustrate:
df1 = pd.DataFrame({
    'A':[ 1, np.nan, 3, 5, np.nan, ],
    'B':[ np.nan, 2, 3, np.nan, 6, ],
})

df2 = pd.DataFrame({
    "A": [5.0, 2.0, 4.0, np.nan, 3.0, 7.0],
    "B": [np.nan, np.nan, 3.0, 4.0, 6.0, 8.0],
})

df1.combine_first(df2)

General DataFrame combine
--------------------------
The combine_first() method above calls the more general DataFrame.combine(). This method takes another DataFrame and a combiner function, aligns the input DataFrame and then passes the combiner function pairs of Series (i.e., columns whose names are the same).
So, for instance, to reproduce combine_first() as above:
def combiner(x,y):
    return np.where(pd.isna(x),y,x)

df1.combine(df2, combiner)

2.3.5 Descriptive statistics
-----------------------------
There exists a large number of methods for computing descriptive statistics and other related operations on Series, DataFrame. Most of these are aggregations (hence producing a lower-dimensional result) like sum(), mean(), and quantile(), but some of them, like cumsum() and cumprod(), produce an object of the same size. Generally speaking, these methods take an axis argument, just like ndarray.{sum, std, . . . }, but the axis can be specified by name or integer:

• Series: no axis argument needed
• DataFrame: “index” (axis=0, default), “columns” (axis=1)

df.mean(0)
df.mean(1)

All such methods have a skipna option signaling whether to exclude missing data (True by default):
df.sum(0, skipna=False)
df.sum(0, skipna=True)

Combined with the broadcasting / arithmetic behavior, one can describe various statistical procedures, like standard- ization (rendering data zero mean and standard deviation of 1), very concisely:
standardization formula z = x - µ / std

ts_stand = (df - df.mean()) / df.std()
ts_stand.std()
ts_stand.mean()

xs_stand = df.sub(df.mean(1), axis=0).div(df.std(1), axis=0)
xs_stand.std(1)
xs_stand.mean(1)

Note that methods like cumsum() and cumprod() preserve the location of NaN values. This is somewhat different from expanding() and rolling() since NaN behavior is furthermore dictated by a min_periods parameter.
df.cumsum()

Here is a quick reference summary table of common functions. Each also takes an optional level parameter which applies only if the object has a hierarchical index.
Function             Description
count               Number of non-NA observations
sum                 Sum of values
mean                Mean of values
mad                 Mean absolute deviation
median              Arithmetic median of values
min                 Minimum
max                 Maximum
mode                Mode
abs                 Absolute Value
prod                Product of values
std                 Bessel-corrected sample standard deviation
var                 Unbiased variance
sem                 Standard error of the mean
skew                Sample skewness (3rd moment)
kurt                Sample kurtosis (4th moment)
quantile            Sample quantile (value at %)
cumsum              Cumulative sum
cumprod             Cumulative product
cummax              Cumulative maximum
cummin              Cumulative minimum

Note that by chance some NumPy methods, like mean, std, and sum, will exclude NAs on Series input by default:
np.mean(df.one) # 2.0
np.mean(df.one.to_numpy()) # nan

Series.nunique() will return the number of unique non-NA values in a Series:
series = pd.Series(np.arange(1,11))
series[6:] = np.nan
series[3:6] = 5
series.nunique()

Summarizing data: describe
----------------------------
There is a convenient describe() function which computes a variety of summary statistics about a Series or the columns of a DataFrame (excluding NAs of course):
series = pd.Series(np.random.randn(1000))
series[::2] = np.nan
series.describe()

frame = pd.DataFrame(np.random.randn(1000, 5), columns=["a", "b", "c", "d","e"])
frame.iloc[::2] = np.nan
frame.describe()

You can select specific percentiles to include in the output:
series.describe(percentiles=[0.05, 0.25, 0.75, 0.95])

By default, the median is always included.
For a non-numerical Series object, describe() will give a simple summary of the number of unique values and most frequently occurring values:
s = pd.Series(["a", "a", "b", "b", "a", "a", np.nan, "c", "d", "a"])
s.describe()

Note that on a mixed-type DataFrame object, describe() will restrict the summary to include only numerical columns or, if none are, only categorical columns:
frame = pd.DataFrame({"a": ["Yes", "Yes", "No", "No"], "b": range(4)})
frame.describe()

This behavior can be controlled by providing a list of types as include/exclude arguments. The special value all can also be used:
frame.describe(include=['object'])
frame.describe(include=['number'])
frame.describe(include='all')

That feature relies on select_dtypes.

Index of min/max values
------------------------
The idxmin() and idxmax() functions on Series and DataFrame compute the index labels with the minimum and maximum corresponding values:
s = pd.Series(np.random.choice(10, 5, replace=False))
0    4
1    6
2    7
3    8
4    0
s.idxmin(), s.idxmax() # (4, 3)

df1 = pd.DataFrame(
    np.random.choice(20, 15, replace=False).reshape(5,3),
    columns=['A', 'B', 'C']
)
df1
df1.idxmax(), df1.idxmin()
df1.idxmax(axis=1), df1.idxmin(axis=1)

When there are multiple rows (or columns) matching the minimum or maximum value, idxmin() and idxmax() return the first matching index:
df1 = pd.DataFrame(
    np.random.choice(20, 15).reshape(5,3),
    columns=['A', 'B', 'C']
)
df1

    A	B	C
0	4	5	13
1	19	12	16
2	1	4	13
3	19	19	8
4	13	6	15
df1['A'].idxmax(), df1.idxmin()
df1.idxmax().idxmax() - To get the index of the max value in a df

Note: idxmin and idxmax are called argmin and argmax in NumPy.

Value counts (histogramming) / mode
-------------------------------------
The value_counts() Series method and top-level function computes a histogram of a 1D array of values. It can also be used as a function on regular arrays:
data = np.random.randint(0,4, size=50)
s = pd.Series(data)
s.value_counts()
pd.value_counts(s)

The value_counts() method can be used to count combinations across multiple columns. By default all columns
are used but a subset can be selected using the subset argument.
data = {"a": [1, 2, 3, 4,1], "b": ["x", "x", "y", "y",'x']}
frame = pd.DataFrame(data)
frame.value_counts()
a  b
1  x    2
2  x    1

To find if a row is duplicated in a dataframe then we can go with duplicated method.
frame[frame.duplicated()]

So, if we just want to rows which is duplicated then we can go with duplicated method, if we want to counts also then we can use value_counts().
frame.value_counts()[frame.value_counts() > 1]

Similarly, you can get the most frequently occurring value(s), i.e. the mode, of the values in a Series or DataFrame:
s5 = pd.Series([1, 1, 3, 3, 3, 3, 5, 5, 7, 7, 7,])
s5.mode() #     0    3

df = pd.DataFrame({
    'A': np.random.randint(0,7, size=5),
    'B': np.random.randint(-10, 15, size=5)
})
df.mode()

Discretization and quantiling
-------------------------------
Continuous values can be discretized using the cut() (bins based on values) and qcut() (bins based on sample quantiles) functions:
arr = np.arange(1,9).astype(float)
factor = pd.cut(arr, [-5, -1, 0, 1, 5])
factor

qcut() computes sample quantiles. For example, we could slice up some normally distributed data into equal-size quartiles like so:
arr = np.arange(1,9).astype(float)
factor = pd.qcut(arr, [0, .25, .5, .75, 1])
factor
pd.value_counts(factor)

We can also pass infinite values to define the bins:
arr = np.arange(1,9).astype(float)
factor = pd.cut(arr, [-np.inf, 0, np.inf])
factor

2.3.6 Function application
==========================
To apply your own or another library’s functions to pandas objects, you should be aware of the three methods below. The appropriate method to use depends on whether your function expects to operate on an entire DataFrame or Series, row- or column-wise, or elementwise.
1. Tablewise Function Application: pipe()
2. Row or Column-wise Function Application: apply()
3. Aggregation API: agg() and transform()
4. Applying Elementwise Functions: applymap()

Tablewise function application
-------------------------------
DataFrames and Series can be passed into functions. However, if the function needs to be called in a chain, consider using the pipe() method.

df_p = pd.DataFrame({'city_and_code': ['Chicago, IL']})
df_p

def extract_city_name(df):
    """
        Chicago, IL -> Chicago for city_name column
    """
    df['city_name'] = df['city_and_code'].str.split(",").str.get(0)
    return df

def add_country_name(df, country_name=None):
    """
        Chicago -> Chicago-US for city_name column
    """
    col = 'city_name'
    df['city_and_country'] = df[col] + country_name
    return df

extract_city_name and add_country_name are functions taking and returning DataFrames.

Now compare the following:
add_country_name(extract_city_name(df_p), country_name='US')

Is equivalent to:
df_p.pipe(extract_city_name).pipe(add_country_name, country_name='US')

pandas encourages the second style, which is known as method chaining. pipe makes it easy to use your own or another library’s functions in method chains, alongside pandas’ methods.

In the example above, the functions extract_city_name and add_country_name each expected a DataFrame as the first positional argument. What if the function you wish to apply takes its data as, say, the second argument? In this case, provide pipe with a tuple of (callable, data_keyword). .pipe will route the DataFrame to the argument specified in the tuple.

For example, we can fit a regression using statsmodels. Their API expects a formula first and a DataFrame as the second argument,data.We pass in the function, keyword pair(sm.ols, 'data')to pipe:
bb.query("h>0").assign(ln_h=lambda df:np.log(df.h)).pipe(
    (sm.ols, 'data'), "hr ~ ln_h + year + g + C(lg)").fit().summary()

The pipe method is inspired by unix pipes and more recently dplyr and magrittr, which have introduced the popular (%>%) (read pipe) operator for R. The implementation of pipe here is quite clean and feels right at home in Python. We encourage you to view the source code of pipe().

Row or column-wise function application
----------------------------------------
Arbitrary functions can be applied along the axes of a DataFrame using the apply() method, which, like the descriptive statistics methods, takes an optional axis argument:

df.apply(np.mean)
df.apply(np.mean)
df.apply(np.mean, axis=1)
df.apply(lambda x:x.max() - x.min())
df.apply(np.cumsum)
df.apply(np.exp)

The apply() method will also dispatch on a string method name.
df.apply("mean")
df.apply("mean", axis=1)

The return type of the function passed to apply() affects the type of the final output from DataFrame.apply for the default behaviour:
• If the applied function returns a Series, the final output is a DataFrame. The columns match the index of the Series returned by the applied function.
• If the applied function returns any other type, the final output is a Series.

This default behaviour can be overridden using the result_type, which accepts three options: reduce,
broadcast, and expand. These will determine how list-likes return values expand (or not) to a DataFrame.

apply() combined with some cleverness can be used to answer many questions about a data set. For example,
suppose we wanted to extract the date where the maximum value for each column occurred:

tsdf.apply("idxmax")

You may also pass additional arguments and keyword arguments to the apply() method. For instance, consider the following function you would like to apply:
def sub_and_div(x, sub, div=1):
    return (x - sub)/div
df.apply(sub_and_div, args=(3,),div=2)

Another useful feature is the ability to pass Series methods to carry out some Series operation on each column or row:
df.apply(pd.Series.interpolate)
-----------------------------------------------------------------------
pd.Series.interpolate - Fill NaN values using an interpolation method.
-----------------------------------------------------------------------
Finally, apply() takes an argument raw which is False by default, which converts each row or column into a Series before applying the function. When set to True, the passed function will instead receive an ndarray object, which has positive performance implications if you do not need the indexing functionality.
df.apply(np.mean, raw=True)

Which one is faster?
%timeit df.apply(np.mean, raw=True) ## 312 µs ± 28.5 µs per loop
%timeit df.apply('mean') ## 727 µs ± 4.89 µs per loop

np is way faster, another example
%timeit df.apply(np.median, raw=True) # 423 µs ± 35.3 µs per loop, (mean ± std. dev. of 7 runs, 1000 loops each)
%timeit df.apply('median') ## 1.6 ms ± 49.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

Aggregation API
----------------
The aggregation API allows one to express possibly multiple aggregation operations in a single concise way. This API is similar across pandas objects, see groupby API, the window API, and the resample API. The entry point for aggregation is DataFrame.aggregate(), or the alias DataFrame.agg().
Using a single function is equivalent to apply(). You can also pass named methods as strings. These will return a Series of the aggregated output:
We will use a similar starting frame from above:
tsdf = pd.DataFrame(np.random.choice(20, 15, replace=False).reshape(5,3),
                    columns=['A', 'B', 'C'],
                   index = pd.date_range('2020.01.01', periods=5))
tsdf.iloc[2:4] = np.nan
tsdf.agg(np.sum)
tsdf.agg('sum')
# these are equivalent to a ``.sum()`` because we are aggregating # on a single function
tsdf.sum()

Single aggregations on a Series this will return a scalar value:
tsdf["A"].agg("sum") # 31.0

Aggregating with multiple functions
-----------------------------------
You can pass multiple aggregation arguments as a list. The results of each of the passed functions will be a row in the resulting DataFrame. These are naturally named from the aggregation function.
Multiple functions yield multiple rows:
tsdf.agg(['sum'])
tsdf.agg(['sum', 'mean'])

On a Series, multiple functions return a Series, indexed by the function names:
tsdf['A'].agg(['sum', 'mean'])

Passing a lambda function will yield a <lambda> named row:
tsdf['A'].agg(['sum', lambda x: x.mean()])
sum         31.000000
<lambda>    10.333333

Passing a named function will yield that name for the row:
def my_mean(x):
    return x.mean()
tsdf['A'].agg(['sum', my_mean])
sum        31.000000
my_mean    10.333333

Aggregating with a dict
------------------------
Passing a dictionary of column names to a scalar or a list of scalars, to DataFrame.agg allows you to customize which functions are applied to which columns. Note that the results are not in any particular order, you can use an OrderedDict instead to guarantee ordering.
tsdf.agg({
    'A':'mean',
    'B':'sum'
})

Passing a list-like will generate a DataFrame output. You will get a matrix-like output of all of the aggregators. The output will consist of all unique functions. Those that are not noted for a particular column will be NaN:
tsdf.agg({
    'A':['max', 'min'],
    'B':'sum'
})

Mixed dtypes
------------
When presented with mixed dtypes that cannot aggregate, .agg will only take the valid aggregations. This is similar to how .groupby.agg works.
mdf = pd.DataFrame({
    'A':[1,2,3],
    'B':[1.0,2.0,3.0],
    'C':['foo', 'bar', 'baz'],
    'D':pd.date_range('2020.01.01', periods=3, freq='2D')
})
mdf.dtypes
mdf.agg(['min', 'sum'])

Custom describe
-----------------
With .agg() it is possible to easily create a custom describe function, similar to the built in describe function.
from functools import partial
from functools import partial
q_25 = partial(pd.Series.quantile, q=0.25)
q_25.__name__ = "25%"
q_75 = partial(pd.Series.quantile, q=0.75)
q_75.__name__ = "75%"
tsdf.agg(["count", "mean", "std", "min", q_25, "median", q_75, "max"])

Transform API
===============
The transform() METHOD RETURNS AN OBJECT THAT IS INDEXED THE SAME (SAME SIZE) AS THE ORIGINAL. This API allows you to provide multiple operations at the same time rather than one-by-one. Its API is quite similar to the .agg API.
Transform the entire frame. .transform() allows input functions as: a NumPy function, a string function name or a user defined function.
tsdf.transform(np.abs)
tsdf.transform('abs')
tsdf.transform(lambda x: x.abs())

Here transform() received a single function; this is equivalent to a ufunc application.
np.abs(tsdf)

Passing a single function to .transform() with a Series will yield a single Series in return.
tsdf['A'].transform('abs')
tsdf['A'].agg('abs') # transform is similar to agg.

Transform with multiple functions
Passing multiple functions will yield a column MultiIndexed DataFrame. The first level will be the original frame column names; the second level will be the names of the transforming functions.
tsdf.transform([np.abs, lambda x:x+1])

                        A	                B	            C
            absolute	<lambda>	absolute	<lambda>	absolute	<lambda>
2021-01-01	4.0	            5.0	        12.0	13.0	        18.0	19.0
2021-01-04	7.0	            8.0	        3.0	    4.0	            5.0     12.0

Passing multiple functions to a Series will yield a DataFrame. The resulting column names will be the transforming functions.
tsdf['A'].transform([np.abs, lambda x:x+1])

Transforming with a dict
Passing a dict of functions will allow selective transforming per column.
tsdf.transform({"A": np.abs, "B": lambda x: x + 1})

Passing a dict of lists will generate a MultiIndexed DataFrame with these selective transforms.
tsdf.transform({"A": np.abs, "B": [lambda x: x + 1, "sqrt"]})

Difference between agg and transform?
https://stackoverflow.com/questions/40957932/transform-vs-aggregate-in-pandas#:~:text=1%20Answer&text=maybe%20you%20want%20these%20values,as%20what%20you%20started%20with.&text=agg%20is%20used%20when%20you,run%20on%20the%20same%20column.

consider the dataframe df
dft = pd.DataFrame(dict(A=list('aabb'), B=[1, 2, 3, 4], C=[0, 9, 0, 9]))

groupby is the standard use aggregater
dft.groupby('A').mean()

	B	C
A
a	1.5	4.5
b	3.5	4.5

maybe you want these values broadcast across the whole group and return something with the same index as what you started with.
use transform
dft.groupby('A').transform('mean')
	B	C
0	1.5	4.5
1	1.5	4.5
2	3.5	4.5
3	3.5	4.5

dft.set_index('A').groupby(level='A').transform('mean')
	B	C
A
a	1.5	4.5
a	1.5	4.5
b	3.5	4.5
b	3.5	4.5

agg is used when you have specific things you want to run for different columns or more than one thing run on the same column.
df.groupby('A').agg(['mean', 'std'])

        B	            C
    mean	std	    mean	std
A
a	1.5	0.707107	4.5	6.363961
b	3.5	0.707107	4.5	6.363961

dft.groupby('A').agg(dict(B='sum', C=['mean', 'prod']))

        B	        C
        sum	    mean	prod
A
a	    3	    4.5	    0
b	    7	    4.5	    0

Applying elementwise functions
===============================
Since not all functions can be vectorized (accept NumPy arrays and return another array or value), the methods applymap() on DataFrame and analogously map() on Series accept any Python function taking a single value and returning a single value. For example:
def f(x):
    return len(str(x))

tsdf.A.map(f)
tsdf.applymap(f)

Series.map() has an additional feature; it can be used to easily “link” or “map” values defined by a secondary series. This is closely related to merging/joining functionality:
s = pd.Series(["six", "seven", "six", "seven", "six"], index=["a", "b", "c", "d", "e"])
t = pd.Series({"six": 6.0, "seven": 7.0})
s.map(t)

2.3.7 Reindexing and altering labels
=====================================
reindex() is the fundamental data alignment method in pandas. It is used to implement nearly all other features relying on label-alignment functionality. To reindex means to conform the data to match a given set of labels along a particular axis. This accomplishes several things:
• Reorders the existing data to match a new set of labels
• Inserts missing value (NA) markers in label locations where no data for that label existed
• If specified, fill data for missing labels using logic (highly relevant to working with time series data)
Here is a simple example:
s = pd.Series(np.random.randn(5), index=["a", "b", "c", "d", "e"])
s.reindex(["e", "b", "f", "d"])

Here, the f label was not contained in the Series and hence appears as NaN in the result.
With a DataFrame, you can simultaneously reindex the index and columns:
df = DataFrame(
    np.random.randn(4, 3),
    columns=['one', 'two', 'three'],
    index=['a', 'b', 'c', 'd']
)
df.loc['a']['three'] = np.nan
df.loc['d']['one'] = np.nan
df

df.reindex(index=['c', 'f', 'a'], columns=['three', 'two', 'one'])

which one is faster to rearrange columns?
%timeit df.reindex(columns=['three', 'two', 'one']) # 435 µs ± 17.1 µs per loop
%timeit df[['three', 'two', 'one']] # 572 µs ± 26.2 µs per loop

You may also use reindex with an axis keyword:
df.reindex(["c", "f", "b"], axis="index")

Note that the Index objects containing the actual axis labels can be shared between objects. So if we have a Series and a DataFrame, the following can be done:
rs = s.reindex(df.index)
rs.index is df.index # True

This means that the reindexed Series’s index is the same Python object as the DataFrame’s index.
DataFrame.reindex() also supports an “axis-style” calling convention, where you specify a single labels argument and the axis it applies to.
df.reindex(["c", "f", "b"], axis="index")
df.reindex(["three", "two", "one"], axis="columns")

Note: When writing performance-sensitive code, there is a good reason to spend some time becoming a reindexing ninja: many operations are faster on pre-aligned data. Adding two unaligned DataFrames internally triggers a reindexing step. For exploratory analysis you will hardly notice the difference (because reindex has been heavily optimized), but when CPU cycles matter sprinkling a few explicit reindex calls here and there can have an impact.

Reindexing to align with another object
----------------------------------------
You may wish to take an object and reindex its axes to be labeled the same as another object. While the syntax for this is straightforward albeit verbose, it is a common enough operation that the reindex_like() method is available to make this simpler:
This means, you can change the shape of the dataframe as per another dataframe.
In below example we are changing the shape of df(4,3) similar to df2(3,2)
df2 = DataFrame(
    np.random.randn(3,2),
    columns=['one', 'two'],
    index = ['a', 'b', 'c']
)
df2

df.reindex_like(df2)

Aligning objects with each other with align:
--------------------------------------------
The align() method is the fastest way to simultaneously align two objects. It supports a join argument (related to joining and merging):
• join='outer': take the union of the indexes (default)
• join='left': use the calling object’s index
• join='right': use the passed object’s index
• join='inner': intersect the indexes

It returns a tuple with both of the reindexed Series:
s = pd.Series(np.arange(1,6), index=['a', 'b', 'c', 'd', 'e'])
s1 = s[:4]
s2 = s[1:]
s1.align(s2)
s1.align(s2, join='inner')
s1.align(s2, join='left')

For DataFrames, the join method will be applied to both the index and the columns by default:
df2.align(df)
df2.align(df, join='inner')
df2.align(df, join='right')

You can also pass an axis option to only align on the specified axis:
df.align(df2, join='inner', axis=0)

If you pass a Series to DataFrame.align(), you can choose to align both objects either on the DataFrame’s index or columns using the axis argument:
df.align(df.iloc[0], axis=1)

Filling while reindexing
-------------------------
reindex() takes an optional parameter method which is a filling method chosen from the following table:
Method              Action
pad / ffill         Fill values forward
bfill / backfill    Fill values backward
nearest             Fill from the nearest index value

We illustrate these fill methods on a simple Series:
rng = pd.date_range('2021.01.01', periods=8)
ts = pd.Series(np.arange(1,9), index=rng)
ts2 = ts[[0, 3, 6]]
ts2.reindex(ts.index)
ts2.reindex(ts.index, method='ffill')
ts2.reindex(ts.index, method='bfill')
ts2.reindex(ts.index, method='nearest')

These methods require that the indexes are ordered increasing or decreasing.
Note that the same result could have been achieved using fillna (except for method='nearest') or interpolate:
ts2.reindex(ts.index).fillna(method="ffill")
ts2.reindex(ts.index).fillna(method="bfill")

reindex() will raise a ValueError if the index is not monotonically increasing or decreasing. fillna() and interpolate() will not perform any checks on the order of the index.
tst = pd.Series(np.arange(1,5), index=['4', '1', '3', '2'])
tst.reindex(index = ['2', '8', '4'], method='ffill') # ValueError
tst.reindex(index = ['2', '8', '4']).fillna(method='ffill')

Limits on filling while reindexing
------------------------------------
The limit and tolerance arguments provide additional control over filling while reindexing. Limit specifies the maximum count of consecutive matches:
ts2.reindex(ts.index, method='ffill', limit=1)

In contrast, tolerance specifies the maximum distance between the index and indexer value
ts2.reindex(ts.index, method='ffill', tolerance='1 day')

Notice that when used on a DatetimeIndex, TimedeltaIndex or PeriodIndex, tolerance will coerced into a Timedelta if possible. This allows you to specify tolerance with appropriate strings.

Dropping labels from an axis
-----------------------------
A method closely related to reindex is the drop() function. It removes a set of labels from an axis:
df.drop(['a', 'd'], axis=0)
df.drop(['two'], axis=1)
Note that the following also works, but is a bit less obvious / clean:
df.reindex(df.index.difference(['a', 'd']))

Renaming / mapping labels
--------------------------
The rename() method allows you to relabel an axis based on some mapping (a dict or Series) or an arbitrary function.
s = pd.Series(['ram', 'sur', 'cha', 'mag', 'pho'], index=['a', 'b', 'c', 'd', 'e'])
s.rename(str.upper) # renames the index

If you pass a function, it must return a value when called with any of the labels (and must produce a set of unique values). A dict or Series can also be used:
df.rename(columns={'one': 'foo', 'two':'bar'},
         index={'a':'aam', 'b':'badaaam', 'd':'doorhaiaam'}
         )

If the mapping doesn’t include a column/index label, it isn’t renamed. Note that extra labels in the mapping don’t throw an error.
Added 'e' to column but it didn't throw an error
df.rename(columns={'one': 'foo', 'two':'bar'},
         index={'a':'aam', 'b':'badaaam', 'd':'doorhaiaam', 'e':'eaam'}
         )
DataFrame.rename() also supports an “axis-style” calling convention, where you specify a single mapper and the axis to apply that mapping to.
df.rename({"one": "foo", "two": "bar"}, axis="columns")
df.rename({"a": "apple", "b": "banana", "d": "durian"}, axis="index")

The rename() method also provides an inplace named parameter that is by default False and copies the under- lying data. Pass inplace=True to rename the data in place.
Finally, rename() also accepts a scalar or list-like for altering the Series.name attribute.
s.rename('scaler-name')

The methods DataFrame.rename_axis() and Series.rename_axis() allow specific names of a MultiIndex to be changed (as opposed to the labels).
df = pd.DataFrame({
    'x':np.arange(1,7), 'y':np.arange(10,61,10)
}, index=pd.MultiIndex.from_product(
    [['a', 'b', 'c'], [1, 2]], names=['let', 'num']),
)
df.rename_axis(index={'let':'abc'})
df.rename_axis(index=str.upper)

================
2.3.8 Iteration
================
The behavior of basic iteration over pandas objects depends on the type. When iterating over a Series, it is regarded as array-like, and basic iteration produces the values. DataFrames follow the dict-like convention of iterating over the “keys” of the objects.
Inshort,basic iteration(for i in object)produces:
• Series: values
• DataFrame: column labels
Thus, for example, iterating over a DataFrame gives you the column names:
df = pd.DataFrame(
    np.arange(6).reshape(3,2),
    index = ['a', 'b', 'c'],
    columns=['col1', 'col2']
)
for col in df:
    print (col)

for val in s:
    print(val)

pandas objects also have the dict-like items() method to iterate over the (key, value) pairs. To iterate over the rows of a DataFrame, you can use the following methods:
• iterrows(): Iterate over the rows of a DataFrame as (index, Series) pairs. This converts the rows to Series objects, which can change the dtypes and has some performance implications.
• itertuples(): Iterate over the rows of a DataFrame as namedtuples of the values. This is a lot faster than iterrows(), and is in most cases preferable to use to iterate over the values of a DataFrame.
 Warning: Iterating through pandas objects is generally slow.In many cases,iterating manually over the rows is not needed and can be avoided with one of the following approaches:
• Look for a vectorized solution: many operations can be performed using built-in methods or NumPy functions, (boolean) indexing etc
• When you have a function that cannot work on the full Data Frame/Series at once,it is better to use apply() instead of iterating over the values.
• If you need to do iterative manipulations on the values but performance is important,consider writing the inner loop with cython or numba.

Warning: You should never modify something you are iterating over.This is not guaranteed to work in all cases. Depending on the data types, the iterator returns a copy and not a view, and writing to it will have no effect!
For example, in the following case setting the value has no effect:
df = pd.DataFrame({"a": [1, 2, 3], "b": ["a", "b", "c"]})
for index, row in df.iterrows():
    print(index)
    print(row)
    row['a'] = 10
df

items
------
Consistent with the dict-like interface, items() iterates through key-value pairs:
• Series: (index, scalar value) pairs
• DataFrame: (column, Series) pairs

for label, ser in df.items():
    print(ser)
    print(label)

iterrows():
-----------
iterrows() allows you to iterate through the rows of a DataFrame as Series objects. It returns an iterator yielding each index value along with a Series containing the data in each row:
for index,colNameValue in df.iterrows():
    print(index)
    print(colNameValue)

Note: Because iterrows() returns a Series for each row, it does not preserve dtypes across the rows (dtypes are preserved across columns for DataFrames). For example,
df_orig = pd.DataFrame([[1, 1.5]], columns=["int", "float"])
df_orig
df_orig.dtypes
row = next(df_orig.iterrows())[1]
print(row)
type(row) # pandas.core.series.Series
row['int'].dtype # dtype('float64')
df_orig['int'].dtype # dtype('int64')

---------next()--------------
The next() function returns the next item in an iterator.
You can add a default return value, to return if the iterable has reached to its end.
Syntax - next(iterable, default)
mylist = iter(["apple", "banana", "cherry"])
x = next(mylist, "orange")
print(x)
------------------------------

To preserve dtypes while iterating over the rows, it is better to use itertuples() which returns namedtuples of the values and which is generally much faster than iterrows().

For instance, a contrived way to transpose the DataFrame would be:
df2 = pd.DataFrame({"x": [1, 2, 3], "y": [4, 5, 6]})
df2.T

pd.DataFrame({index: colSeries for index, colSeries in df2.iterrows()}) ~ df2.T

itertuples
----------
The itertuples() method will return an iterator yielding a namedtuple for each row in the DataFrame. The first element of the tuple will be the row’s corresponding index value, while the remaining values are the row values.
For instance:
for row in df.itertuples():
    print(row)
Pandas(Index=0, a=1, b='a')
Pandas(Index=1, a=2, b='b')
Pandas(Index=2, a=3, b='c')

This method does not convert the row to a Series object; it merely returns the values inside a namedtuple. Therefore, itertuples() preserves the data type of the values and is generally faster as iterrows().
Note: The column names will be renamed to positional names if they are invalid Python identifiers,repeated,or start with an underscore. With a large number of columns (>255), regular tuples are returned.

---------------------
2.3.9 .dt accessor
---------------------
Series has an accessor to succinctly return datetime like properties for the values of the Series, if it is a date- time/period like Series. This will return a Series, indexed like the existing Series.
s = pd.Series(pd.date_range('2021.01.01 10:30:15', periods=4))
s.dt.second
This enables nice expressions like this:
s[s.dt.day == 2]
You can easily produces tz aware transformations:
s.dt.tz_localize('US/Eastern')
You can also chain these types of operations:
s.dt.tz_localize('UTC').dt.tz_convert('US/Eastern')

You can also format datetime values as strings with Series.dt.strftime() which supports the same format as the standard strftime().
datetime to object(str)
s = pd.Series(pd.date_range("20130101", periods=4))
s.dt.strftime('%Y/%m/%d')

The .dt accessor works for period and timedelta dtypes.
period to object(Str)
s = pd.Series(pd.period_range("20130101", periods=4))
s.dt.strftime('%Y/%m/%d')
s.dt.month
s.dt.day

timedelta:
s = pd.Series(pd.timedelta_range('1 day 00:05:10', periods=4, freq="s"))
s.dt.days
s.dt.seconds
s.dt.components

Hence different forms of dateTime for timeseries
* DatetimeIndex
* PeriodIndex
* Period
* timedelta

Note: Series.dt will raise a TypeError if you access with a non-datetime-like values.

2.3.10 Vectorized string methods
---------------------------------
Series is equipped with a set of string processing methods that make it easy to operate on each element of the array. Perhaps most importantly, these methods exclude missing/NA values automatically. These are accessed via the Series’s str attribute and generally have names matching the equivalent (scalar) built-in string methods. For example:
s = pd.Series( ["A", "B", "C", "Aaba", "Baca", np.nan, "CABA", "dog", "cat"], dtype="string")
s.str.lower()

Powerful pattern-matching methods are provided as well, but note that pattern-matching generally uses regular expres- sions by default (and in some cases always uses them).

2.3.11 Sorting
---------------
pandas supports three kinds of sorting: sorting by index labels, sorting by column values, and sorting by a combination of both.

By index
--------
The Series.sort_index() and DataFrame.sort_index() methods are used to sort a pandas object by its index levels.

df = pd.DataFrame({
    "one": pd.Series(np.random.choice(10, 3, replace=False), index=["a", "b", "c"]),
    "two": pd.Series(np.random.choice(10, 4, replace=False), index=["a", "b", "c", "d"]),
    "three": pd.Series(np.random.choice(10, 3, replace=False), index=["b", "c", "d"]),
})
df

unsorted_df = df.reindex(index=["a", "d", "c", "b"], columns=["three", "two", "one"])
print(unsorted_df)

unsorted_df.sort_index()
unsorted_df.sort_index(ascending=False)
unsorted_df.sort_index(axis=1) ## sorted column name alphabetically
unsorted_df["three"].sort_index() # Series

Sorting by index also supports a key parameter that takes a callable function to apply to the index being sorted. For
MultiIndex objects, the key is applied per-level to the levels specified by level.
s1 = pd.DataFrame({"a": ["B", "a", "C"], "b": [1, 2, 3], "c": [2, 3, 4]}
                 ).set_index(list('ab'))
s1.sort_index(level='a')
s1.sort_index(level="a", key=lambda idx: idx.str.lower())

By values
----------
The Series.sort_values() method is used to sort a Series by its values. The DataFrame. sort_values() method is used to sort a DataFrame by its column or row values. The optional by parameter to DataFrame.sort_values() may used to specify one or more columns to use to determine the sorted order.
df1 = pd.DataFrame({"one": [2, 1, 1, 1], "two": [1, 3, 2, 4], "three": [5, 4, 3, 2]})
df1.sort_values(by='two')

The by parameter can take a list of column names, e.g.:
df1.sort_values(by=['one', 'two'])

These methods have special treatment of NA values via the na_position argument:
s[2] = np.nan
s.sort_values() # places NAN at the end
s.sort_values(na_position='first')

Sorting also supports a key parameter that takes a callable function to apply to the values being sorted.
s1 = pd.Series(["B", "a", "C"])
s1.sort_values()
s1.sort_values(key=lambda x: x.str.lower())

key will be given the Series of values and should return a Series or array of the same shape with the transformed values. For DataFrame objects, the key is applied per column, so the key should still expect a Series and return a Series, e.g.
df = pd.DataFrame({"a": ["B", "a", "C"], "b": [1, 2, 3]})
df.sort_values(by="a", key=lambda col: col.str.lower())
The name or type of each column can be used to apply different functions to different columns.

By indexes and values
---------------------
Strings passed as the by parameter to DataFrame.sort_values() may refer to either columns or index level names.
idx = pd.MultiIndex.from_tuples(
[("a", 1), ("a", 2), ("a", 2), ("b", 2), ("b", 1), ("b", 1)])
idx.names=["first", "second"]
df_multi = pd.DataFrame({"A":np.arange(6,0,-1)}, index=idx)
df_multi.sort_values(by=['second', 'A'])

Note: If a string matches both a column name and an index level name then a warning is issued and the column takes precedence. This will result in an ambiguity error in a future version.

searchsorted
-------------
Series has the searchsorted() method, which works similarly to numpy.ndarray.searchsorted().

-----------np.searchsorted()------------------------
numpy.searchsorted(a, v, side='left', sorter=None)[source]
Find indices where elements should be inserted to maintain order.
Binary search is used to find the required insertion points.
np.searchsorted([1,2,3,4,5], 3) # 2
np.searchsorted([1,2,3,4,5], 3, side='right') # 3
np.searchsorted([1,2,3,4,5], [-10, 10, 2, 3]) # array([0, 5, 1, 2])
-----------------------------------------------------

ser = pd.Series([1, 2, 9])
ser.searchsorted(4) # 2, this is the index where 4 can be placed to maintain the sorted order of Series
ser.searchsorted([0, 6]) # array([0, 2])
ser.searchsorted([1,9]) # array([0, 2])
ser.searchsorted([1,9], side='right') # array([1, 3]), default being left
ser = pd.Series([3, 1, 2])
np.argsort([3,1,2]) # array([1, 2, 0]), smallest value at 1nth position
ser.searchsorted([0, 3], sorter=np.argsort(ser)) # array([0, 2])

smallest / largest values
--------------------------
Series has the nsmallest() and nlargest() methods which return the smallest or largest 𝑛 values. For a large Series this can be much faster than sorting the entire Series and calling head(n) on the result.
s = pd.Series(np.random.permutation(10))
s.sort_values()
s.nsmallest(3)
s.nlargest(3)
s.nlargest(3).sort_values()

DataFrame also has the nlargest and nsmallest methods.
df = pd.DataFrame({
    "a": [1,5,3,5,7,9,8],
    "b": list("abdceff"),
    "c": [1.0, 2.0, 4.0, 3.2, np.nan, 3.0, 4.0],
})
print(df.nlargest(5,'a'))
df.nlargest(5,['a', 'c'])
df.nsmallest(3,'c')

Sorting by a MultiIndex column
-------------------------------
You must be explicit about sorting when the column is a MultiIndex, and fully specify all levels to by.
df1.columns = pd.MultiIndex.from_tuples([("a", "one"), ("a", "two"), ("b", "three")])
df1.sort_values(by=("a", "two"))

2.3.12 Copying
---------------
The copy() method on pandas objects copies the underlying data (though not the axis indexes, since they are im- mutable) and returns a new object. Note that it is seldom necessary to copy objects. For example, there are only a handful of ways to alter a DataFrame in-place:
• Inserting, deleting, or modifying a column.
• Assigning to the index or columns attributes.
• For homogeneous data, directly modifying the values via the values attribute or advanced indexing.
To be clear, no pandas method has the side effect of modifying your data; almost every method returns a new object, leaving the original object untouched. If the data is modified, it is because you did so explicitly.

2.3.13 dtypes
-------------
For the most part, pandas uses NumPy arrays and dtypes for Series or individual columns of a DataFrame. NumPy provides support for float, int, bool, timedelta64[ns] and datetime64[ns] (note that NumPy does not support timezone-aware datetimes).
pandas and third-party libraries extend NumPy’s type system in a few places. This section describes the extensions pandas has made internally.
The following table lists all of pandas extension types. For methods requiring dtype arguments, strings can be specified as indicated. See the respective documentation sections for more on each type.

Kind of Data        Data Type       Scalar      Array                       String Aliases          Documentation
tz-aware datetime   DatetimeTZDtype Timestamp   arrays.DatetimeArray    'datetime64[ns, <tz>]'  Time zone handling
Categorical         CategoricalDtype (none)     Categorical             'category'               Categorical data
period (time spans) PeriodDtype     Period      arrays.PeriodArray  'period[<freq>]', 'Period[<freq>]'  Time span representation
sparse              SparseDtype     (none)      arrays.SparseArray      'Sparse', 'Sparse[int]', 'Sparse[float]'
Sparse data structures
intervals           IntervalDtype   Interval    arrays.IntervalArray
'interval', 'Interval', 'Interval[<numpy_dtype>]', 'Interval[datetime64[ns, <tz>]]', 'Interval[timedelta64[<freq>]]'
IntervalIndex

nullable integer    Int64Dtype, …   (none)      arrays.IntegerArray 'Int8', 'Int16', 'Int32', 'Int64', 'UInt8', 'UInt16', 'UInt32', 'UInt64'        Nullable integer data type

Strings             StringDtype         str     arrays.StringArray          'string'        Working with text data
Boolean (with NA)   BooleanDtype        bool    arrays.BooleanArray         'boolean'   Boolean data with missing values

pandas has two ways to store strings.
1. object dtype, which can hold any Python object, including strings.
2. StringDtype, which is dedicated to strings.
Generally, we recommend using StringDtype.

Finally, arbitrary objects may be stored using the object dtype, but should be avoided to the extent possible (for
performance and interoperability with other libraries and methods.

A convenient dtypes attribute for DataFrame returns a Series with the data type of each column.

dft = pd.DataFrame({
    "A": np.random.rand(3),
    "B": 1,
    "C": "foo",
    "D": pd.Timestamp("20010102"),
    "E": pd.Series([1.0] * 3).astype("float32"),
    "F": False,
    "G": pd.Series([1] * 3, dtype="int8"),
})
dft.dtypes

On a Series object, use the dtype attribute.
dft['A'].dtype

If a pandas object contains data with multiple dtypes in a single column, the dtype of the column will be chosen to accommodate all of the data types (object is the most general).
# these ints are coerced to floats
pd.Series([1, 2, 3, 4, 5, 6.0])

# string data forces an ``object`` dtype
pd.Series([1, 2, 3, 6.0, "foo"])

The number of columns of each type in a DataFrame can be found by calling DataFrame.dtypes. value_counts().
dft.dtypes.value_counts()

Numeric dtypes will propagate and can coexist in DataFrames. If a dtype is passed (either directly via the dtype keyword, a passed ndarray, or a passed Series), then it will be preserved in DataFrame operations. Furthermore, different numeric dtypes will NOT be combined. The following example will give you a taste.

df1 = pd.DataFrame(np.random.randn(8, 1), columns=["A"], dtype="float32")
df1.dtypes

df2 = pd.DataFrame({
    "A": pd.Series(np.random.randn(8), dtype="float16"),
    "B": pd.Series(np.random.randn(8)),
    "C": pd.Series(np.array(np.random.randn(8), dtype="uint8")),
})
df2.dtypes
## float64 is not combined with float16 in above example.

defaults
---------
By default integer types are int64 and float types are float64, regardless of platform (32-bit or 64-bit). The following will all result in int64 dtypes.

pd.DataFrame([1, 2], columns=["a"]).dtypes # a    int64
pd.DataFrame({"a": [1, 2]}).dtypes # a    int64
pd.DataFrame({"a": 1}, index=list(range(2))).dtypes # a    int64

Note that Numpy will choose platform-dependent types when creating arrays. The following WILL result in int32 on 32-bit platform.
frame = pd.DataFrame(np.array([1, 2]))

upcasting:
----------
Types can potentially be upcasted when combined with other types, meaning they are promoted from the current type (e.g. int to float).
print(df1.head())
print(df1.dtypes)
print('*'*10)
print(df2.head())
print(df2.dtypes)
df3 = df1.reindex_like(df2).fillna(value=0.0) + df2
df3.dtypes

DataFrame.to_numpy() will return the lower-common-denominator of the dtypes, meaning the dtype that can accommodate ALL of the types in the resulting homogeneous dtyped NumPy array. This can force some upcasting.
df3.to_numpy().dtype # dtype('float64')

astype
------
You can use the astype() method to explicitly convert dtypes from one to another. These will by default return a copy, even if the dtype was unchanged (pass copy=False to change this behavior). In addition, they will raise an exception if the astype operation is invalid.
Upcasting is always according to the NumPy rules. If two different dtypes are involved in an operation, then the more general one will be used as the result of the operation.
# conversion of dtypes
df3.astype('float32').dtypes

Convert a subset of columns to a specified type using astype().
dft = pd.DataFrame({"a": [1, 2, 3], "b": [4, 5, 6], "c": [7, 8, 9]})
dft.dtypes
dft[["a", "b"]] = dft[["a", "b"]].astype(np.uint8)
dft.dtypes

Convert certain columns to a specific dtype by passing a dict to astype().
dft1 = pd.DataFrame({"a": [1, 0, 1], "b": [4, 5, 6], "c": [7, 8, 9]})
dft1 = dft1.astype({"a": np.bool_, "c": np.float64})
dft1.dtypes

Note: When trying to convert a subset of columns to a specified type using astype() and loc(),upcasting occurs. loc() tries to fit in what we are assigning to the current dtypes, while [] will overwrite them taking the dtype from
the right hand side. Therefore the following piece of code produces the unintended result.
dft = pd.DataFrame({"a": [1, 2, 3], "b": [4, 5, 6], "c": [7, 8, 9]})
dft.loc[:, ["a", "b"]].astype(np.uint8).dtypes
dft.loc[:, ["a", "b"]] = dft.loc[:, ["a", "b"]].astype(np.uint8)
dft.dtypes

object conversion
------------------
pandas offers various functions to try to force conversion of types from the object dtype to other types. In cases where the data is already of the correct type, but stored in an object array, the DataFrame.infer_objects() and Series.infer_objects() methods can be used to soft convert to the correct type.
import datetime
df = pd.DataFrame([
    [1, 2,],
    ["a", "b"],
    [datetime.datetime(2019,1,2), datetime.datetime(2019,2,3)]
])
df = df.T
df.dtypes

0            object
1            object
2    datetime64[ns]

Because the data was transposed the original inference stored all columns as object, which infer_objects will correct.
df.infer_objects().dtypes
0             int64
1            object
2    datetime64[ns]

The following functions are available for one dimensional object arrays or scalars to perform hard conversion of objects to a specified type:
• to_numeric() (conversion to numeric dtypes)
m = [1.1, 2, 3]
pd.to_numeric(m)

• to_datetime() (conversion to datetime objects)
import datetime
m = ['2021.01.02', datetime.datetime(2021,1,3)]
pd.to_datetime(m)

• to_timedelta() (conversion to timedelta objects)
m = ['5µs', pd.Timedelta("1day")]
pd.to_timedelta(m)

To force a conversion, we can pass in an errors argument, which specifies how pandas should deal with elements that cannot be converted to desired dtype or object. By default, errors='raise', meaning that any errors encountered will be raised during the conversion process. However, if errors='coerce', these errors will be ignored and pandas will convert problematic elements to pd.NaT (for datetime and timedelta) or np.nan (for numeric). This might be useful if you are reading in data which is mostly of the desired dtype (e.g. numeric, datetime), but occasionally has non-conforming elements intermixed that you want to represent as missing:
import datetime
m = ['apple', datetime.datetime(2020, 4, 8)]
pd.to_datetime(m, errors='coerce') # DatetimeIndex(['NaT', '2020-04-08'],

m = ['apple', 2, 3.3]
pd.to_numeric(m, errors='coerce') # array([nan, 2. , 3.3])

m = ['apple', pd.Timedelta("1day")]
pd.to_timedelta(m, errors='coerce') # TimedeltaIndex([NaT, '1 days']

The errors parameter has a third option of errors='ignore', which will simply return the passed in data if it encounters any errors with the conversion to a desired data type:
import datetime
m = ['apple', datetime.datetime(2020, 4, 8)]
pd.to_datetime(m, errors='ignore') # Index(['apple', 2020-04-08 00:00:00], dtype='object')

m = ['apple', 2, 3.3]
pd.to_numeric(m, errors='ignore') # array(['apple', 2, 3.3], dtype=object)

m = ['apple', pd.Timedelta("1day")]
pd.to_timedelta(m, errors='ignore') # array(['apple', Timedelta('1 days 00:00:00')], dtype=object)

In addition to object conversion, to_numeric() provides another argument downcast, which gives the option of downcasting the newly (or already) numeric data to a smaller dtype, which can conserve memory:
In [410]: m = ["1", 2, 3]
In [411]: pd.to_numeric(m, downcast="integer") # smallest signed int dtype
Out[411]: array([1, 2, 3], dtype=int8)
In [412]: pd.to_numeric(m, downcast="signed") # same as 'integer'
Out[412]: array([1, 2, 3], dtype=int8)
In [413]: pd.to_numeric(m, downcast="unsigned") # smallest unsigned int dtype
Out[413]: array([1, 2, 3], dtype=uint8)
In [414]: pd.to_numeric(m, downcast="float") # smallest float dtype Out[414]: array([1., 2., 3.], dtype=float32)

As these methods apply only to one-dimensional arrays, lists or scalars; they cannot be used directly on multi- dimensional objects such as DataFrames. However, with apply(), we can “apply” the function over each column efficiently:

import datetime
df = pd.DataFrame([['2021.02.21', datetime.datetime(2021,3,2)]]*2)
print(df.info())
df.apply(pd.to_datetime).info()

df = pd.DataFrame([["1.1", 2, 3]] * 2, dtype="O")
print(df.dtypes)
df.apply(pd.to_numeric).dtypes

df = pd.DataFrame([["5us", pd.Timedelta("1day")]] * 2, dtype="O")
print(df.dtypes)
df.apply(pd.to_timedelta).dtypes

gotchas
-------
Performing selection operations on integer type data can easily upcast the data to floating. The dtype of the input data will be preserved in cases where nans are not introduced.
df3
print(df3.dtypes)
dfi = df3.astype("int32")
print(dfi.dtypes)
dfi['E'] = 1
print(dfi.dtypes)
dfi[dfi>0].dtypes

While float dtypes are unchanged.
dfa = df3.copy()
print(dfa.dtypes)
dfa['A'] = dfa['A'].astype('float32')
dfa[dfa>0].dtypes

2.3.14 Selecting columns based on dtype
---------------------------------------
The select_dtypes() method implements subsetting of columns based on their dtype.
First, let’s create a DataFrame with a slew of different dtypes:
df = pd.DataFrame({
    'string': list('abc'),
    'int64': list(range(1,4)),
    "uint8": np.arange(3, 6).astype("u1"),
    "float64": np.arange(4.0, 7.0),
    "bool1": [True, False, True],
    "bool2": [False, True, False],
    "dates": pd.date_range("now", periods=3),
    "category": pd.Series(list("ABC")).astype("category"),
})
df["tdeltas"] = df.dates.diff()
df["uint64"] = np.arange(3, 6).astype("u8")
df["other_dates"] = pd.date_range("20130101", periods=3)
df["tz_aware_dates"] = pd.date_range("20130101", periods=3, tz="US/Eastern")
df.dtypes

select_dtypes() has two parameters include and exclude that allow you to say “give me the columns with these dtypes” (include) and/or “give the columns without these dtypes” (exclude).
For example, to select bool columns:
df.select_dtypes(include=['bool'])
You can also pass the name of a dtype in the NumPy dtype hierarchy:
df.select_dtypes(include=['datetime64'])

select_dtypes() also works with generic dtypes as well.
For example, to select all numeric and boolean columns while excluding unsigned integers:
df.select_dtypes(include=['number', 'bool'], exclude=['unsignedinteger'])

To select string columns you must use the object dtype:
df.select_dtypes(include=['object'])

To see all the child dtypes of a generic dtype like numpy.number you can define a function that returns a tree of child dtypes:
def subtypes(dtype):
    subs = dtype.__subclasses__()
    if not subs:
        return dtype
    return [dtype, [subtypes(dt) for dt in subs]]
subtypes(np.number)
subtypes(np.integer)
subtypes(np.generic)

Note: pandas also defines the types category, and datetime64[ns, tz], which are not integrated into the normal NumPy hierarchy and won’t show up with the above function.

2.4 IO tools (text, CSV, HDF5, . . . )
---------------------------------------
The pandas I/O API is a set of top level reader functions accessed like pandas.read_csv() that generally return a pandas object. The corresponding writer functions are object methods that are accessed like DataFrame. to_csv(). Below is a table containing available readers and writers.
FormatType      DataDescription             Reader                  Writer
text                CSV                     read_csv                to_csv
text        Fixed-Width Text File           read_fwf
text                JSON                    read_json               to_json
text                HTML                    read_html               to_html
text            Local clipboard             read_clipboard          to_clipboard
binary             MS Excel                 read_excel              to_excel
binary          OpenDocument                read_excel
binary          HDF5 Format                 read_hdf                to_hdf
binary          Feather Format              read_feather            to_feather
binary          Parquet Format              read_parquet            to_parquet
binary          ORC Format                  read_orc
binary          Msgpack                     read_msgpack            to_msgpack
binary          Stata                       read_stata              to_stata
binary          SAS                         read_sas
binary          SPSS                        read_spss
binary      Python Pickle Format            read_pickle             to_pickle
SQL             SQL                         read_sql                to_sql
SQL         Google BigQuery                 read_gbq                to_gbq

Note: For examples that use the StringIO class, make sure you import it with from io import StringIO for Python 3.

2.4.1 CSV & text files
=========================
The workhorse function for reading text files (a.k.a. flat files) is read_csv().

Parsing options
read_csv() accepts the following common arguments:

Basic
filepath_or_buffer [various] Either a path to a file (a str, pathlib.Path, or py._path.local. LocalPath), URL (including http, ftp, and S3 locations), or any object with a read() method (such as an open file or StringIO).

sep [str, defaults to ',' for read_csv(), \t for read_table()] Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python’s builtin sniffer tool, csv.Sniffer. In addition, separators longer than 1 character and different from '\s+' will be interpreted as regular expressions and will also force the use of the Python parsing engine. Note that regex delimiters are prone to ignoring quoted data. Regex example: '\\r\\t'.

delimiter [str, default None] Alternative argument name for sep.

delim_whitespace [boolean, default False] Specifies whether or not whitespace (e.g. ' ' or '\t') will be used as the delimiter. Equivalent to setting sep='\s+'. If this option is set to True, nothing should be passed in for the delimiter parameter.

Column and index locations and names
header [int or list of ints, default 'infer'] Row number(s) to use as the column names, and the start of the data. Default behavior is to infer the column names: if no names are passed the behavior is identical to header=0 and column names are inferred from the first line of the file, if column names are passed explicitly then the behavior is identical to header=None. Explicitly pass header=0 to be able to replace existing names.
The header can be a list of ints that specify row locations for a MultiIndex on the columns e.g. [0,1,3]. Intervening rows that are not specified will be skipped (e.g. 2 in this example is skipped). Note that this parameter ignores commented lines and empty lines if skip_blank_lines=True, so header=0 denotes the first line of data rather than the first line of the file.

names [array-like, default None] List of column names to use. If file contains no header row, then you should explicitly pass header=None. Duplicates in this list are not allowed.

index_col [int, str, sequence of int / str, or False, default None] Column(s) to use as the row labels of the DataFrame, either given as string name or column index. If a sequence of int / str is given, a MultiIndex is used.
Note: index_col=False can be used to force pandas to not use the first column as the index, e.g. when you have a malformed file with delimiters at the end of each line.
The default value of None instructs pandas to guess. If the number of fields in the column header row is equal to the number of fields in the body of the data file, then a default index is used. If it is one larger, then the first field is used as an index.

usecols [list-like or callable, default None] Return a subset of the columns. If list-like, all elements must either be positional (i.e. integer indices into the document columns) or strings that correspond to column names provided either by the user in names or inferred from the document header row(s). For example, a valid list- likeusecolsparameterwouldbe[0, 1, 2]or['foo', 'bar', 'baz'].
Element order is ignored, so usecols=[0, 1] is the same as [1, 0]. To instantiate a DataFrame from data with element order preserved use pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']] for columns in ['foo', 'bar'] order or pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]for['bar', 'foo']order.
If callable, the callable function will be evaluated against the column names, returning names where the callable function evaluates to True:

from io import StringIO
data = 'col1,col2,col3\na,b,1\na,b,2\na,c,3'
pd.read_csv(StringIO(data))
pd.read_csv(StringIO(data), usecols=lambda x: x.upper() in['COL1', 'COL3'])

squeeze [boolean, default False] If the parsed data only contains one column then return a Series.
prefix [str, default None] Prefix to add to column numbers when no header, e.g. ‘X’ for X0, X1, . . .
mangle_dupe_cols [boolean, default True] Duplicate columns will be specified as ‘X’, ‘X.1’. . . ’X.N’, rather than ‘X’. . . ’X’. Passing in False will cause data to be overwritten if there are duplicate names in the columns.

General parsing configuration
dtype [Type name or dict of column -> type, default None] Data type for data or columns. E.g. {'a': np. float64, 'b': np.int32} (unsupported with engine='python'). Use str or object together with suitable na_values settings to preserve and not interpret dtype.
engine [{'c', 'python'}] Parser engine to use. The C engine is faster while the Python engine is currently more feature-complete.
converters [dict,defaultNone]Dictoffunctionsforconvertingvaluesincertaincolumns.Keyscaneitherbeintegers or column labels.
true_values [list, default None] Values to consider as True. false_values [list, default None] Values to consider as False. skipinitialspace [boolean, default False] Skip spaces after delimiter.

skiprows [list-like or integer, default None] Line numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file.
If callable, the callable function will be evaluated against the row indices, returning True if the row should be skipped and False otherwise:
data = "col1,col2,col3\na,b,1\na,b,2\nc,d,3"
pd.read_csv(StringIO(data))
pd.read_csv(StringIO(data), skiprows=lambda x: x % 2 != 0)

na_values [scalar, str, list-like, or dict, default None] Additional strings to recognize as NA/NaN. If dict passed, specific per-column NA values. See na values const below for a list of the values interpreted as NaN by default.
keep_default_na [boolean, default True] Whether or not to include the default NaN values when parsing the data. Depending on whether na_values is passed in, the behavior is as follows:
• Ifkeep_default_naisTrue,andna_valuesarespecified,na_valuesisappendedtothedefault NaN values used for parsing.
• If keep_default_na is True, and na_values are not specified, only the default NaN values are used for parsing.
• If keep_default_na is False, and na_values are specified, only the NaN values specified na_values are used for parsing.
• If keep_default_na is False, and na_values are not specified, no strings will be parsed as NaN. Note that if na_filter is passed in as False, the keep_default_na and na_values parameters will
be ignored.
na_filter [boolean, default True] Detect missing value markers (empty strings and the value of na_values). In data
without any NAs, passing na_filter=False can improve the performance of reading a large file. verbose [boolean, default False] Indicate number of NA values placed in non-numeric columns. skip_blank_lines [boolean, default True] If True, skip over blank lines rather than interpreting as NaN values.

Datetime handling
------------------
parse_dates [boolean or list of ints or names or list of lists or dict, default False.]
• If True -> try parsing the index.
• If [1, 2, 3] -> try parsing columns 1, 2, 3 each as a separate date column.
• If [[1, 3]] -> combine columns 1 and 3 and parse as a single date column.
• If {'foo': [1, 3]} -> parse columns 1, 3 as date and call result ‘foo’. A fast-path exists for iso8601- formatted dates.
infer_datetime_format [boolean, default False] If True and parse_dates is enabled for a column, attempt to infer the datetime format to speed up the processing.
keep_date_col [boolean, default False] If True and parse_dates specifies combining multiple columns then keep the original columns.
date_parser [function, default None] Function to use for converting a sequence of string columns to an array of datetime instances. The default uses dateutil.parser.parser to do the conversion. pandas will try to call date_parser in three different ways, advancing to the next if an exception occurs: 1) Pass one or more arrays (as defined by parse_dates) as arguments; 2) concatenate (row-wise) the string values from the columns defined by parse_dates into a single array and pass that; and 3) call date_parser once for each row using one or more strings (corresponding to the columns defined by parse_dates) as arguments.
dayfirst [boolean, default False] DD/MM format dates, international and European format.
cache_dates [boolean, default True] If True, use a cache of unique, converted dates to apply the datetime conversion.
May produce significant speed-up when parsing duplicate date strings, especially ones with timezone offsets.

Iteration
iterator [boolean, default False] Return TextFileReader object for iteration or getting chunks with get_chunk().
chunksize [int, default None] Return TextFileReader object for iteration

Quoting, compression, and file format
compression [{'infer', 'gzip', 'bz2', 'zip', 'xz', None, dict}, default 'infer'] For on-the-fly de- compression of on-disk data. If ‘infer’, then use gzip, bz2, zip, or xz if filepath_or_buffer is path-like ending in ‘.gz’, ‘.bz2’, ‘.zip’, or ‘.xz’, respectively, and no decompression otherwise. If using ‘zip’, the ZIP file must contain only one data file to be read in. Set to None for no decompression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, or bz2.BZ2File. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}.
thousands [str, default None] Thousands separator.
decimal [str, default '.'] Character to recognize as decimal point. E.g. use ',' for European data.

float_precision [string, default None] Specifies which converter the C engine should use for floating-point values. The options are None for the ordinary converter, high for the high-precision converter, and round_trip for the round-trip converter.
lineterminator [str (length 1), default None] Character to break file into lines. Only valid with C parser.
quotechar [str (length 1)] The character used to denote the start and end of a quoted item. Quoted items can include
the delimiter and it will be ignored.
quoting [int or csv.QUOTE_* instance, default 0] Control field quoting behavior per csv.QUOTE_* constants.
Use one of QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).
doublequote [boolean, default True] When quotechar is specified and quoting is not QUOTE_NONE, indi- cate whether or not to interpret two consecutive quotechar elements inside a field as a single quotechar element.
escapechar [str (length 1), default None] One-character string used to escape delimiter when quoting is QUOTE_NONE.
comment [str, default None] Indicates remainder of line should not be parsed. If found at the beginning of a line, the line will be ignored altogether. This parameter must be a single character. Like empty lines (as long as skip_blank_lines=True), fully commented lines are ignored by the parameter header but not by skiprows. For example, if comment='#', parsing ‘#empty\na,b,c\n1,2,3’ with header=0 will result in ‘a,b,c’ being treated as the header.
encoding [str,defaultNone]EncodingtouseforUTFwhenreading/writing(e.g.'utf-8').ListofPythonstandard encodings.
dialect [str or csv.Dialect instance, default None] If provided, this parameter will override values (default or not) for the following parameters: delimiter, doublequote, escapechar, skipinitialspace, quotechar, and quoting. If it is necessary to override values, a ParserWarning will be issued. See csv. Dialect documentation for more details.

Error handling
error_bad_lines [boolean, default True] Lines with too many fields (e.g. a csv line with too many commas) will by default cause an exception to be raised, and no DataFrame will be returned. If False, then these “bad lines” will dropped from the DataFrame that is returned. See bad lines below.
warn_bad_lines [boolean, default True] If error_bad_lines is False, and warn_bad_lines is True, a warning for each “bad line” will be output.

Specifying column data types
You can indicate the data type for the whole DataFrame or individual columns:
data = "a,b,c,d\n1,2,3,4\n5,6,7,8\n9,10,11"
df = pd.read_csv(StringIO(data), dtype=object)
df = pd.read_csv(StringIO(data), dtype={"b": object, "c": np.float64, "d":"Int64"})

Fortunately, pandas offers more than one way to ensure that your column(s) contain only one dtype. If you’re unfamiliar with these concepts,

For instance, you can use the converters argument of read_csv():
data = "col_1\n1\n2\n'A'\n4.22"
df = pd.read_csv(StringIO(data), converters={"col_1": str})
df["col_1"].apply(type).value_counts()

Or you can use the to_numeric() function to coerce the dtypes after reading in the data,which will convert all valid parsing to floats, leaving the invalid parsing as NaN.
df2 = pd.read_csv(StringIO(data))
df2["col_1"] = pd.to_numeric(df2["col_1"], errors="coerce")
df2["col_1"].apply(type).value_counts()

Ultimately, how you deal with reading in columns containing mixed dtypes depends on your specific needs. In the case above, if you wanted to NaN out the data anomalies, then to_numeric() is probably your best option. However, if you wanted for all the data to be coerced, no matter the type, then using the converters argument of read_csv() would certainly be worth trying.

Note: In some cases, reading in abnormal data with columns containing mixed dtypes will result in an inconsistent dataset. If you rely on pandas to infer the dtypes of your columns, the parsing engine will go and infer the dtypes for different chunks of the data, rather than the whole dataset at once. Consequently, you can end up with column(s) with mixed dtypes. For example,
will result with mixed_df containing an int dtype for certain chunks of the column, and str for others due to the mixed dtypes from the data that was read in. It is important to note that the overall column will be marked with a dtype of object, which is used for columns with mixed dtypes.
col1 = list(range(500000)) + ["a", "b"] + list(range(50000))
df=pd.DataFrame({'col1': col1})
df.to_csv('foo.csv')
mixed_df = pd.read_csv('foo.csv')
mixed_df['col1'].apply(type).value_counts()

Specifying categorical dtype
-----------------------------
Categorical columns can be parsed directly by dtype=CategoricalDtype(categories, ordered).
data = "col1,col2,col3\na,b,1\na,b,2\nc,d,3"
pd.read_csv(StringIO(data)).dtypes
pd.read_csv(StringIO(data), dtype="category").dtypes
Individual columns can be parsed as a Categorical using a dict specification:
pd.read_csv(StringIO(data), dtype={'col1':'category'}).dtypes

Specifying dtype='category' will result in an unordered Categorical whose categories are the unique values observed in the data. For more control on the categories and order, create a CategoricalDtype ahead of time, and pass that for that column’s dtype.
dtype = CategoricalDtype(['d', 'c', 'b', 'a'], ordered=True)
pd.read_csv(StringIO(data), dtype={'col1':dtype}).dtypes

When using dtype=CategoricalDtype, “unexpected” values outside of dtype.categories are treated as missing values.
dtype = CategoricalDtype(["a", "b", "d"]) # No 'c'
pd.read_csv(StringIO(data), dtype={'col1':dtype}).col1

This matches the behavior of Categorical.set_categories().
Note: With dtype='category', the resulting categories will always be parsed as strings (object dtype). If the
categories are numeric they can be converted using the to_numeric() function, or as appropriate, another converter
such as to_datetime().
When dtype is a CategoricalDtype with homogeneous categories ( all numeric, all datetimes, etc.), the
conversion is done automatically.
df = pd.read_csv(StringIO(data), dtype="category")
df.dtypes
df.col3
df["col3"].cat.categories = pd.to_numeric(df['col3'].cat.categories)
df.col3

Naming and using columns
Handling column names
============================
A file may or may not have a header row. pandas assumes the first row should be used as the column names:
data = "a,b,c\n1,2,3\n4,5,6\n7,8,9"
print(data)
pd.read_csv(StringIO(data))
By specifying the names argument in conjunction with header you can indicate other names to use and whether or not to throw away the header row (if any):
pd.read_csv(StringIO(data), names=["foo", "bar", "baz"], header=0)
# Replace Header name with provided names
pd.read_csv(StringIO(data), names=["foo", "bar", "baz"], header=0)
# Add new column names and treat existing data as rows
pd.read_csv(StringIO(data), names=["foo", "bar", "baz"], header=None)
pd.read_csv(StringIO(data), names=["foo", "bar", "baz"])

If the header is in a row other than the first, pass the row number to header. This will skip the preceding rows:
data = "skip this skip it\na,b,c\n1,2,3\n4,5,6\n7,8,9"
pd.read_csv(StringIO(data), header=1)
Note: Default behavior is to infer the column names: if no names are passed the behavior is identical to header=0 and column names are inferred from the first non-blank line of the file, if column names are passed explicitly then the behavior is identical to header=None.

Duplicate names parsing
-----------------------
If the file or header contains duplicate names, pandas will by default distinguish between them so as to prevent overwriting data:
data = "a,b,a\n0,1,2\n3,4,5"
pd.read_csv(StringIO(data))
There is no more duplicate data because mangle_dupe_cols=True by default, which modifies a series of dupli- cate columns ‘X’, . . . , ‘X’ to become ‘X’, ‘X.1’, . . . , ‘X.N’. If mangle_dupe_cols=False, duplicate data can arise:
To prevent users from encountering this problem with duplicate data, a ValueError exception is raised if mangle_dupe_cols != True:
data = 'a,b,a\n0,1,2\n3,4,5'
pd.read_csv(StringIO(data), mangle_dupe_cols=False)
ValueError: Setting mangle_dupe_cols=False is not supported yet

Filtering columns (usecols):
----------------------------
The usecols argument allows you to select any subset of the columns in a file, either using the column names, position numbers or a callable:
data = "a,b,c,d\n1,2,3,foo\n4,5,6,bar\n7,8,9,baz"
pd.read_csv(StringIO(data), usecols=['b', 'd'])
pd.read_csv(StringIO(data), usecols=[0,2,3])
pd.read_csv(StringIO(data), usecols=lambda x: x.upper() in ["A", "C"])

The usecols argument can also be used to specify which columns not to use in the final result:
pd.read_csv(StringIO(data), usecols=lambda x: x not in ["a", "c"])
In this case, the callable is specifying that we exclude the “a” and “c” columns from the output.

Comments and empty lines
Ignoring line comments and empty lines
----------------------------------------
If the comment parameter is specified, then completely commented lines will be ignored. By default, completely blank lines will be ignored as well.
data = "\na,b,c\n \n# commented line\n1,2,3\n\n4,5,6"
pd.read_csv(StringIO(data), comment="#")
If skip_blank_lines=False, then read_csv will not ignore blank lines:
data = "a,b,c\n\n1,2,3\n\n\n4,5,6"
pd.read_csv(StringIO(data), skip_blank_lines=False)

Warning: The presence of ignored lines might create ambiguities involving line numbers;the parameter header uses row numbers (ignoring commented/empty lines), while skiprows uses line numbers (including commented/empty lines):
data = "#comment\na,b,c\nA,B,C\n1,2,3"
pd.read_csv(StringIO(data), comment="#", header=1)
data = "A,B,C\n#comment\na,b,c\n1,2,3"
pd.read_csv(StringIO(data), comment="#", skiprows=2)
If both header and skiprows are specified, header will be relative to the end of skiprows. For example:
data = (
 "# empty\n"
 "# second empty line\n"
 "# third emptyline\n"
 "X,Y,Z\n"
 "1,2,3\n"
 "A,B,C\n"
 "1,2.,4.\n"
 "5.,NaN,10.0\n"  )
pd.read_csv(StringIO(data), comment="#", skiprows=4, header=1)

Comments
========
Sometimes comments or meta data may be included in a file:
file - temp.csv
ID,level,category
Patient1,123000,x # really unpleasant
Patient2,23000,y # wouldn't take his medicine
Patient3,1234018,z # awesome

We can suppress the comments using the comment keyword:
df = pd.read_csv('temp.csv', comment='#')
df

Dealing with Unicode data
--------------------------
The encoding argument should be used for encoded unicode data, which will result in byte strings being decoded to unicode in the result:
from io import BytesIO
data = b"word,length\n" b"Tr\xc3\xa4umen,7\n" b"Gr\xc3\xbc\xc3\x9fe,5"
data = data.decode("utf8").encode("latin-1")
df = pd.read_csv(BytesIO(data), encoding="latin-1")
df['word'][1]
Some formats which encode all characters as multiple bytes, like UTF-16, won’t parse correctly at all without specifying the encoding.

Index columns and trailing delimiters
--------------------------------------
If a file has one more column of data than the number of column names, the first column will be used as the DataFrame’s row names i.e index:
data = "a,b,c\n4,apple,bat,5.7\n8,orange,cow,10"
pd.read_csv(StringIO(data))
data = "index,a,b,c\n4,apple,bat,5.7\n8,orange,cow,10"
pd.read_csv(StringIO(data), index_col=0)

Ordinarily, you can achieve this behavior using the index_col option.
There are some exception cases when a file has been prepared with delimiters at the end of each data line, confusing
the parser. To explicitly disable the index column inference and discard the last column, pass index_col=False:
data = "a,b,c\n4,apple,bat,\n8,orange,cow,"
pd.read_csv(StringIO(data))
pd.read_csv(StringIO(data), index_col=False)

if column name also contains an extra comma
data = "a,b,c,\n4,apple,bat,\n8,orange,cow,"
pd.read_csv(StringIO(data), index_col=False, names=['a','b','c'], header=0)

If a subset of data is being parsed using the usecols option, the index_col specification is based on that subset, not the original data.
data = "a,b,c\n4,apple,bat,\n8,orange,cow,"
pd.read_csv(StringIO(data), usecols=["b", "c"], index_col=False)


Specifying categorical dtype
----------------------------
To better facilitate working with datetime data, read_csv() uses the keyword arguments parse_dates and date_parser to allow users to specify a variety of columns and date/time formats to turn the input text data into datetime objects.
The simplest case is to just pass in parse_dates=True:
# Use a column as an index, and parse it as dates.
df = pd.DataFrame({
    'date':pd.date_range('2021.02.01', periods=3),
    'A':['a','b','c'],
    'B':[1, 3, 4],
    'C':[2, 4, 5],
})
df.to_csv('foo.csv', index=False)
pd.read_csv('foo.csv', index_col=0, parse_dates=True)
# These are Python datetime objects
df.index # DatetimeIndex(['2021-02-01', '2021-02-02', '2021-02-03'], dtype='datetime64[ns]', name='date', freq=None)

It is often the case that we may want to store date and time data separately, or store various date fields separately. the parse_dates keyword can be used to specify a combination of columns to parse the dates and/or times from.
You can specify a list of column lists to parse_dates, the resulting date columns will be prepended to the output (so as to not affect the existing column order) and the new column names will be the concatenation of the component column names:
print(open('temp.csv').read())
pd.read_csv('temp.csv', header=None, parse_dates=[[1, 2], [1, 3]])

By default the parser removes the component date columns, but you can choose to retain them via the keep_date_col keyword:
pd.read_csv("temp.csv", header=None, keep_date_col=True, parse_dates=[[1, 2], [1, 3]])

Note that if you wish to combine multiple columns into a single date column, a nested list must be used. In other words,parse_dates=[1, 2]indicates that the second and third columns should each be parsed as separate date columns while parse_dates=[[1, 2]]means the two columns should be parsed into a single column.
You can also use a dict to specify custom name columns:
date_spec = {"nominal": [1, 2], "actual": [1, 3]}
pd.read_csv("temp.csv", header=None, parse_dates=date_spec)

It is important to remember that if multiple text columns are to be parsed into a single date column, then a new column is prepended to the data. The index_col specification is based off of this new set of columns rather than the original data columns:
pd.read_csv("temp.csv", header=None, parse_dates=date_spec, index_col=0)
pd.read_csv("temp.csv", header=None, parse_dates=date_spec, index_col=1)

Note: If a column or index contains an unparsable date, the entire column or index will be returned unaltered as an object data type. For non-standard datetime parsing, use to_datetime() after pd.read_csv.
Note: read_csv has a fast_path for parsing datetime strings in iso8601 format,e.g“2000-01-01T00:01:02+00:00” and similar variations. If you can arrange for your data to store datetimes in this format, load times will be significantly faster, ~20x has been observed.

Date parsing functions
-----------------------
Finally, the parser allows you to specify a custom date_parser function to take full advantage of the flexibility of the date parsing API:
date_spec = {"nominal": [1, 2], "actual": [1, 3]}
print(pd.read_csv('temp.csv', parse_dates=date_spec).dtypes)
pd.read_csv('temp.csv', parse_dates=date_spec, date_parser=pd.to_datetime).dtypes

pandas will try to call the date_parser function in three different ways. If an exception is raised, the next one is tried:
1. date_parser is first called with one or more arrays as arguments, as defined using parse_dates (e.g., date_parser(['2013', '2013'], ['1', '2'])).
2. If #1 fails, date_parser is called with all the columns concatenated row-wise into a single array (e.g., date_parser(['2013 1', '2013 2'])).
Note that performance-wise, you should try these methods of parsing dates in order:
1. Try to infer the format using infer_datetime_format=True (see section below).
2. If you know the format, use pd.to_datetime(): date_parser=lambda x: pd. to_datetime(x, format=...).
3. If you have a really non-standard format, use a custom date_parser function. For optimal performance, this should be vectorized, i.e., it should accept arrays as arguments.

Parsing a CSV with mixed timezones
------------------------------------
pandas cannot natively represent a column or index with mixed timezones. If your CSV file contains columns with a mixture of timezones, the default result will be an object-dtype column with strings, even with parse_dates.
content = """\
a
2000-01-01T00:00:00+05:00
2000-01-01T00:00:00+06:00"""
print(pd.read_csv(StringIO(content), parse_dates=['a']))
(pd.read_csv(StringIO(content), parse_dates=True))
To parse the mixed-timezone values as a datetime column, pass a partially-applied to_datetime() with utc=True as the date_parser.
pd.read_csv(StringIO(content), parse_dates=['a'],
            date_parser=lambda col:pd.to_datetime(col, utc=True))

Inferring datetime format
---------------------------
If you have parse_dates enabled for some or all of your columns, and your datetime strings are all formatted the same way, you may get a large speed up by setting infer_datetime_format=True. If set, pandas will attempt to guess the format of your datetime strings, and then use a faster means of parsing the strings. 5-10x parsing speeds have been observed. pandas will fallback to the usual parsing if either the format cannot be guessed or the format that was guessed cannot properly parse the entire column of strings. So in general, infer_datetime_format should not have any negative consequences if enabled.
Here are some examples of datetime strings that can be guessed (All representing December 30th, 2011 at 00:00:00):
Note that infer_datetime_format is sensitive to dayfirst. With dayfirst=True, it will guess “01/12/2011” to be December 1st. With dayfirst=False (default) it will guess “01/12/2011” to be January 12th.
# Try to infer the format for the index column
pd.read_csv('foo.csv', index_col=0, parse_dates=True, infer_datetime_format=True)

International date formats
While US date formats tend to be MM/DD/YYYY, many international formats use DD/MM/YYYY instead. For convenience, a dayfirst keyword is provided:
date,value,cat
1/6/2000,5,a
2/6/2000,10,b
3/6/2000,15,c
pd.read_csv('temp.csv', parse_dates=[0], dayfirst=True)

Writing CSVs to binary file objects
------------------------------------
df.to_csv(..., mode="wb") allows writing a CSV to a file object opened binary mode. In most cases, it is
not necessary to specify mode as Pandas will auto-detect whether the file object is opened in text or binary mode
import io
data = pd.DataFrame([0, 1, 2])
data.to_csv('test.csv', encoding="utf-8", index=False, compression='gzip')
pd.read_csv('test.csv', encoding="utf-8", compression='gzip')

Specifying method for floating-point conversion
------------------------------------------------
The parameter float_precision can be specified in order to use a specific floating-point converter during parsing with the C engine. The options are the ordinary converter, the high-precision converter, and the round-trip converter (which is guaranteed to round-trip values after writing to a file). For example:
val = "0.3066101993807095471566981359501369297504425048828125"
data = f"a,b,c\n1,2,{val}"
abs(pd.read_csv(StringIO(data), engine='c', float_precision=None)['c'][0] - float(val)) # 5.551115123125783e-17
abs(pd.read_csv(StringIO(data), engine='c', float_precision='high')['c'][0] - float(val)) # 5.551115123125783e-17
abs(pd.read_csv(StringIO(data), engine='c', float_precision='round_trip')['c'][0] - float(val)) # 0.0

Thousand separators
--------------------
For large numbers that have been written with a thousands separator, you can set the thousands keyword to a string of length 1 so that integers will be parsed correctly:
By default, numbers with a thousands separator will be parsed as strings:
ID|level|category
Patient1|123,000|x
Patient2|23,000|y
Patient3|1,234,018|z
df = pd.read_csv('temp.csv', sep='|')
df.level.dtype # dtype('O')
The thousands keyword allows integers to be parsed correctly:
df = pd.read_csv('temp.csv', sep='|', thousands=',')
df.level.dtype # dtype('int64')

NA Values:
----------
To control which values are parsed as missing values (which are signified by NaN), specify a string in na_values. If you specify a list of strings, then all values in it are considered to be missing values. If you specify a number (a float, like 5.0 or an integer like 5), the corresponding equivalent values will also imply a missing value (in this case effectively[5.0, 5]arerecognizedasNaN).
To completely override the default values that are recognized as missing, specify keep_default_na=False.
The default NaN recognized values are ['-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN', '#N/ A N/A', '#N/A', 'N/A', 'n/a', 'NA', '<NA>', '#NA', 'NULL', 'null', 'NaN', '-NaN', 'nan', '-nan', ''].

pd.read_csv("path_to_file.csv", na_values=[5])
In the example above 5 and 5.0 will be recognized as NaN, in addition to the defaults. A string will first be interpreted as a numerical 5, then as a NaN.

pd.read_csv("path_to_file.csv", keep_default_na=False, na_values=[""])
Above, only an empty field will be recognized as NaN.

pd.read_csv("path_to_file.csv", keep_default_na=False, na_values=["NA", "0"])
Above, both NA and 0 as strings are NaN.

pd.read_csv("path_to_file.csv", na_values=["Nope"])
The default values, in addition to the string "Nope" are recognized as NaN.

Test:
s,val
'a',10
'b',5
'c',5.0
'd',20
'e',
'f',30
'g',0
'h',"NA"
'i',"Nope"
pd.read_csv('temp.csv', na_values=[5])
pd.read_csv('temp.csv', keep_default_na=False, na_values=[""])
pd.read_csv('temp.csv', keep_default_na=False, na_values=["NA", "0"])
pd.read_csv('temp.csv', na_values=["Nope"])

Infinity
--------
inf like values will be parsed as np.inf (positive infinity), and -inf as -np.inf (negative infinity). These will ignore the case of the value, meaning Inf, will also be parsed as np.inf.

Returning Series
-----------------
Using the squeeze keyword, the parser will return output with a single column as a Series:
level
Patient1,123000
Patient2,23000
Patient3,1234018

output = pd.read_csv('temp.csv', squeeze=True)
type(output) # pandas.core.series.Series

Boolean values
--------------
The common values True, False, TRUE, and FALSE are all recognized as boolean. Occasionally you might want to recognize other values as being boolean. To do this, use the true_values and false_values options as follows:
data = "a,b,c\n1,Yes,2\n3,No,4"
pd.read_csv(StringIO(data), true_values=['Yes'], false_values=['No', '0'])

Handling “bad” lines
---------------------
Some files may have malformed lines with too few fields or too many. Lines with too few fields will have NA values filled in the trailing fields. Lines with too many fields will raise an error by default:
data = "a,b,c\n1,2,3\n4,5,6,7\n8,9,10"
pd.read_csv(StringIO(data))
ParserError: Error tokenizing data. C error: Expected 3 fields in line 3, saw 4
You can elect to skip bad lines:
pd.read_csv(StringIO(data), error_bad_lines=False)
You can also use the usecols parameter to eliminate extraneous column data that appear in some lines but not others:
pd.read_csv(StringIO(data), usecols=[0,1,2])

Dialect
-------
The dialect keyword gives greater flexibility in specifying the file format. By default it uses the Excel dialect but you can specify either the dialect name or a csv.Dialect instance.
Suppose you had data with unenclosed quotes:
By default, read_csv uses the Excel dialect and treats the double quote as the quote character, which causes it to fail when it finds a newline before it finds the closing double quote.
We can get around this using dialect:
label1,label2,label3
index1,"a,c,e
index2,b,d,f

import csv
dia = csv.excel()
dia.quoting = csv.QUOTE_NONE
pd.read_csv('temp.csv', dialect=dia)

All of the dialect options can be specified separately by keyword arguments:
data = "a,b,c~1,2,3~4,5,6"
pd.read_csv(StringIO(data), lineterminator='~')

Another common dialect option is skipinitialspace, to skip any whitespace after a delimiter:
data = "a, b , c\n1, 2, 3\n4, 5, 6"
pd.read_csv(StringIO(data), skipinitialspace=True)

The parsers make every attempt to “do the right thing” and not be fragile. Type inference is a pretty big deal. If a column can be coerced to integer dtype without altering the contents, the parser will do so. Any non-numeric columns will come through as object dtype as with the rest of pandas objects.

Quoting and Escape Characters
------------------------------
Quotes (and other escape characters) in embedded fields can be handled in any number of ways. One way is to use backslashes; to properly parse this data, you should pass the escapechar option:
data = 'a,b\n"hello, \\"Bob\\", nice to see you",5'
pd.read_csv(StringIO(data), escapechar='\\')

Files with fixed width columns
-------------------------------
While read_csv() reads delimited data, the read_fwf() function works with data files that have known and fixed column widths. The function parameters to read_fwf are largely the same as read_csv with two extra parameters, and a different usage of the delimiter parameter:
• colspecs: A list of pairs (tuples) giving the extents of the fixed-width fields of each line as half-open intervals (i.e., [from, to[ ). String value ‘infer’ can be used to instruct the parser to try detecting the column specifications from the first 100 rows of the data. Default behavior, if not specified, is to infer.
• widths: A list of field widths which can be used instead of ‘colspecs’ if the intervals are contiguous.
• delimiter: Characters to consider as filler characters in the fixed-width file. Can be used to specify the filler
character of the fields if it is not spaces (e.g., ‘~’).

Consider a typical fixed-width data file:
id8141 360.242940 149.910199 11950.7
id1594 444.953632 166.985655 11788.4
id1849 364.136849 183.628767 11806.2
id1230 413.836124 184.375703 11916.8
id1948 502.953953 173.237159 12468.3

In order to parse this file into a DataFrame, we simply need to supply the column specifications to the read_fwf function along with the file name:

# Column specifications are a list of half-intervals
colspecs = [(0, 6), (8, 20), (21, 33), (35, 43)]
#pd.read_fwf('temp.csv', colspecs=colspecs, header=None, index_col=0)
pd.read_fwf('temp.csv', header=None, index_col=0)

Note how the parser automatically picks column names X.<column number> when header=None argument is spec- ified. Alternatively, you can supply just the column widths for contiguous columns:
widths = [6, 14, 13, 10]
df = pd.read_fwf("temp.csv", widths=widths, header=None)

The parser will take care of extra white spaces around the columns so it’s ok to have extra separation between the columns in the file.
By default, read_fwf will try to infer the file’s colspecs by using the first 100 rows of the file. It can do it only in cases when the columns are aligned and correctly separated by the provided delimiter (default delimiter is whitespace).
pd.read_fwf('temp.csv', header=None, index_col=0)

read_fwf supports the dtype parameter for specifying the types of parsed columns to be different from the inferred type.
pd.read_fwf("temp.csv", header=None, index_col=0).dtypes
pd.read_fwf("temp.csv", header=None, index_col=0, dtype={2: "object"}).dtypes

Indexes
Files with an “implicit” index column
======================================
Consider a file with one less entry in the header than the number of data column:
A,B,C
20090101,a,1,2
20090102,b,3,4
20090103,c,4,5

df = pd.read_csv('temp.csv', parse_dates=True)
df.index

Reading an index with a MultiIndex
Suppose you have data indexed by two columns:
year,indiv,zit,xit
1977,"A",1.2,.6
1977,"B",1.5,.5
1977,"C",1.7,.8
1978,"A",.2,.06
1978,"B",.7,.2
1978,"C",.8,.3
1978,"D",.9,.5
1978,"E",1.4,.9
1979,"C",.2,.15
1979,"D",.14,.05
1979,"E",.5,.15
1979,"F",1.2,.5
1979,"G",3.4,1.9
1979,"H",5.4,2.7
1979,"I",6.4,1.2
The index_col argument to read_csv can take a list of column numbers to turn multiple columns into a MultiIndex for the index of the returned object:
df = pd.read_csv('temp.csv', index_col=[0,1])
df
df.loc[1978]

Reading columns with a MultiIndex
----------------------------------
By specifying list of row locations for the header argument, you can read in a MultiIndex for the columns.
Specifying non-consecutive rows will skip the intervening rows.
from pandas._testing import makeCustomDataframe as mkdf
df = mkdf(5, 3, r_idx_nlevels=2, c_idx_nlevels=4)
df

C0,,C_l0_g0,C_l0_g1,C_l0_g2
C1,,C_l1_g0,C_l1_g1,C_l1_g2
C2,,C_l2_g0,C_l2_g1,C_l2_g2
C3,,C_l3_g0,C_l3_g1,C_l3_g2
R0,R1,,,
R_l0_g0,R_l1_g0,R0C0,R0C1,R0C2
R_l0_g1,R_l1_g1,R1C0,R1C1,R1C2
R_l0_g2,R_l1_g2,R2C0,R2C1,R2C2
R_l0_g3,R_l1_g3,R3C0,R3C1,R3C2
R_l0_g4,R_l1_g4,R4C0,R4C1,R4C2

read_csv is also able to interpret a more common format of multi-columns indices.
,a,a,a,b,c,c
,q,r,s,t,u,v
one,1,2,3,4,5,6
two,7,8,9,10,11,12

pd.read_csv('temp.csv', index_col=0, header=[0, 1])
Note: If an index_col is not specified (e.g. you don’t have an index, or wrote it with df.to_csv(..., index=False), then any names on the columns index will be lost.

Automatically “sniffing” the delimiter
---------------------------------------
read_csv is capable of inferring delimited (not necessarily comma-separated) files, as pandas uses the csv. Sniffer class of the csv module. For this, you have to specify sep=None.
:0:1:2:3
0:0.4691122999071863:-0.2828633443286633:-1.5090585031735124:-1.1356323710171934
1:1.2121120250208506:-0.17321464905330858:0.11920871129693428:-1.0442359662799567
2:-0.8618489633477999:-2.1045692188948086:-0.4949292740687813:1.071803807037338
3:0.7215551622443669:-0.7067711336300845:-1.0395749851146963:0.27185988554282986
4:-0.42497232978883753:0.567020349793672:0.27623201927771873:-1.0874006912859915
5:-0.6736897080883706:0.1136484096888855:-1.4784265524372235:0.5249876671147047
6:0.4047052186802365:0.5770459859204836:-1.7150020161146375:-1.0392684835147725
7:-0.3706468582364464:-1.1578922506419993:-1.344311812731667:0.8448851414248841
8:1.0757697837155533:-0.10904997528022223:1.6435630703622064:-1.4693879595399115
9:0.35702056413309086:-0.6746001037299882:-1.776903716971867:-0.9689138124473498

pd.read_csv('temp.csv', sep=None, engine='python', index_col=0)

Reading multiple files to create a single DataFrame:
----------------------------------------------------
It’s best to use concat() to combine multiple files.

Iterating through files chunk by chunk
---------------------------------------
Suppose you wish to iterate through a (potentially very large) file lazily rather than reading the entire file into memory, such as the following:
|0|1|2|3
0|0.4691122999071863|-0.2828633443286633|-1.5090585031735124|-1.1356323710171934
1|1.2121120250208506|-0.17321464905330858|0.11920871129693428|-1.0442359662799567
2|-0.8618489633477999|-2.1045692188948086|-0.4949292740687813|1.071803807037338
3|0.7215551622443669|-0.7067711336300845|-1.0395749851146963|0.27185988554282986
4|-0.42497232978883753|0.567020349793672|0.27623201927771873|-1.0874006912859915
5|-0.6736897080883706|0.1136484096888855|-1.4784265524372235|0.5249876671147047
6|0.4047052186802365|0.5770459859204836|-1.7150020161146375|-1.0392684835147725
7|-0.3706468582364464|-1.1578922506419993|-1.344311812731667|0.8448851414248841
8|1.0757697837155533|-0.10904997528022223|1.6435630703622064|-1.4693879595399115
9|0.35702056413309086|-0.6746001037299882|-1.776903716971867|-0.9689138124473498

To read the file in one go:
pd.read_csv('temp.csv', sep='|', index_col=0)

By specifying a chunksize to read_csv, the return value will be an iterable object of type TextFileReader:
with pd.read_csv('temp.csv', sep='|', iterator=True, index_col=0) as reader:
    p = reader.get_chunk(5)
print(p)

Specifying the parser engine
----------------------------
Under the hood pandas uses a fast and efficient parser implemented in C as well as a Python implementation which is currently more feature-complete. Where possible pandas uses the C parser (specified as engine='c'), but may fall back to Python if C-unsupported options are specified. Currently, C-unsupported options include:
• sep other than a single character (e.g. regex separators) • skipfooter
• sep=None with delim_whitespace=False
Specifying any of the above options will produce a ParserWarning unless the python engine is selected explicitly using engine='python'.

Reading/writing remote files
-----------------------------
You can pass in a URL to read or write remote files to many of pandas’ IO functions - the following example shows reading a CSV file:
df = pd.read_csv("https://download.bls.gov/pub/time.series/cu/cu.item", sep="\t")
df.head()
df.info()
df.shape
df.sample(5)

All URLs which are not local files or HTTP(s) are handled by fsspec, if installed, and its various filesystem implementations (including Amazon S3, Google Cloud, SSH, FTP, webHDFS. . . ). Some of these implementations will require additional packages to be installed, for example S3 URLs require the s3fs library:
df = pd.read_json("s3://pandas-test/adatafile.json")
When dealing with remote storage systems, you might need extra configuration with environment variables or config files in special locations. For example, to access data in your S3 bucket, you will need to define credentials in one of the several ways listed in the S3Fs documentation. The same is true for several of the storage backends, and you
should follow the links at fsimpl1 for implementations built into fsspec and fsimpl2 for those not included in the main fsspec distribution.
You can also pass parameters directly to the backend driver. For example, if you do not have S3 credentials, you can still access public data by specifying an anonymous connection, such as
New in version 1.2.0.
pd.read_csv( "s3://ncei-wcsd-archive/data/processed/SH1305/18kHz/SaKe2013" "-D20130523-T080854_to_SaKe2013-D20130523-T085643.csv", storage_options={"anon": True},
)
fsspec also allows complex URLs, for accessing data in compressed archives, local caching of files, and more. To locally cache the above example, you would modify the call to
where we specify that the “anon” parameter is meant for the “s3” part of the implementation, not to the caching implementation. Note that this caches to a temporary directory for the duration of the session only, but you can also specify a permanent store.

------------------------
Writing out data
Writing to CSV format
------------------------
df = pd.DataFrame({
    'a':np.arange(400000),
    'b':np.arange(400000, 800000)
})
df.to_csv('temp.csv', sep='|', na_rep='Ramesh', index=False, chunksize=2)

The Series and DataFrame objects have an instance method to_csv which allows storing the contents of the object as a comma-separated-values file. The function takes a number of arguments. Only the first is required.
• path_or_buf: A string path to the file to write or a file object. If a file object it must be opened with newline=''
• sep : Field delimiter for the output file (default “,”)
• na_rep: A string representation of a missing value (default ‘’)
• float_format: Format string for floating point numbers
• columns: Columns to write (default None)
• header: Whether to write out the column names (default True)
• index: whether to write row (index) names (default True)
• index_label: Column label(s) for index column(s) if desired. If None (default), and header and index are True, then the index names are used. (A sequence should be given if the DataFrame uses MultiIndex).
• mode : Python write mode, default ‘w’
• encoding: a string representing the encoding to use if the contents are non-ASCII, for Python versions prior
to 3
• line_terminator: Character sequence denoting line end (default os.linesep)
• quoting: Set quoting rules as in csv module (default csv.QUOTE_MINIMAL). Note that if you have set a float_format then floats are converted to strings and csv.QUOTE_NONNUMERIC will treat them as non-numeric
• quotechar: Character used to quote fields (default ‘”’)
• doublequote: Control quoting of quotechar in fields (default True)
• escapechar: Character used to escape sep and quotechar when appropriate (default None)
• chunksize: Number of rows to write at a time
• date_format: Format string for datetime objects

Writing a formatted string
--------------------------
The DataFrame object has an instance method to_string which allows control over the string representation of the object. All arguments are optional:
• buf default None, for example a StringIO object
• columns default None, which columns to write
• col_space default None, minimum width of each column.
• na_rep default NaN, representation of NA value
• formatters default None, a dictionary (by column) of functions each of which takes a single argument and returns a formatted string
• float_format default None, a function which takes a single (float) argument and returns a formatted string; to be applied to floats in the DataFrame.
• sparsify default True, set to False for a DataFrame with a hierarchical index to print every MultiIndex key at each row.
• index_names default True, will print the names of the indices
• index default True, will print the index (ie, row labels)
• header default True, will print the column labels
• justify default left, will print column headers left- or right-justified
The Series object also has a to_string method, but with only the buf, na_rep, float_format arguments. There is also a length argument which, if set to True, will additionally output the length of the Series.

=============================================
2.4.2 JSON
Read and write JSON format files and strings.
=============================================
Writing JSON
A Series or DataFrame can be converted to a valid JSON string. Use to_json with optional parameters:
• path_or_buf : the pathname or buffer to write the output This can be None in which case a JSON string is
returned
• orient : Series:
– default is index
– allowed values are {split, records, index} DataFrame:
– default is columns
– allowed values are {split, records, index, columns, values, table}

The format of the JSON string
split       dict like {index -> [index], columns -> [columns], data -> [values]}
records     list like [{column -> value}, . . . , {column -> value}]
index       dict like {index -> {column -> value}}
columns     dict like {column -> {index -> value}}
values      just the values array

• date_format : string, type of date conversion, ‘epoch’ for timestamp, ‘iso’ for ISO8601.
• double_precision : The number of decimal places to use when encoding floating point values, default 10.
• force_ascii : force encoded string to be ASCII, default True.
• date_unit : The time unit to encode to, governs timestamp and ISO8601 precision. One of ‘s’, ‘ms’, ‘us’ or ‘ns’ for seconds, milliseconds, microseconds and nanoseconds respectively. Default ‘ms’.
• default_handler : The handler to call if an object cannot otherwise be converted to a suitable format for JSON. Takes a single argument, which is the object to convert, and returns a serializable object.
• lines : If records orient, then will write each record per line as json.
Note NaN’s, NaT’s and None will be converted to null and datetime objects will be converted based on the
date_format and date_unit parameters.

dfj = pd.DataFrame(np.random.choice(10, 6).reshape(3,2), columns=list("AB"))
print(dfj)
dfj.to_json()

Orient options
---------------
There are a number of different options for the format of the resulting JSON file / string. Consider the following DataFrame and Series:
dfjo = pd.DataFrame(
    dict(A=range(1,4), B=range(4, 7), C=range(7, 10)),
    columns=list("ABC"),
    index=list("xyz")
)
dfjo

sjo = pd.Series(dict(x=15, y=16, z=17), name='D')
sjo

Column oriented (the default for DataFrame) serializes the data as nested JSON objects with column labels acting as the primary index:
# Not available for Series
dfjo.to_json(orient='columns') # default

Index oriented (the default for Series) similar to column oriented but the index labels are now primary:
dfjo.to_json(orient="index")
sjo.to_json(orient="index")

Record oriented serializes the data to a JSON array of column -> value records, index labels are not included. This is useful for passing DataFrame data to plotting libraries, for example the JavaScript library d3.js:
dfjo.to_json(orient="records")
sjo.to_json(orient="records")

Value oriented is a bare-bones option which serializes to nested JSON arrays of values only, column and index labels are not included:
dfjo.to_json(orient="values")
# Not available for Series

Split oriented serializes to a JSON object containing separate entries for values, index and columns. Name is also included for Series:
dfjo.to_json(orient="split")
sjo.to_json(orient="split")

Table oriented serializes to the JSON Table Schema, allowing for the preservation of metadata including but not limited to dtypes and index names.
dfjo.to_json(orient="table")

Note: Any orient option that encodes to a JSON object will not preserve the ordering of index and column la- bels during round-trip serialization. If you wish to preserve label ordering use the split option as it uses ordered containers.

===============================
2.5 Indexing and selecting data
===============================
The axis labeling information in pandas objects serves many purposes:
• Identifies data (i.e. provides metadata) using known indicators, important for analysis, visualization, and inter- active console display.
• Enables automatic and explicit data alignment.
• Allows intuitive getting and setting of subsets of the data set.
In this section, we will focus on the final point: namely, how to slice, dice, and generally get and set subsets of pandas objects. The primary focus will be on Series and DataFrame as they have received more development attention in this area.

Note: The Python and NumPy indexing operators [] and attribute operator . provide quick and easy access to pandas data structures across a wide range of use cases. This makes interactive work intuitive, as there’s little new to learn if you already know how to deal with Python dictionaries and NumPy arrays. However, since the type of the data to be accessed isn’t known in advance, directly using standard operators has some optimization limits. For production code, we recommended that you take advantage of the optimized pandas data access methods exposed in this chapter.

 Warning: Whether a copy or a reference is returned for a setting operation, may depend on the context. This is sometimes called chained assignment and should be avoided.

2.5.1 Different choices for indexing
--------------------------------------
Object selection has had a number of user-requested additions in order to support more explicit location based index- ing. pandas now supports three types of multi-axis indexing.
• .loc is primarily label based, but may also be used with a boolean array. .loc will raise KeyError when the items are not found. Allowed inputs are:
– A single label, e.g. 5 or 'a' (Note that 5 is interpreted as a label of the index. This use is not an integer position along the index.).
– A list or array of labels ['a', 'b', 'c'].
– A slice object with labels 'a':'f' (Note that contrary to usual Python slices, both the start and the stop
are included, when present in the index! See Slicing with labels and Endpoints are inclusive.)
– A boolean array (any NA values will be treated as False).
– A callable function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above).
• .iloc is primarily integer position based (from 0 to length-1 of the axis), but may also be used with a boolean array. .iloc will raise IndexError if a requested indexer is out-of-bounds, except slice indexers which allow out-of-bounds indexing. (this conforms with Python/NumPy slice semantics). Allowed inputs are:
– An integer e.g. 5.
– A list or array of integers [4, 3, 0].
– A slice object with ints 1:7.
– A boolean array (any NA values will be treated as False).
– A callable function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above).
• .loc, .iloc, and also [] indexing can accept a callable as indexer. See more at Selection By Callable.
Getting values from an object with multi-axes selection uses the following notation (using .loc as an example, but the following applies to .iloc as well). Any of the axes accessors may be the null slice :. Axes left out of the specification are assumed to be:,e.g.p.loc['a']is equivalent to p.loc['a', :, :].
Object Type         Indexers
Series              s.loc[indexer]
DataFrame           df.loc[row_indexer,column_indexer]

2.5.2 Basics
------------
As mentioned when introducing the data structures in the last section, the primary function of indexing with [] (a.k.a. __getitem__ for those familiar with implementing class behavior in Python) is selecting out lower-dimensional slices. The following table shows return type values when indexing pandas objects with []:
ObjectType             Selection                ReturnValueType
Series                series[label]             scalar value
DataFrame             frame[colname]            Series corresponding to colname

Here we construct a simple time series data set to use for illustrating the indexing functionality:
dates=pd.date_range('1/2/2021', periods=8)
df = pd.DataFrame(np.random.randn(8, 4), columns=list("ABCD"),
                 index=dates)
df

Note: None of the indexing functionality is timeseries specific unless specifically stated.
Thus, as per above, we have the most basic indexing using []:
s = df['A']
s[dates[5]]

You can pass a list of columns to [] to select columns in that order. If a column is not contained in the DataFrame, an exception will be raised. Multiple columns can also be set in this manner:
df[['B', 'A']] = df[['A', 'B']]
df
You may find this useful for applying a transform (in-place) to a subset of the columns.
Warning: pandas aligns all AXES when setting Series and DataFrame from .loc, and .iloc. This will not modify df because the column alignment is before value assignment.
df.loc[:, ['B', 'A']] = df[['A', 'B']] ## This is not the correct way, no affect on the output
df[['A', 'B']]

The correct way to swap column values is by using raw values:
df.loc[:, ['B', 'A']]= df[['A', 'B']].to_numpy()

2.5.3 Attribute access
-----------------------
You may access an index on a Series or column on a DataFrame directly as an attribute:
sa = pd.Series([1, 2, 3], index=list('abc'))
sa.b
s['2021-01-07']
dfa = df.copy()
dfa.A

print(sa)
sa.a = 5
sa

dfa.A = list(range(len(dfa.index))) # # ok if A already exists
dfa

dfa.E = list(range(len(dfa.index))) ## Error - UserWarning: Pandas doesn't allow columns to be created via a new attribute name
dfa

# use below form to create a new ˓→column
dfa['E'] = list(range(len(dfa.index)))
dfa

Warning:
• You can use this access only if the index element is a valid Python identifier, e.g. s.1 is not allowed.
• The attribute will not be available if it conflicts with an existing method name, e.g. s.min is not allowed, but s['min'] is possible.
• Similarly, the attribute will not be available if it conflicts with any of the following list: index, major_axis, minor_axis, items.
• In any of these cases, standard indexing will still work, e.g. s['1'], s['min'], and s['index'] will access the corresponding element or column.

If you are using the IPython environment, you may also use tab-completion to see these accessible attributes.
You can also assign a dict to a row of a DataFrame:
x = pd.DataFrame({'x': [1, 2, 3], 'y': [3, 4, 5]})
x.iloc[1] = {'x':9, 'y':99}
x

You can use attribute access to modify an existing element of a Series or column of a DataFrame, but be careful; if you try to use attribute access to create a new column, it creates a new attribute rather than a new column.
df = pd.DataFrame({'one': [1., 2., 3.]})
df.two = [4, 5, 6] ## Error - UserWarning: Pandas doesn't allow columns to be created via a new attribute name
df

2.5.4 Slicing ranges
----------------------
The most robust and consistent way of slicing ranges along arbitrary axes is described in the Selection by Position section detailing the .iloc method. For now, we explain the semantics of slicing using the [] operator.
With Series, the syntax works exactly as with an ndarray, returning a slice of the values and the corresponding labels:
s[:5]
s[::2]
s[::-1]

#Note that setting works as well:
s2 = s.copy()
s2[:5] = 0
s2

With DataFrame, slicing inside of [] slices the rows. This is provided largely as a convenience since it is such a common operation.
df[:3]
df[::-1]

2.5.5 Selection by label
------------------------
 Warning: Whether a copy or a reference is returned for a setting operation, may depend on the context. This is sometimes called chained assignment and should be avoided.
 Warning: .loc is strict when you present slicers that are not compatible (or convertible) with the index type. For example using integers in a DatetimeIndex. These will raise a TypeError.
df1 = pd.DataFrame(np.random.randn(5, 4), columns=list('ABCD'), index=pd.date_range('20130101', periods=5))
#df1.loc[2:4] #TypeError: cannot do slice indexing on DatetimeIndex with these indexers [2] of type int
df1.iloc[2:4]
String likes in slicing can be convertible to the type of the index and lead to natural slicing.
df1.loc['2013-01-03': '2013-01-04']
df1.loc[['2013-01-03', '2013-01-04']]

pandas provides a suite of methods in order to have purely label based indexing. This is a strict inclusion based protocol. Every label asked for must be in the index, or a KeyError will be raised. When slicing, both the start bound AND the stop bound are included, if present in the index. Integers are valid labels, but they refer to the label and not the position.
The .loc attribute is the primary access method. The following are valid inputs:
• A single label, e.g. 5 or 'a' (Note that 5 is interpreted as a label of the index. This use is not an integer
position along the index.).
• A list or array of labels ['a', 'b', 'c'].
• A slice object with labels 'a':'f' (Note that contrary to usual Python slices, both the start and the stop are included, when present in the index! See Slicing with labels.
• A boolean array.
• A callable,
s1 = pd.Series(np.random.randn(6), index=list('abcdef'))
s1.loc['c':]
s1.loc['b']
s1.loc['c':] = 0
s1

With a DataFrame:
df1 = pd.DataFrame(np.random.randn(6, 4), index=list('abcdef'), columns=list('ABCD'))
df1.loc[['a', 'b', 'd'], :]
Accessing via label slices:
df1.loc['d':, 'A':'C']

For getting a cross section using a label (equivalent to df.xs('a')):
df1.loc['a']

For getting values with a boolean array:
df1.loc['a'] > 0

df1.loc[:, df1.loc['a'] > 0]

NA values in a boolean array propagate as False:
mask = pd.array([True, False, True, False, pd.NA, False], dtype="boolean")
mask
df1[mask]

For getting a value explicitly:
# this is also equivalent to ``df1.at['a','A']`
df1.loc['a', 'A']

Slicing with labels
--------------------
When using .loc with slices, if both the start and the stop labels are present in the index, then elements located between the two (including them) are returned:
s = pd.Series(list('abcde'), index=[0, 3, 2, 5, 4])
s.loc[3:5]

If at least one of the two is absent, but the index is sorted, and can be compared against start and stop labels, then slicing will still work as expected, by selecting labels which rank between the two:
s.sort_index()
s.sort_index().loc[1:6]

However, if at least one of the two is absent and the index is not sorted, an error will be raised (since doing otherwise would be computationally expensive, as well as potentially ambiguous for mixed type indexes). For instance, in the above example, s.loc[1:6] would raise KeyError.
s = pd.Series(list('abcdef'), index=[0, 3, 2, 5, 4, 2])
s.loc[3:5]
Also, if the index has duplicate labels and either the start or the stop label is dupulicated, an error will be raised. For instance, in the above example, s.loc[2:5] would raise a KeyError.

2.5.6 Selection by position
----------------------------
 Warning: Whether a copy or a reference is returned for a setting operation, may depend on the context. This is sometimes called chained assignment and should be avoided.
pandas provides a suite of methods in order to get purely integer based indexing. The semantics follow closely Python and NumPy slicing. These are 0-based indexing. When slicing, the start bound is included, while the upper bound is excluded. Trying to use a non-integer, even a valid label will raise an IndexError.
The .iloc attribute is the primary access method. The following are valid inputs: • An integer e.g. 5.
• A list or array of integers [4, 3, 0].
• A slice object with ints 1:7.
• A boolean array.
• A callable,

# this is also equivalent to ``df1.iat[1,1]``
df1.iloc[1, 1]
For getting a cross section using an integer position (equiv to df.xs(1)):
df1.iloc[1]

Out of range slice indexes are handled gracefully just as in Python/NumPy.
s.iloc[4:10]
Note that using slices that go out of bounds can result in an empty axis (e.g. an empty DataFrame being returned).

A single indexer that is out of bounds will raise an IndexError. A list of indexers where any element is out of bounds will raise an IndexError.
dfl.iloc[[4, 5, 6]] # IndexError: positional indexers are out-of-bounds
dfl.iloc[:, 4] # IndexError: positional indexers are out-of-bounds

2.5.7 Selection by callable
----------------------------
.loc, .iloc, and also [] indexing can accept a callable as indexer. The callable must be a function with one argument (the calling Series or DataFrame) that returns valid output for indexing.
df1.loc[lambda df: df['A'] > 0, :]
df1.loc[:, lambda df: ['A', 'B']]
df1.iloc[:, lambda df: [0, 1]]
df1[lambda df: df.columns[0]]

You can use callable indexing in Series.
df1['A'].loc[lambda s: s > 0]

Using these methods / indexers, you can chain data selection operations without using a temporary variable.
bb = pd.read_csv('data/baseball.csv', index_col='id')
bb.groupby(['year', 'team']).sum().loc[lambda df: df['r'] > 100]

2.5.8 Combining positional and label-based indexing
----------------------------------------------------
If you wish to get the 0th and the 2nd elements from the index in the ‘A’ column, you can do:
dfd = pd.DataFrame({'A': [1, 2, 3],
                    'B': [4, 5, 6]},
                   index=list('abc'))
dfd.A[['a', 'c']]
dfd.loc[['a','c'],'A']
dfd.loc[dfd.index[[0,2]], 'A']

This can also be expressed using .iloc, by explicitly getting locations on the indexers, and using positional indexing to select things.
dfd.iloc[[0, 2], dfd.columns.get_loc('A')]

For getting multiple indexers, using .get_indexer:
dfd.iloc[[0, 2], dfd.columns.get_indexer(['A', 'B'])]

Reindexing
----------
The idiomatic way to achieve selecting potentially not-found elements is via .reindex().
s = pd.Series([1, 2, 3])
s.reindex([1,2,3])

Alternatively, if you want to select only valid keys, the following is idiomatic and efficient; it is guaranteed to preserve the dtype of the selection.
s=pd.Series([ 1, 2, 3, ])
labels = [ 1, 2, 3, ]
s.loc[s.index.intersection(labels)]

Having a duplicated index will raise for a .reindex():
s = pd.Series(np.arange(4), index=['a', 'a', 'b', 'c'])
labels = ['c', 'd']
s.reindex(labels)
ValueError: cannot reindex from a duplicate axis

Generally, you can intersect the desired labels with the current axis, and then reindex.
s.loc[s.index.intersection(labels)].reindex(labels)

However, this would still raise if your resulting index is duplicated.
labels = ['a', 'd']
s.loc[s.index.intersection(labels)].reindex(labels)
ValueError: cannot reindex from a duplicate axis

2.5.10 Selecting random samples
--------------------------------
A random selection of rows or columns from a Series or DataFrame with the sample() method. The method will sample rows by default, and accepts a specific number of rows/columns to return, or a fraction of rows.
s = pd.Series([0, 1, 2, 3, 4, 5])

# When no arguments are passed, returns 1 row.
s.sample()

# One may specify either a number of rows:
s.sample(n=3)
s.sample(2)

# Or a fraction of the rows: below means return half of the rows from the series
s.sample(frac=0.5)

By default, sample will return each row at most once, but one can also sample with replacement using the replace option:
s = pd.Series([0, 1, 2, 3, 4, 5])
# Without replacement (default):
s.sample(6, replace=False)
# With replacement: i.e get duplicate rows
s.sample(n=6, replace=True)

By default, each row has an equal probability of being selected, but if you want rows to have different probabilities, you can pass the sample function sampling weights as weights. These weights can be a list, a NumPy array, or a Series, but they must be of the same length as the object you are sampling. Missing values will be treated as a weight of zero, and inf values are not allowed. If weights do not sum to 1, they will be re-normalized by dividing all weights by the sum of the weights. For example:
s = pd.Series([0, 1, 2, 3, 4, 5])
example_weights = [0, 0, 0.2, 0.2, 0.2, 0.4]
s.sample(n=3, weights=example_weights)

# Weights will be re-normalized automatically
example_weights2 = [0.5, 0, 0, 0, 0, 0]
s.sample(n=1, weights=example_weights2)

When applied to a DataFrame, you can use a column of the DataFrame as sampling weights (provided you are sampling rows and not columns) by simply passing the name of the column as a string.
df2 = pd.DataFrame({'col1': [9, 8, 7, 6], 'weight_column': [0.4, 0.3, 0.2, 0.1]})
df2.sample(3, weights='weight_column')

sample also allows users to sample columns instead of rows using the axis argument.
df3 = pd.DataFrame({'col1': [1, 2, 3], 'col2': [2, 3, 4]})
df3.sample(1, axis=1)

Finally, one can also set a seed for sample’s random number generator using the random_state argument, which will accept either an integer (as a seed) or a NumPy RandomState object.
df4 = pd.DataFrame({'col1': [1, 2, 3], 'col2': [2, 3, 4]})
# With a given seed, the sample will always draw the same rows.
df4.sample(n=2, random_state=2)

2.5.11 Setting with enlargement
--------------------------------
The .loc/[] operations can perform enlargement(changing the dtype to bigger dtype) when setting a non-existent key for that axis. In the Series case this is effectively an appending operation.
se = pd.Series([1, 2, 3])
se[5] = 5. # converted the series from int to float

A DataFrame can be enlarged on either axis via .loc.
dfi = pd.DataFrame(np.arange(6).reshape(3, 2), columns=['A', 'B'])
dfi.loc[:, 'C'] = dfi.loc[:, 'A'] ## This is not performant way
dfi['C'] = dfi['A'] ## use this instead
dfi.loc[3] = 5
dfi

2.5.12 Fast scalar value getting and setting
---------------------------------------------
Since indexing with [] must handle a lot of cases (single-label access, slicing, boolean indexing, etc.), it has a bit of overhead in order to figure out what you’re asking for. If you only want to access a scalar value, the fastest way is to use the at and iat methods, which are implemented on all of the data structures.
Similarly to loc, at provides label based scalar lookups, while, iat provides integer based lookups analogously to iloc
s.iat[5]
df.at[dates[5], 'A']
df.iat[3, 0]
You can also set using these same indexers.
df.at[dates[5], 'E'] = 7
df.iat[3, 0] = 7
at may enlarge the object in-place as above if the indexer is missing.
df.at[dates[-1] + pd.Timedelta('1 day'), 0] = 7

2.5.13 Boolean indexing
-----------------------
Another common operation is the use of boolean vectors to filter the data. The operators are: | for or, & for and, and ~ for not. These must be grouped by using parentheses, since by default Python will evaluate an expression such as df['A'] > 2 & df['B'] < 3 as df['A'] > (2 & df['B']) < 3, while the desired evaluation order is (df['A'] > 2) & (df['B'] < 3).
Using a boolean vector to index a Series works exactly as in a NumPy ndarray:
s = pd.Series(range(-3, 4))
s[s > 0]
s[(s < -1) | (s > 0.5)]
s[~(s < 0)]

You may select rows from a DataFrame using a boolean vector the same length as the DataFrame’s index (for example, something derived from one of the columns of the DataFrame):
df[df['A'] > 0]

List comprehensions and the map method of Series can also be used to produce more complex criteria:
df2 = pd.DataFrame({'a': ['one', 'one', 'two', 'three', 'two', 'one', 'six'],
                    'b': ['x', 'y', 'y', 'x', 'y', 'x', 'x'],
                    'c': np.random.randn(7)})
df2
# only want 'two' or 'three'
criterion = df2['a'].map(lambda x: x.startswith('t'))
df2[criterion]
# equivalent but slower
criterion = [x.startswith('t') for x in df2.a]
df2[criterion]
# Multiple criteria
df2[criterion & (df2['b'] == 'x')]

With the choice methods Selection by Label, Selection by Position, and Advanced Indexing you may select along more than one axis using boolean vectors combined with other indexing expressions.
df2.loc[criterion & (df2['b'] == 'x'), 'b':'c']

Warning: iloc supports two kinds of boolean indexing. If the indexer is a boolean Series, an error will be raised. For instance, in the following example, df.iloc[s.values, 1] is ok. The boolean indexer is an array.But df.iloc[s, 1] would raise ValueError.

df = pd.DataFrame([[1, 2], [3, 4], [5, 6]],
                  index=list('abc'),
                  columns=['A', 'B'])
df
s = (df['A'] > 2)
df.loc[s, 'B']
#df.iloc[s,1] # ValueError: Location based indexing can only have
df.iloc[s.values, 1] # works perfectly
# s returns a series but s.values returns a numpyarray
# Therefore iloc fails with s and works with s.values

2.5.14 Indexing with isin
--------------------------
Consider the isin() method of Series, which returns a boolean vector that is true wherever the Series elements exist in the passed list. This allows you to select rows where one or more columns have values you want:
s = pd.Series(np.arange(10,51,10), index=np.arange(5), dtype='int64')
s.isin([10, 30])
s[s.isin([10, 30])]

The same method is available for Index objects and is useful for the cases when you don’t know which of the sought labels are in fact present:
s.index.isin([2, 4])
s[s.index.isin([2, 4, 6])]

# compare it to the following
s.reindex([2, 4, 6]) ## reindex returns nan if the requested index is not present

In addition to that, MultiIndex allows selecting a separate level to use in the membership check:
s_mi = pd.Series(np.arange(6),
        index=pd.MultiIndex.from_product([[0, 1], ['a', 'b', 'c']]))
s_mi.index.isin([(1, 'a'), (2, 'b'), (0, 'c')])
s_mi[s_mi.index.isin([(1, 'a'), (2, 'b'), (0, 'c')])]

s_mi.iloc[s_mi.index.isin(['a', 'c', 'e'], level=1)]

DataFrame also has an isin() method. When calling isin, pass a set of values as either an array or dict. If values is an array, isin returns a DataFrame of booleans that is the same shape as the original DataFrame, with True wherever the element is in the sequence of values.
df = pd.DataFrame({'vals': [1, 2, 3, 4], 'ids': ['a', 'b', 'f', 'n'],
                   'ids2': ['a', 'n', 'c', 'n']})
values = [ 'a', 'b', 1, 2, ]
df[df.isin(values)]

Often times you’ll want to match certain values with certain columns. Just make values a dict where the key is the column, and the value is a list of items you want to check for.
df = pd.DataFrame({'vals': [1, 2, 3, 4], 'ids': ['a', 'b', 'f', 'n'],
                   'ids2': ['a', 'n', 'c', 'n']})
values = {'ids': ['a', 'b'], 'vals': [1, 3]}
df[df.isin(values)]

Combine DataFrame’s isin with the any() and all() methods to quickly select subsets of your data that meet a given criteria. To select a row where each column meets its own criterion:
df = pd.DataFrame({'vals': [1, 2, 3, 4], 'ids': ['a', 'b', 'f', 'n'],
                   'ids2': ['a', 'n', 'c', 'n']})
values = {'ids': ['a', 'b'], 'ids2': ['a', 'c'], 'vals': [1, 3]}
mask = df.isin(values).all(axis=1)
df[mask]

2.5.15 The where() Method and Masking
--------------------------------------
Selecting values from a Series with a boolean vector generally returns a subset of the data. To guarantee that selection
output has the same shape as the original data, you can use the where method in Series and DataFrame. To return only the selected rows:
s = pd.Series(range(-3, 4))
s[s>0]
To return a Series of the same shape as the original:
s.where(s>0)

Selecting values from a DataFrame with a boolean criterion now also preserves input data shape. where is used under the hood as the implementation.The code below is equivalent to df.where(df < 0).
dates=pd.date_range('1/2/2021', periods=8)
df = pd.DataFrame(np.random.randn(8, 4), columns=list("ABCD"),
                 index=dates)
df[df<0]

In addition, where takes an optional other argument for replacement of values where the condition is False, in the returned copy.
df.where(df<0, -df)

You may wish to set values based on some boolean criteria. This can be done intuitively like so:
s2 = s.copy()
s2[s2 < 0] = 0
s2

df2= df.copy()
df2[df2<0] = 0
df2

By default, where returns a modified copy of the data. There is an optional parameter inplace so that the original data can be modified without creating a copy:
df_orig = df.copy()
df_orig.where(df_orig>0, 0., inplace=True)
df_orig

Note: The signature for DataFrame.where() differs from numpy.where(). Roughly df1.where(m, df2)isequivalenttonp.where(m, df1, df2)
df.where(df < 0, -df) == np.where(df < 0, df, -df)

which is faster?
np.where is much faster then df.where.
%timeit np.where(df < 0, df, -df) # 276 µs ± 27.7 µs per loop
%timeit df.where(df < 0, -df) # 1.22 ms ± 113 µs per loop

Alignment
Furthermore, where aligns the input boolean condition (ndarray or DataFrame), such that partial selection with setting is possible. This is analogous to partial setting via .loc (but on the contents rather than the axis labels).
df2 = df.copy()
print(df2)
df2[df2[1:4] > 0] = 3
df2

Where can also accept axis and level parameters to align the input when performing the where.
df2 = DataFrame({
    'a':np.arange(-3,3),
    'b':np.arange(-2,4),
    'c':np.arange(-1,5)
})
print(df2)
df2.where(df2>0, df2['a'], axis='index')

another example:
df2 = DataFrame({
    'a':np.arange(-3,3),
    'b':np.arange(-2,4),
    'c':np.arange(-1,5)
})
print(df2)
print(df2.where(df2>0, df2['a'], axis=0))

This is equivalent to (but faster than) the following.
df2.apply(lambda x,y: x.where(x>0,y), y=df2['a'])

how fast is df.where then apply?
%timeit df2.where(df2>0, df2['a'], axis=0) # 939 µs ± 31.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
%timeit df2.apply(lambda x,y: x.where(x>0,y), y=df2['a']) # 2.99 ms ± 147 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

np.where is faster than both:
%timeit np.where(df2>0, df2, df2[list(len(df2.columns)*'a')]) # 809 µs ± 29.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

where can accept a callable as condition and other arguments. The function must be with one argument (the calling Series or DataFrame) and that returns valid output as condition and other argument.
df3 = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [7, 8, 9]})
df3.where(lambda x:x>4, lambda x:x+10)
df3.where(df3>4, lambda x:x+10)

Mask
-----
mask() is the inverse boolean operation of where.
print(s.where(s>0))
s.mask(s>0)
df3.mask(df3>4, lambda x:x+10)

2.5.16 Setting with enlargement conditionally using numpy()
------------------------------------------------------------
An alternative to where() is to use numpy.where(). Combined with setting a new column, you can use it to
enlarge a dataframe where the values are determined conditionally.
Consider you have two choices to choose from in the following dataframe. And you want to set a new column color to ‘green’ when the second column has ‘Z’. You can do the following:
df = pd.DataFrame({'col1': list('ABBC'), 'col2': list('ZZXY')})
df['color'] = np.where(df['col2']=='Z', 'green', 'red')
df

If you have multiple conditions, you can use numpy.select() to achieve that. Say corresponding to three conditions there are three choice of colors, with a fourth color as a fallback, you can do the following.
conditions = [
    (df['col1']=='A') & (df['col2']=='Z'),
    (df['col1']=='B') & (df['col2']=='Z'),
    (df['col1']=='B')
]
choices = ['yellow', 'blue', 'blue']
df['color'] = np.select(conditions, choices, default='black')
df

2.5.17 The query() Method
--------------------------
DataFrame objects have a query() method that allows selection using an expression.
You can get the value of the frame where column b has values between the values of columns a and c. For example:
df = DataFrame({
    'a': np.random.choice(10, 5),
    'b': np.random.choice(10, 5),
    'c': np.random.choice(10, 5),
})
df
# pure python
df[(df['a'] < df['b']) & (df['b'] < df['c'])]

# query
df.query('(a<b) & (b<c)')

which one is faster?
pure python is much faster than query
%timeit df[(df['a'] < df['b']) & (df['b'] < df['c'])] # 679 µs ± 22.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
%timeit df.query('(a<b) & (b<c)') # 1.89 ms ± 63.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

Do the same thing but fall back on a named index if there is no column with the name a.
df.set_index('a', inplace=True)
df[(df['a'] < df['b']) & (df['b'] < df['c'])]
df.query('a<b & b<c')

If instead you don’t want to or cannot name your index, you can use the name index in your query expression:
df.query('index<b<c')

Note: If the name of your index overlaps with a column name,the column name is given precedence. For example,
df = df.reset_index()
df.index.name='a'
df.query('a<b<c')

You can still use the index in a query expression by using the special identifier ‘index’:
df.query('index > 2')
If for some reason you have a column named index, then you can refer to the index as ilevel_0 as well, but at this point you should consider renaming your columns to something less ambiguous.

MultiIndex query() Syntax
You can also use the levels of a DataFrame with a MultiIndex as if they were columns in the frame:
n=10
colors = np.random.choice(['green', 'red'], size=n)
foods = np.random.choice(['eggs', 'ham'], size=n)
index = pd.MultiIndex.from_arrays([colors, foods], names=['color', 'food'])
df = pd.DataFrame(np.random.choice(n, 20).reshape(10,2), index=index)
df.query('color=="red"')

If the levels of the MultiIndex are unnamed, you can refer to them using special names:
df.index.names = [None, None]
df.query('ilevel_0 == "red"')
The convention is ilevel_0, which means “index level 0” for the 0th level of the index.

query() Use Cases
-----------------
A use case for query() is when you have a collection of DataFrame objects that have a subset of column names (or index levels/names) in common. You can pass the same query to both frames without having to specify which frame you’re interested in querying
df1 = DataFrame({
    'a':np.random.choice(10, 5),
    'b':np.random.choice(10, 5),
    'c':np.random.choice(10, 5),
})

df2 = DataFrame({
    'a':np.random.choice(np.arange(5,15,3), 8),
    'b':np.random.choice(np.arange(5,13,2), 8),
    'c':np.random.choice(np.arange(3,11,3), 8),
})
expr = '4<a<c<9'
map(lambda frame:frame.query(expr), [df1, df2])

query() Python versus pandas Syntax Comparison
-----------------------------------------------
Full numpy-like syntax:
df = pd.DataFrame(np.random.randint(n, size=(n, 3)), columns=list('abc'))
df.query('(a < b) & (b < c)')
df[(df['a'] < df['b']) & (df['b'] < df['c'])]

Slightly nicer by removing the parentheses (by binding making comparison operators bind tighter than & and |).
df.query('a < b & b < c')
Use English instead of symbols:
df.query('a < b and b < c')
Pretty close to how you might write it on paper:
df.query('a < b < c')

The in and not in operators
---------------------------
query() also supports special use of Python’s in and not in comparison operators, providing a succinct syntax
for calling the isin method of a Series or DataFrame.
# get all rows where columns "a" and "b" have overlapping values
df = pd.DataFrame({'y': list('aabbccddeeff'), 'z': list('aaaabbbbcccc'),
                   'c': np.random.randint(5, size=12),
                   'd': np.random.randint(9, size=12)})
df.query('y in z')
# How you'd do it in pure Python
df[df['y'].isin(df['z'])]
df.query('y not in z')
# pure Python
df[~df['y'].isin(df['z'])]

You can combine this with other expressions for very succinct queries:
# rows where cols a and b have overlapping values # and col c's values are less than col d's
df.query('y in z and c > d')
#pure python
df[(df.y.isin(df.z)) & (df.c > df.d)]

Note: Note that in and not in are evaluated in Python, since numexpr has no equivalent of this operation. However,only the in/not in expression itself is evaluated in vanilla Python.For example, in the expression
df.query('a in b + c + d')
(b + c + d) is evaluated by numexpr and then the in operation is evaluated in plain Python. In general, any
operations that can be evaluated using numexpr will be.

Special use of the == operator with list objects
-------------------------------------------------
Comparing a list of values to a column using ==/!= works similarly to in/not in.
df.query('z == ["y", "z", "c"]')
it is equivalent to  - df.query('z in ["y", "z", "c"]')

which one is faster in or ==?
both are almost same

df.query('z == ["y", "z", "c"]')
#pure python
df[df['z'].isin(['y','z','c'])]

df.query('c == [1, 2]')
df.query('c != [1, 2]')

df.query('[1, 2] in c')
df.query('[1, 2] not in c')
# pure Python
df[df['c'].isin([1, 2])]

Boolean operators
------------------
You can negate boolean expressions with the word not or the ~ operator.
df = pd.DataFrame(np.random.rand(n, 3), columns=list('abc'))
df['bools'] = np.random.rand(len(df)) > 0.5
df.query('~bools')
df.query('not bools')
df.query('not bools') == df[~df['bools']]

Of course, expressions can be arbitrarily complex too:
# short query syntax
shorter = df.query('a < b < c and (not bools) or bools > 2')
# equivalent in pure Python
longer = df[(df['a'] < df['b']) & (df['b'] < df['c']) & (~df['bools']) | (df['bools'] > 2)]

Performance of query()
DataFrame.query() using numexpr is slightly faster than Python for large frames.
Note: You will only see the performance benefits of using the numexpr engine with DataFrame.query() if your frame has more than approximately 200,000 rows.




Attributes:
.name
.array - get the series or index in the form of Extension array
.cat.categories
df.T
.freqstr
.freq
.dt.month, .dt.year. .dt.hour, .dt.day, .dt.weekday
.index
columns
shape
dtypes - get the datatypes of a df
dtype - get the datatype of a series
loc[] - get values based on label
iloc[] - get values based on index
at[] - whenever you need a scalar value go with at or iat. use at in case of labels. It is faster.
iat[] - get the scalar values based on index

Methods:
np.select(condition, choices, default=) - can be used for more than one if condition
np.where(condition,true,false) - can be used for if else conditions in the dataframe
df.mask() - exactly opposite of df.where
np.where(condition, df, replacement) - apply condition on a df and replace it with values in replacement, faster than df.where
df.where(condition, replacement, axis) - apply condition on a df and replace it with values in replacement
sample(n, frac, weights, random_state) - get sample from a dataframe
df.columns.get_indexers(['A', 'B']) - get multiple columns from a dataframe
df.columns.get_loc('A') - get columns of a dataframe
select_dtypes(include=[], exclude=[]) - select columns of particular datatype from a df
to_datetime(), to_timedelta() - similar to to_numeric() but for datetime and timedelta type.
to_numeric(errors='raise|coerce', downcast='integer|float') - convert a series, array to numeric value of certain type.
astype() - convert a series, df column to other datatype
copy() - copy() the dataframe and series, it is used for deepcopy.
nsmallest(), nlargest() - get n smallest or n largest numbers from a series or df.
searchsorted() - in a sorted array tell the index where the value can be placed to maintain the sorting order. It is similar to np.searchsorted().
sort_values() - sort values by columns, custom functions can be applied with key argument.
sort_index() - sort index, row wise or column wise or both.
itertuples() - iterates over the rows, return namedTuple of index, colValues of a df
iterrows() - iterates over the rows, return index, colValues of a df
items() - similar to dict, return index, colValues of a df
rename_axis() - rename the name of axis labels i.e index and column for multiindex df.
rename() - rename the name of axis labels i.e index and column.
align() - to get the values of both df, series in the form of tuple by join across various options.
reindex_like() - change the shape of a df, series similar to another df, series
reindex() - reindex the index or columns, it is faster than normal rearrange of columns.
applymap(), map() - since all functions are not vectorized, these functions are applied on elements. applymap on dict and map on series.
.transform() - aggregating the data, similar to agg but the index remains same
.agg(), .aggregate() -- aggregate the series or dataframe based on the function provided
.apply() -- apply the function row-wise or column-wise.
.pipe() -- apply the function on a dataframe by passing the dataframe as an arg to the function.
cut(), qcut() - discretize and quantile continuous data
mode() - get the most occurring value in a series or df
duplicated() - get the duplicated rows in a df
value_counts() - count the occurrence of an element in a series or df
idxmin(), idxmax() - get the index of the min and max value in the series or dataframe
describe([percentiles=[], include=[]) - summary desc stats, you can modify the percentiles as per your needs.
nunique - provide the count of the unique elements in a series/df excluding np.nan
combine - combine two dataframes based on a provided function
combine_first() - get the non- NAN values from both df while merging
gt(),lt(),ne() - greater than, less than, not equal
divmod() - provide the quotient and remainder in the form of a tuple
equals() - compare NDFrames at once, == will return boolean values per element
fill_value - fill the nan with a value within a method
.fillna() - fill the nan with a value
pd.set_option() - console width, height. Not use any dependent lib.
np.asarray(s,df) - convert a dataframe or series to numpy
to_numpy() - convert a dataframe or series to numpy
query() - query a dataframe, condition/filter can be applied on df
assign() - assign a column to a df using calculation or lambda
insert(position_index, column_name, value) - insert a column at a particular position
rename() - rename a series
.cat.set_categories()
period.to_timestamp - get the first day of the month
timestamp.to_period - convert date to the freq like in Months
tz_convert - convert timezone to anotehr timezone
tz_localize - specify timezone of a particular country
pd.MultiIndex.from_tuples() 
stack()
df.rsub - opposite of sub(), first apply sub and then multiply values with -1
df.sub() - subtract
isna()
fillna()
dropna()
reindex()
date_range()
idxmax() - get the max index
replace({from:to})
read_csv(parse_dates=['colName'])
to_datetime()
merge(,,how, left_on, right_on)
concat(, axis=, keys=)
melt(id_var=, value_var=, value_name=, var_name=)
set_index()
reset_index()
pivot_table(colums=, values=, index=, aggfunc=, margins=)
pivot(columns=, values=)
size()
count()
sort_index()
sort_values()
value_counts()
mean(), median(), mode(), skew(), min(), max()
groupby().aggregateFunction()
agg({col=[fn1, fn2]}) - aggreagate the data based on columns of a dataframe
rename(columns={'currentCol':'newCol'})
fig.savefig()
plt.subplot()
plot()
notna()
isin()
max()
head()
tail()
describe()
info()

Important points while reading csv in pandas:
----------------------------------------------
source - https://towardsdatascience.com/all-the-pandas-read-csv-you-should-know-to-speed-up-your-data-analysis-1e16fe1039f3
Pandas read_csv() tricks you should know to speed up your data analysis
Some of the most helpful Pandas tricks to speed up your data analysis

Importing data is the first step in any data science project. Often, you’ll work with data in CSV files and run into problems at the very beginning. In this article, you’ll see how to use the Pandas read_csv() function to deal with the following common problems.
Dealing with different character encodings
Dealing with headers
Dealing with columns
Parsing date columns
Setting data type for columns
Finding and locating invalid value
Appending data to an existing CSV file
Loading a huge CSV file with chunksize
Please check out my Github repo for the source code.
1. Dealing with different character encodings
Character encodings are specific sets of rules for mapping from raw binary byte strings to characters that make up the human-readable text [1]. Python has built-in support for a list of standard encodings.
Character encoding mismatches are less common today as UTF-8 is the standard text encoding in most of the programming languages including Python. However, it is definitely still a problem if you are trying to read a file with a different encoding than the one it was originally written. You are most likely to end up with something like below or DecodeError when that happens:

Source from Kaggle character encoding
The Pandas read_csv() function has an argument call encoding that allows you to specify an encoding to use when reading a file.
Let’s take a look at an example below:
First, we create a DataFrame with some Chinese characters and save it with encoding='gb2312' .
df = pd.DataFrame({'name': '一 二 三 四'.split(), 'n': [2, 0, 2, 3]})
df.to_csv('data/data_1.csv', encoding='gb2312', index=False)
Then, you should get an UnicodeDecodeError when trying to read the file with the default utf8 encoding.
# Read it with default encoding='utf8'
# You should get an error
pd.read_csv('data/data_1.csv')

In order to read it correctly, you should pass the encoding that the file was written.
pd.read_csv('data/data_1.csv', encoding='gb2312')

2. Dealing with headers
Headers refer to the column names. For some datasets, the headers may be completely missing, or you might want to consider a different row as headers. The read_csv() function has an argument called header that allows you to specify the headers to use.
No headers
If your CSV file does not have headers, then you need to set the argument header to None and the Pandas will generate some integer values as headers
For example to import data_2_no_headers.csv
pd.read_csv('data/data_2_no_headers.csv', header=None)

Consider different row as headers
Let’s take a look at data_2.csv
x1,       x2,      x3,     x4
product,  price,   cost,   profit
a,        10,      5,      1
b,        20,      12,     2
c,        30,      20,     3
d,        40,      30,     4
It seems like more sensible columns name would be product, price, … profit, but they are not in the first row. The argument header also allows you to specify the row number to use as the column names and the start of data. In this case, we would like to skip the first row and use the 2nd row as headers:
pd.read_csv('data/data_2.csv', header=1)
3. Dealing with columns
When your input dataset contains a large number of columns, and you want to load a subset of those columns into a DataFrame, then usecols will be very useful.
Performance-wise, it is better because instead of loading an entire DataFrame into memory and then deleting the spare columns, we can select the columns we need while loading the dataset.
Let’s use the same dataset data_2.csv and select the product and cost columns.
pd.read_csv('data/data_2.csv',
            header=1,
            usecols=['product', 'cost'])

We can also pass the column index to usecols:
pd.read_csv('data/data_2.csv',
            header=1,
            usecols=[0, 1])
4. Parsing date columns
Date columns are represented as objects by default when loading data from a CSV file.
df = pd.read_csv('data/data_3.csv')
df.info()
RangeIndex: 4 entries, 0 to 3
Data columns (total 5 columns):
 #   Column   Non-Null Count  Dtype
---  ------   --------------  -----
 0   date     4 non-null      object
 1   product  4 non-null      object
 2   price    4 non-null      int64
 3   cost     4 non-null      int64
 4   profit   4 non-null      int64
dtypes: int64(3), object(2)
memory usage: 288.0+ bytes
To read the date column correctly, we can use the argument parse_dates to specify a list of date columns.
df = pd.read_csv('data/data_3.csv', parse_dates=['date'])
df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 4 entries, 0 to 3
Data columns (total 5 columns):
 #   Column   Non-Null Count  Dtype
---  ------   --------------  -----
 0   date     4 non-null      datetime64[ns]
 1   product  4 non-null      object
 2   price    4 non-null      int64
 3   cost     4 non-null      int64
 4   profit   4 non-null      int64
dtypes: datetime64[ns](1), int64(3), object(1)
memory usage: 288.0+ bytes
Sometime date is split up into multiple columns, for example, year, month, and day. To combine them into a datetime, we can pass a nested list to parse_dates.
df = pd.read_csv('data/data_4.csv',
                 parse_dates=[['year', 'month', 'day']])
df.info()
RangeIndex: 4 entries, 0 to 3
Data columns (total 5 columns):
 #   Column          Non-Null Count  Dtype
---  ------          --------------  -----
 0   year_month_day  4 non-null      datetime64[ns]
 1   product         4 non-null      object
 2   price           4 non-null      int64
 3   cost            4 non-null      int64
 4   profit          4 non-null      int64
dtypes: datetime64[ns](1), int64(3), object(1)
memory usage: 288.0+ bytes
To specify a custom column name instead of the auto-generated year_month_day, we can pass a dictionary instead.
df = pd.read_csv('data/data_4.csv',
                 parse_dates={ 'date': ['year', 'month', 'day'] })
df.info()
If your date column is in a different format, then you can customize a date parser and pass it to the argument date_parser:
from datetime import datetime
custom_date_parser = lambda x: datetime.strptime(x, "%Y %m %d %H:%M:%S")
pd.read_csv('data/data_6.csv',
             parse_dates=['date'],
             date_parser=custom_date_parser)
For more about parsing date columns, please check out this article
4 tricks you should know to parse date columns with Pandas read_csv()
Some of the most helpful Pandas tricks


5. Setting data type
If you want to set the data type for the DataFrame columns, you can use the argument dtype , for example
pd.read_csv('data/data_7.csv',
                 dtype={
                     'Name': str,
                     'Grade': int
                 })
6. Finding and locating invalid values
You might get the TypeError when setting data type using the argument dtype

It is always useful to find and locate the invalid values when this error happens. Here is how you can find them:
df = pd.read_csv('data/data_8.csv')
is_error = pd.to_numeric(df['Grade'], errors='coerce').isna()
df[is_error]

7. Appending data to an existing CSV file
You can specify a Python write mode in the Pandas to_csv() function. For appending data to an existing CSV file, we can use mode='a':
new_record = pd.DataFrame([['New name', pd.to_datetime('today')]],
                      columns=['Name', 'Date'])
new_record.to_csv('data/existing_data.csv',
              mode='a',
              header=None,
              index=False)
8. Loading a huge CSV file with chunksize
By default, Pandas read_csv() function will load the entire dataset into memory, and this could be a memory and performance issue when importing a huge CSV file.
read_csv() has an argument called chunksize that allows you to retrieve the data in a same-sized chunk. This is especially useful when reading a huge dataset as part of your data science project.
Let’s take a look at an example below:
First, let’s make a huge dataset with 400,000 rows and save it to big_file.csv
# Make up a huge dataset
nums = 100_000
for name in 'a b c d'.split():
    df = pd.DataFrame({
        'col_1': [1]*nums,
        'col_2': np.random.randint(100, 2000, size=nums)
    })
    df['name'] = name
    df.to_csv('data/big_file.csv',
              mode='a',
              index=False,
              header= name=='a')

Next, let’s specify a chucksize of 50,000 when loading data with read_csv()
dfs = pd.read_csv('data/big_file.csv',
                  chunksize=50_000,
                  dtype={
                      'col_1': int,
                      'col_2': int,
                      'name': str
                  })
Let’s perform some aggregations on each chunk and then concatenate the result into a single DataFrame.
res_dfs = []
for chunk in dfs:
    res = chunk.groupby('name').col_2.agg(['count', 'sum'])
    res_dfs.append(res)
pd.concat(res_dfs).groupby(level=0).sum()

Let’s validate the result against a solution without chunksize
pd.read_csv('data/big_file.csv',
              dtype={
                  'col_1': int,
                  'col_2': int,
                  'name': str
              }).groupby('name').col_2.agg(['count', 'sum'])
And you should get the same output.




-------------__init__------------

What’s __init__ for me?
Designing for Python package import patterns
Jacob Deppen
Jacob Deppen

Jan 8, 2019·9 min read


I have had a few conversations lately about Python packaging, particularly around structuring the import statements to access the various modules of a package. This was something I had to do a lot of investigation of and experimentation with when I was organizing the leiap package. Still, I have not seen a good guide to best practices in various scenarios, so I thought I would share my thoughts here.
Import patterns

The key to designing how the user will interact with the modules is the package’s __init__.py file. This will define what gets brought into the namespace with the import statement.
Modules
It is usually a good idea to split code into smaller modules for a couple of reasons. Primarily, modules can contain all of the code related to a particular coherent topic (e.g., all of the I/O functionality) without being cluttered by code related to something completely different (e.g., plotting). For this reason, it is common to see large classes get a dedicated module (e.g., geodataframe.py within geopandas). Secondarily, dividing code into appropriate logical units makes it easier to read and easier to understand.
However, a good module structure for the developer may or may not be a good module structure for the user. In some cases, the user might not need to know that there are various modules underlying the package. In other cases, there might be good reasons that the user should explicitly ask only for the modules they need. That is what I want to explore here: what are the different use cases and what approach do they call for from the package developer.
An example package
Python packages come in a variety of structures, but let’s create a simple demo one here that we can use in all the examples.
/src
    /example_pkg
        __init__.py
        foo.py
        bar.py
        baz.py
    setup.py
    README.md
    LICENSE
It is composed of three modules: foo.py, bar.py, and baz.py, each of which has a single function that prints the name of the module where the function resides.
foo.py
def foo_func():
    print(‘this is a foo function’)
bar.py
def bar_func():
    print(‘this is a bar function’)
baz.py
def baz_func():
    print(‘this is a baz function’)
Your code as a grocery store
Now is a good time to acknowledge that talking about import statements and package structures can be pretty hard to follow, especially in text. To help make things a bit clearer, let’s think about a Python package as a grocery store and your users as the shoppers. As the developer, you are the store owner and manager. Your job is to figure out how to set up your store so that you serve your customers best. The structure of your __init__.py file will determine that setup. Below, I’ll walk through three alternative ways to set up that file: the general store, the convenience store, and the online store.
The General Store

In this scenario, the user gets access to everything right away on import example_pkg. In their code, they only need to type the package name and the class, function, or other object they want, regardless of what module of the source code it lives in.
This scenario is like an old-timey general store. Once the customer walks in the door, they can see all the goods placed with minimal fuss in bins and shelves around the store.
Behind the scenes
# __init__.py
from .foo import *
from .bar import *
from .baz import *
User implementation
import example_pkg
example_pkg.foo_func()
example_pkg.bar_func()
example_pkg.baz_func()
Advantages
Users do not need to know module names or remember, for instance, which function is in which module. They only need the package name and the function name. In the general store, all the products are on display with minimal signage. The customer doesn’t need to know which aisle to go down.
Users can access any functionality once the top-level package is imported. Everything is on display.
Tab-completion gives you everything with just example_pkg.<TAB>. Tab-completion is like the general store grocer who knows exactly where everything is and is happy to help.
When new features are added to modules, you do not need to update any import statements; they will automatically be included. In the general store, there is no fancy signage to change. Just put a new item on the shelf.
Disadvantages
Requires that all functions and classes must be uniquely named (i.e., there are not functions called save() in both the foo and bar modules). You don’t want to confuse your customers by putting apples in two different bins.
If the package is large, it can add a lot to the namespace and (depending on a lot of factors) can slow things down. A general store can have a lot of little odds and ends that any individual customer might not want. That can might be overwhelming for your customers.
Requires a bit more effort and vigilance to keep some elements away from the user. For example, you might need to use underscores to keep functions from importing (e.g., _function_name()). Most general stores don’t have a big storage area where things like brooms and mops are kept; those items are visible to the customer. Even if it is unlikely that they would pick up a broom and start sweeping your floors, you might not want them to. In that case, you have to take extra steps to hide those supplies from view.
Recommendations
Use when it is hard to predict the workflow of a typical user (e.g., general packages like pandas or numpy). This is the “general” part of general store.
Use when the user might frequently bounce around between different modules (e.g., the leiap package)
Use when function and class names are very descriptive and easy to remember and specifying the module names will not improve readability. If your products are familiar things like fruits and vegetables, you don’t need a lot of signage; customers will figure things out quite easily.
Use with just a few modules. If there are many modules, it can be more difficult for a new user to find the functionality they want in the docs. If your general store gets too big, customers won’t be able to find the things they want.
Use when objects might be added or removed frequently. It’s easy to add and remove products in the general store without disrupting the customer.
Well-known examples
pandas
numpy (with additional complexity)
seaborn
The Convenience Store

Photo by Caio Resende from Pexels
By far the easiest to read and understand is a variation on the general store scenario that I call the convenience store. Instead of from .module import *, you can specify exactly what to import with from .module import func within __init__.py.
The convenience store shares a lot of traits with the general store. It has a relatively limited selection of goods which can be replaced at any time with minimal hassle. The customer doesn’t need a lot of signage to find what they need because most of the goods are easily in view. The biggest difference is that a convenience store has a bit more order. The empty boxes, brooms, and mops are all kept out of view of the customer and only the products for sale are on the shelves.
Behind the scenes
# __init__.py
from .foo import foo_func
from .bar import bar_func
from .baz import baz_func
User implementation
import example_pkg
example_pkg.foo_func()
example_pkg.bar_func()
example_pkg.baz_func()
Advantages
Shares all of the advantages of the general store, and adds:
Somewhat easier to control what objects are made available to the user
Disadvantages
__init__.py can end up very cluttered if there are many modules with many functions. Like the general store, a convenience store that is too cluttered will be harder for customers to navigate.
When new features are added to a module (i.e., new class or functions), they have to be explicitly added to the __init__.py file too. Modern IDEs can help detect missed imports, but it is still easy to forget. Your convenience store has some minimal signage and price tags. You have to remember to update these when you change what is on the shelf.
Recommendations
I would add the following to the recommendations from the general store:
Especially useful when your modules more or less consist of a single Class (e.g., from geopandas.geodataframe import GeoDataFrame)
Use when you have a small number of objects to import
Use when your objects have clear names
Use when you know exactly which objects your users will need and which they will not
Use when you do not expect to frequently add a lot of new modules and objects that will need to be imported.
Well-known example
geopandas
Online grocery shopping

Anyone who has bought groceries online knows that ordering the right product can take some effort on the part of the customer. You have to search for the product, choose a brand, choose the desired size, etc. All of these steps, however, allow you to buy exactly what you want from a nearly limitless stockroom.
In the case of Python packages, in some cases, it might be more prudent to eschew the convenience of simply importing the entire package and instead force the user to be more clear about what pieces are being imported. This allows you as the developer to include a lot more pieces to the package without overwhelming the user.
Behind the scenes
# __init__.py
import example_pkg.foo
import example_pkg.bar
import example_pkg.baz
User implementation
There are (at least) three different methods that a user could adopt in this case.
import example_pkg
example_pkg.foo.foo_func()
example_pkg.bar.bar_func()
example_pkg.bar.baz_func()
or
from example_pkg import foo, bar, baz
foo.foo_func()
bar.bar_func()
baz.baz_func()
or
import example_pkg.foo as ex_foo
import example_pkg.bar as ex_bar
import example_pkg.baz as ex_baz
ex_foo.foo_func()
ex_bar.bar_func()
ex_baz.baz_func()
Advantages
Simplifies the __init__.py file. Only needs to be updated when a new module is added. Updating your online store is relatively painless. You only need to change a setting in your product database.
It is flexible. It can be used to import only what the user needs or to import everything. The customers in your online store can search for only what they want or need. No need to bother looking through a “fruit” bin when all you need is an apple. But if they do want everything in the “fruit” bin, they can get that too.
Aliasing can clean up long package.module specifications (e.g., import matplotlib.pyplot as plt). While online grocery shopping can be a big pain at first, if you save your shopping list for the future, your shopping can be done a lot quicker.
Can have multiple objects with the same name (e.g., functions called save() in both the foo and bar modules)
Disadvantages
Some of the import methods can make code more complicated to read. For example, foo.foo_func() does not indicate which package foo comes from.
The most readable method (import example_pkg, with no alias) can lead to long code chunks (e.g., example_pkg.foo.foo_func()) that clutter things up.
Can be hard for users to track down all of the possible functionality. In your online grocery store, it would be hard for the shopper to see all of the possible goods.
Recommendations
Use when you have a complex series of modules, most of which any one user will never need.
Use when import example_pkg imports a LOT of objects and might be slow.
Use when you can define pretty clear workflows for different kinds of users.
Use when you can expect the user to be able to navigate your documentation well.
Examples
matplotlib *
scikit-learn *
bokeh *
scipy *
* These packages actually use combinations of different approaches in their __init__.py files. I include them here because to users, they are generally used à la carte (e.g., import matplotlib.pyplot as plt or import scipy.stats.kde).
Conclusion
The three scenarios I outlined are certainly not the only possible structures for a Python package, but I hope they cover most of the cases that anyone reading learning this from a blog might be considering. In conclusion, I’ll return to a point I made earlier: a good module structure for the developer may or may not be a good module structure for the user. Whatever your decision, don’t forget to put yourself in the user’s shoes even, or especially, because that user is most likely to be you.
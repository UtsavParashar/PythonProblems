PandasOfficialDoc
-----------------

conda activate myenv
jupyter notebook

READ-
----------
==================================================================================

Important points while reading csv in pandas:
----------------------------------------------
source - https://towardsdatascience.com/all-the-pandas-read-csv-you-should-know-to-speed-up-your-data-analysis-1e16fe1039f3
Pandas read_csv() tricks you should know to speed up your data analysis
Some of the most helpful Pandas tricks to speed up your data analysis

Importing data is the first step in any data science project. Often, you’ll work with data in CSV files and run into problems at the very beginning. In this article, you’ll see how to use the Pandas read_csv() function to deal with the following common problems.
Dealing with different character encodings
Dealing with headers
Dealing with columns
Parsing date columns
Setting data type for columns
Finding and locating invalid value
Appending data to an existing CSV file
Loading a huge CSV file with chunksize
Please check out my Github repo for the source code.
1. Dealing with different character encodings
Character encodings are specific sets of rules for mapping from raw binary byte strings to characters that make up the human-readable text [1]. Python has built-in support for a list of standard encodings.
Character encoding mismatches are less common today as UTF-8 is the standard text encoding in most of the programming languages including Python. However, it is definitely still a problem if you are trying to read a file with a different encoding than the one it was originally written. You are most likely to end up with something like below or DecodeError when that happens:

Source from Kaggle character encoding
The Pandas read_csv() function has an argument call encoding that allows you to specify an encoding to use when reading a file.
Let’s take a look at an example below:
First, we create a DataFrame with some Chinese characters and save it with encoding='gb2312' .
df = pd.DataFrame({'name': '一 二 三 四'.split(), 'n': [2, 0, 2, 3]})
df.to_csv('data/data_1.csv', encoding='gb2312', index=False)
Then, you should get an UnicodeDecodeError when trying to read the file with the default utf8 encoding.
# Read it with default encoding='utf8'
# You should get an error
pd.read_csv('data/data_1.csv')

In order to read it correctly, you should pass the encoding that the file was written.
pd.read_csv('data/data_1.csv', encoding='gb2312')

2. Dealing with headers
Headers refer to the column names. For some datasets, the headers may be completely missing, or you might want to consider a different row as headers. The read_csv() function has an argument called header that allows you to specify the headers to use.
No headers
If your CSV file does not have headers, then you need to set the argument header to None and the Pandas will generate some integer values as headers
For example to import data_2_no_headers.csv
pd.read_csv('data/data_2_no_headers.csv', header=None)

Consider different row as headers
Let’s take a look at data_2.csv
x1,       x2,      x3,     x4
product,  price,   cost,   profit
a,        10,      5,      1
b,        20,      12,     2
c,        30,      20,     3
d,        40,      30,     4
It seems like more sensible columns name would be product, price, … profit, but they are not in the first row. The argument header also allows you to specify the row number to use as the column names and the start of data. In this case, we would like to skip the first row and use the 2nd row as headers:
pd.read_csv('data/data_2.csv', header=1)
3. Dealing with columns
When your input dataset contains a large number of columns, and you want to load a subset of those columns into a DataFrame, then usecols will be very useful.
Performance-wise, it is better because instead of loading an entire DataFrame into memory and then deleting the spare columns, we can select the columns we need while loading the dataset.
Let’s use the same dataset data_2.csv and select the product and cost columns.
pd.read_csv('data/data_2.csv',
            header=1,
            usecols=['product', 'cost'])

We can also pass the column index to usecols:
pd.read_csv('data/data_2.csv',
            header=1,
            usecols=[0, 1])
4. Parsing date columns
Date columns are represented as objects by default when loading data from a CSV file.
df = pd.read_csv('data/data_3.csv')
df.info()
RangeIndex: 4 entries, 0 to 3
Data columns (total 5 columns):
 #   Column   Non-Null Count  Dtype
---  ------   --------------  -----
 0   date     4 non-null      object
 1   product  4 non-null      object
 2   price    4 non-null      int64
 3   cost     4 non-null      int64
 4   profit   4 non-null      int64
dtypes: int64(3), object(2)
memory usage: 288.0+ bytes
To read the date column correctly, we can use the argument parse_dates to specify a list of date columns.
df = pd.read_csv('data/data_3.csv', parse_dates=['date'])
df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 4 entries, 0 to 3
Data columns (total 5 columns):
 #   Column   Non-Null Count  Dtype
---  ------   --------------  -----
 0   date     4 non-null      datetime64[ns]
 1   product  4 non-null      object
 2   price    4 non-null      int64
 3   cost     4 non-null      int64
 4   profit   4 non-null      int64
dtypes: datetime64[ns](1), int64(3), object(1)
memory usage: 288.0+ bytes
Sometime date is split up into multiple columns, for example, year, month, and day. To combine them into a datetime, we can pass a nested list to parse_dates.
df = pd.read_csv('data/data_4.csv',
                 parse_dates=[['year', 'month', 'day']])
df.info()
RangeIndex: 4 entries, 0 to 3
Data columns (total 5 columns):
 #   Column          Non-Null Count  Dtype
---  ------          --------------  -----
 0   year_month_day  4 non-null      datetime64[ns]
 1   product         4 non-null      object
 2   price           4 non-null      int64
 3   cost            4 non-null      int64
 4   profit          4 non-null      int64
dtypes: datetime64[ns](1), int64(3), object(1)
memory usage: 288.0+ bytes
To specify a custom column name instead of the auto-generated year_month_day, we can pass a dictionary instead.
df = pd.read_csv('data/data_4.csv',
                 parse_dates={ 'date': ['year', 'month', 'day'] })
df.info()
If your date column is in a different format, then you can customize a date parser and pass it to the argument date_parser:
from datetime import datetime
custom_date_parser = lambda x: datetime.strptime(x, "%Y %m %d %H:%M:%S")
pd.read_csv('data/data_6.csv',
             parse_dates=['date'],
             date_parser=custom_date_parser)
For more about parsing date columns, please check out this article
4 tricks you should know to parse date columns with Pandas read_csv()
Some of the most helpful Pandas tricks


5. Setting data type
If you want to set the data type for the DataFrame columns, you can use the argument dtype , for example
pd.read_csv('data/data_7.csv',
                 dtype={
                     'Name': str,
                     'Grade': int
                 })
6. Finding and locating invalid values
You might get the TypeError when setting data type using the argument dtype

It is always useful to find and locate the invalid values when this error happens. Here is how you can find them:
df = pd.read_csv('data/data_8.csv')
is_error = pd.to_numeric(df['Grade'], errors='coerce').isna()
df[is_error]

7. Appending data to an existing CSV file
You can specify a Python write mode in the Pandas to_csv() function. For appending data to an existing CSV file, we can use mode='a':
new_record = pd.DataFrame([['New name', pd.to_datetime('today')]],
                      columns=['Name', 'Date'])
new_record.to_csv('data/existing_data.csv',
              mode='a',
              header=None,
              index=False)
8. Loading a huge CSV file with chunksize
By default, Pandas read_csv() function will load the entire dataset into memory, and this could be a memory and performance issue when importing a huge CSV file.
read_csv() has an argument called chunksize that allows you to retrieve the data in a same-sized chunk. This is especially useful when reading a huge dataset as part of your data science project.
Let’s take a look at an example below:
First, let’s make a huge dataset with 400,000 rows and save it to big_file.csv
# Make up a huge dataset
nums = 100_000
for name in 'a b c d'.split():
    df = pd.DataFrame({
        'col_1': [1]*nums,
        'col_2': np.random.randint(100, 2000, size=nums)
    })
    df['name'] = name
    df.to_csv('data/big_file.csv',
              mode='a',
              index=False,
              header= name=='a')

Next, let’s specify a chucksize of 50,000 when loading data with read_csv()
dfs = pd.read_csv('data/big_file.csv',
                  chunksize=50_000,
                  dtype={
                      'col_1': int,
                      'col_2': int,
                      'name': str
                  })
Let’s perform some aggregations on each chunk and then concatenate the result into a single DataFrame.
res_dfs = []
for chunk in dfs:
    res = chunk.groupby('name').col_2.agg(['count', 'sum'])
    res_dfs.append(res)
pd.concat(res_dfs).groupby(level=0).sum()

Let’s validate the result against a solution without chunksize
pd.read_csv('data/big_file.csv',
              dtype={
                  'col_1': int,
                  'col_2': int,
                  'name': str
              }).groupby('name').col_2.agg(['count', 'sum'])
And you should get the same output.




-------------__init__------------

What’s __init__ for me?
Designing for Python package import patterns
Jacob Deppen
Jacob Deppen

Jan 8, 2019·9 min read


I have had a few conversations lately about Python packaging, particularly around structuring the import statements to access the various modules of a package. This was something I had to do a lot of investigation of and experimentation with when I was organizing the leiap package. Still, I have not seen a good guide to best practices in various scenarios, so I thought I would share my thoughts here.
Import patterns

The key to designing how the user will interact with the modules is the package’s __init__.py file. This will define what gets brought into the namespace with the import statement.
Modules
It is usually a good idea to split code into smaller modules for a couple of reasons. Primarily, modules can contain all of the code related to a particular coherent topic (e.g., all of the I/O functionality) without being cluttered by code related to something completely different (e.g., plotting). For this reason, it is common to see large classes get a dedicated module (e.g., geodataframe.py within geopandas). Secondarily, dividing code into appropriate logical units makes it easier to read and easier to understand.
However, a good module structure for the developer may or may not be a good module structure for the user. In some cases, the user might not need to know that there are various modules underlying the package. In other cases, there might be good reasons that the user should explicitly ask only for the modules they need. That is what I want to explore here: what are the different use cases and what approach do they call for from the package developer.
An example package
Python packages come in a variety of structures, but let’s create a simple demo one here that we can use in all the examples.
/src
    /example_pkg
        __init__.py
        foo.py
        bar.py
        baz.py
    setup.py
    README.md
    LICENSE
It is composed of three modules: foo.py, bar.py, and baz.py, each of which has a single function that prints the name of the module where the function resides.
foo.py
def foo_func():
    print(‘this is a foo function’)
bar.py
def bar_func():
    print(‘this is a bar function’)
baz.py
def baz_func():
    print(‘this is a baz function’)
Your code as a grocery store
Now is a good time to acknowledge that talking about import statements and package structures can be pretty hard to follow, especially in text. To help make things a bit clearer, let’s think about a Python package as a grocery store and your users as the shoppers. As the developer, you are the store owner and manager. Your job is to figure out how to set up your store so that you serve your customers best. The structure of your __init__.py file will determine that setup. Below, I’ll walk through three alternative ways to set up that file: the general store, the convenience store, and the online store.
The General Store

In this scenario, the user gets access to everything right away on import example_pkg. In their code, they only need to type the package name and the class, function, or other object they want, regardless of what module of the source code it lives in.
This scenario is like an old-timey general store. Once the customer walks in the door, they can see all the goods placed with minimal fuss in bins and shelves around the store.
Behind the scenes
# __init__.py
from .foo import *
from .bar import *
from .baz import *
User implementation
import example_pkg
example_pkg.foo_func()
example_pkg.bar_func()
example_pkg.baz_func()
Advantages
Users do not need to know module names or remember, for instance, which function is in which module. They only need the package name and the function name. In the general store, all the products are on display with minimal signage. The customer doesn’t need to know which aisle to go down.
Users can access any functionality once the top-level package is imported. Everything is on display.
Tab-completion gives you everything with just example_pkg.<TAB>. Tab-completion is like the general store grocer who knows exactly where everything is and is happy to help.
When new features are added to modules, you do not need to update any import statements; they will automatically be included. In the general store, there is no fancy signage to change. Just put a new item on the shelf.
Disadvantages
Requires that all functions and classes must be uniquely named (i.e., there are not functions called save() in both the foo and bar modules). You don’t want to confuse your customers by putting apples in two different bins.
If the package is large, it can add a lot to the namespace and (depending on a lot of factors) can slow things down. A general store can have a lot of little odds and ends that any individual customer might not want. That can might be overwhelming for your customers.
Requires a bit more effort and vigilance to keep some elements away from the user. For example, you might need to use underscores to keep functions from importing (e.g., _function_name()). Most general stores don’t have a big storage area where things like brooms and mops are kept; those items are visible to the customer. Even if it is unlikely that they would pick up a broom and start sweeping your floors, you might not want them to. In that case, you have to take extra steps to hide those supplies from view.
Recommendations
Use when it is hard to predict the workflow of a typical user (e.g., general packages like pandas or numpy). This is the “general” part of general store.
Use when the user might frequently bounce around between different modules (e.g., the leiap package)
Use when function and class names are very descriptive and easy to remember and specifying the module names will not improve readability. If your products are familiar things like fruits and vegetables, you don’t need a lot of signage; customers will figure things out quite easily.
Use with just a few modules. If there are many modules, it can be more difficult for a new user to find the functionality they want in the docs. If your general store gets too big, customers won’t be able to find the things they want.
Use when objects might be added or removed frequently. It’s easy to add and remove products in the general store without disrupting the customer.
Well-known examples
pandas
numpy (with additional complexity)
seaborn
The Convenience Store

Photo by Caio Resende from Pexels
By far the easiest to read and understand is a variation on the general store scenario that I call the convenience store. Instead of from .module import *, you can specify exactly what to import with from .module import func within __init__.py.
The convenience store shares a lot of traits with the general store. It has a relatively limited selection of goods which can be replaced at any time with minimal hassle. The customer doesn’t need a lot of signage to find what they need because most of the goods are easily in view. The biggest difference is that a convenience store has a bit more order. The empty boxes, brooms, and mops are all kept out of view of the customer and only the products for sale are on the shelves.
Behind the scenes
# __init__.py
from .foo import foo_func
from .bar import bar_func
from .baz import baz_func
User implementation
import example_pkg
example_pkg.foo_func()
example_pkg.bar_func()
example_pkg.baz_func()
Advantages
Shares all of the advantages of the general store, and adds:
Somewhat easier to control what objects are made available to the user
Disadvantages
__init__.py can end up very cluttered if there are many modules with many functions. Like the general store, a convenience store that is too cluttered will be harder for customers to navigate.
When new features are added to a module (i.e., new class or functions), they have to be explicitly added to the __init__.py file too. Modern IDEs can help detect missed imports, but it is still easy to forget. Your convenience store has some minimal signage and price tags. You have to remember to update these when you change what is on the shelf.
Recommendations
I would add the following to the recommendations from the general store:
Especially useful when your modules more or less consist of a single Class (e.g., from geopandas.geodataframe import GeoDataFrame)
Use when you have a small number of objects to import
Use when your objects have clear names
Use when you know exactly which objects your users will need and which they will not
Use when you do not expect to frequently add a lot of new modules and objects that will need to be imported.
Well-known example
geopandas
Online grocery shopping

Anyone who has bought groceries online knows that ordering the right product can take some effort on the part of the customer. You have to search for the product, choose a brand, choose the desired size, etc. All of these steps, however, allow you to buy exactly what you want from a nearly limitless stockroom.
In the case of Python packages, in some cases, it might be more prudent to eschew the convenience of simply importing the entire package and instead force the user to be more clear about what pieces are being imported. This allows you as the developer to include a lot more pieces to the package without overwhelming the user.
Behind the scenes
# __init__.py
import example_pkg.foo
import example_pkg.bar
import example_pkg.baz
User implementation
There are (at least) three different methods that a user could adopt in this case.
import example_pkg
example_pkg.foo.foo_func()
example_pkg.bar.bar_func()
example_pkg.bar.baz_func()
or
from example_pkg import foo, bar, baz
foo.foo_func()
bar.bar_func()
baz.baz_func()
or
import example_pkg.foo as ex_foo
import example_pkg.bar as ex_bar
import example_pkg.baz as ex_baz
ex_foo.foo_func()
ex_bar.bar_func()
ex_baz.baz_func()
Advantages
Simplifies the __init__.py file. Only needs to be updated when a new module is added. Updating your online store is relatively painless. You only need to change a setting in your product database.
It is flexible. It can be used to import only what the user needs or to import everything. The customers in your online store can search for only what they want or need. No need to bother looking through a “fruit” bin when all you need is an apple. But if they do want everything in the “fruit” bin, they can get that too.
Aliasing can clean up long package.module specifications (e.g., import matplotlib.pyplot as plt). While online grocery shopping can be a big pain at first, if you save your shopping list for the future, your shopping can be done a lot quicker.
Can have multiple objects with the same name (e.g., functions called save() in both the foo and bar modules)
Disadvantages
Some of the import methods can make code more complicated to read. For example, foo.foo_func() does not indicate which package foo comes from.
The most readable method (import example_pkg, with no alias) can lead to long code chunks (e.g., example_pkg.foo.foo_func()) that clutter things up.
Can be hard for users to track down all of the possible functionality. In your online grocery store, it would be hard for the shopper to see all of the possible goods.
Recommendations
Use when you have a complex series of modules, most of which any one user will never need.
Use when import example_pkg imports a LOT of objects and might be slow.
Use when you can define pretty clear workflows for different kinds of users.
Use when you can expect the user to be able to navigate your documentation well.
Examples
matplotlib *
scikit-learn *
bokeh *
scipy *
* These packages actually use combinations of different approaches in their __init__.py files. I include them here because to users, they are generally used à la carte (e.g., import matplotlib.pyplot as plt or import scipy.stats.kde).
Conclusion
The three scenarios I outlined are certainly not the only possible structures for a Python package, but I hope they cover most of the cases that anyone reading learning this from a blog might be considering. In conclusion, I’ll return to a point I made earlier: a good module structure for the developer may or may not be a good module structure for the user. Whatever your decision, don’t forget to put yourself in the user’s shoes even, or especially, because that user is most likely to be you.


-----------How to group data by time intervals in Python Pandas?----------------------------
How to group data by time intervals in Python Pandas?
One-liners to combine Time-Series data into different intervals like based on each hour, week, or a month.
Ankit Goel
Ankit Goel

Sep 12, 2020·7 min read



Photo by Aron Visuals on Unsplash
If you have ever dealt with Time-Series data analysis, you would have come across these problems for sure —
Combining data into certain intervals like based on each day, a week, or a month.
Aggregating data in the time interval like if you are dealing with price data then problems like total amount added in an hour, or a day.
Finding patterns for other features in the dataset based on a time interval.
In this article, you will learn about how you can solve these problems with just one-line of code using only 2 different Pandas API’s i.e. resample() and Grouper().
As we know, the best way to learn something is to start applying it. So, I am going to use a sample time-series dataset provided by World Bank Open data and is related to the crowd-sourced price data collected from 15 countries. For more details about the data, refer Crowdsourced Price Data Collection Pilot. For this exercise, we are going to use data collected for Argentina.
📚 Resources: Google Colab Implementation | Github Repository | Dataset 📚
Dataset Details
This data is collected by different contributors who participated in the survey conducted by the World Bank in the year 2015. The basic idea of the survey was to collect prices for different goods and services in different countries. We are going to use only a few columns from the dataset for the demo purposes —

Sample Snippet of the Dataset by Author.
Combining data based on different Time Intervals.
Pandas provides an API named as resample() which can be used to resample the data into different intervals. Let’s see a few examples of how we can use this —
Total Amount added each hour.
Let’s say we need to find how much amount was added by a contributor in an hour, we can simply do so using —
# data re-sampled based on an hour
data.resample('H', on='created_at').price.sum()
# output
created_at
2015-12-14 18:00:00     5449.90
2015-12-14 19:00:00       15.98
2015-12-14 20:00:00       66.98
2015-12-14 21:00:00        0.00
2015-12-14 22:00:00        0.00
Here is what we are doing here —
First, we resampled the data into an hour ‘H’ frequency for our date column i.e. created_at. We can use different frequencies, I will go through a few of them in this article. Check out Pandas Time Frequencies for a complete list of frequencies. You can even go up to nanoseconds.
After this, we selected the ‘price’ from the resampled data. Later we will see how we can aggregate on multiple fields i.e. total amount, quantity, and the unique number of items in a single command.
Computed the sum for all the prices. This will give us the total amount added in that hour.
By default, the time interval starts from the starting of the hour i.e. the 0th minute like 18:00, 19:00, and so on. We can change that to start from different minutes of the hour using offset attribute like —
# Starting at 15 minutes 10 seconds for each hour
data.resample('H', on='created_at', offset='15Min10s').price.sum()
# Output
created_at
2015-12-14 17:15:10     5370.00
2015-12-14 18:15:10       79.90
2015-12-14 19:15:10       64.56
2015-12-14 20:15:10       18.40
2015-12-14 21:15:10        0.00
Please note, you need to have Pandas version > 1.10 for the above command to work.
Total Amount added each week.
In this example, we will see how we can resample the data based on each week.
# data re-sampled based on an each week, just change the frequency
data.resample('W', on='created_at').price.sum()
# output
created_at
2015-12-20    4.305638e+04
2015-12-27    6.733851e+04
2016-01-03    4.443459e+04
2016-01-10    1.822236e+04
2016-01-17    1.908385e+05
By default, the week starts from Sunday, we can change that to start from different days i.e. let’s say if we would like to combine based on the week starting on Monday, we can do so using —
# data re-sampled based on an each week, week starting Monday
data.resample('W-MON', on='created_at').price.sum()
# output
created_at
2015-12-14    5.532860e+03
2015-12-21    3.850762e+04
2015-12-28    6.686329e+04
2016-01-04    5.392410e+04
2016-01-11    1.260869e+04
Total Amount added each month.
This is similar to what we have done in the examples before.
# data re-sampled based on each month
data.resample('M', on='created_at').price.sum()
# Output
created_at
2015-12-31    1.538769e+05
2016-01-31    4.297143e+05
2016-02-29    9.352684e+05
2016-03-31    7.425185e+06
2016-04-30    1.384351e+07
One observation to note here is that the output labels for each month are based on the last day of the month, we can use the ‘MS’ frequency to start it from 1st day of the month i.e. instead of 2015–12–31 it would be 2015–12–01 —
# month frequency from start of the month
data.resample('MS', on='created_at').price.sum()
created_at
2015-12-01    1.538769e+05
2016-01-01    4.297143e+05
2016-02-01    9.352684e+05
2016-03-01    7.425185e+06
2016-04-01    1.384351e+07
Multiple Aggregation on sampled data.
Often we need to apply different aggregations on different columns like in our example we might need to find —
Unique items that were added in each hour.
The total quantity that was added in each hour.
The total amount that was added in each hour.
We can do so in a one-line by using agg() on the resampled data. Let’s see how we can do it —
# aggregating multiple fields for each hour
data.resample('H', on='created_at').agg({'price':'sum', 'quantity':'sum','item_code':'nunique'})

Aggregated data based on each hour by Author.
Grouping data based on different Time intervals
In the above examples, we re-sampled the data and applied aggregations on it. What if we would like to group data by other fields in addition to time-interval? Pandas provide an API known as grouper() which can help us to do that.
In this section, we will see how we can group data on different fields and analyze them for different intervals.
Amount added for each store type in each month.
Let’s say we need to analyze data based on store type for each month, we can do so using —
# Grouping data based on month and store type
data.groupby([pd.Grouper(key='created_at', freq='M'), 'store_type']).price.sum().head(15)
# Output
created_at  store_type
2015-12-31  other                          34300.00
            public_semi_public_service       833.90
            small_medium_shop               2484.23
            specialized_shop              107086.00
2016-01-31  market                           473.75
            other                         314741.00
            private_service_provider         325.00
            public_semi_public_service       276.79
            small_medium_shop              31042.79
            specialized_shop               29648.44
2016-02-29  market                          1974.04
            other                         527950.00
            private_service_provider        1620.00
            public_semi_public_service      1028.52
            small_medium_shop             224653.83
Let’s understand how I did it —
First, we passed the Grouper object as part of the groupby statement which groups the data based on month i.e. ‘M’ frequency. This is similar to resample(), so whatever we discussed above applies here as well.
We added store_type to the groupby so that for each month we can see different store types.
For each group, we selected the price, calculated the sum, and selected the top 15 rows.
Total Amount added based on item_name in each month.
As we did in the last example, we can do a similar thing for item_name as well.
# Grouping data based on each month and item_name
data.groupby([pd.Grouper(key='created_at', freq='M'), 'item_name']).price.sum()
# Output
created_at  item_name
2015-12-31  Bar soap, solid, SB                                33.17
            Beer, domestic brand, single bottle, WKB           29.79
            Black tea, BL                                      12.00
            Black tea, in bags, WKB                            60.99
            Bread, white, sliced, WKB                          85.45
                                                              ...
2016-08-31  Wheat flour, not self-rising, BL                  150.38
            White sugar, WKB                                  266.47
            Women's haircut, basic hairdresser               7730.00
            Wrist-watch, men's, CITIZEN Eco-Drive BM6060    52205.00
            Yoghurt, plain, WKB                               150.96
Multiple Aggregation for store_type in each month.
We can apply aggregation on multiple fields similarly the way we did using resample(). The only thing which is different here is that the data would be grouped by store_type as well and also, we can do NamedAggregation (assign a name to each aggregation) on groupby object which doesn’t work for re-sample.
# grouping data and named aggregation on item_code, quantity, and price
data.groupby([pd.Grouper(key='created_at', freq='M'), 'store_type']).agg(unique_items=('item_code', 'nunique'),
         total_quantity=('quantity','sum'),
         total_amount=('price','sum'))

-----------



pandas: powerful Python data analysis toolkit Release 1.2.3
------------------------------------------------------------

In pandas a data table is called a DataFrame.

pandas supports the integration with many file formats or data sources out of the box (csv, excel, sql, json, parquet,. . . ). Importing data from each of these data sources is provided by function with the prefix read_*. Similarly, the to_* methods are used to store data.

There is no need to loop over all rows of your data table to do calculations. Data manipulations on a column work elementwise. Adding a column to a DataFrame based on existing data in other columns is straightforward.

Split-apply-combine approach:
Basic statistics (mean, median, min, max, counts...) are easily calculable. These or custom aggregations can be applied on the entire data set, a sliding window of the data or grouped by categories. The latter is also known as the split-apply-combine approach.

Change the structure of your data table in multiple ways. You can melt() your data table from wide to long/tidy form or pivot() from long to wide format. With aggregations built-in, a pivot table is created with a single command.

Multiple tables can be concatenated both column wise as row wise and database-like join/merge operations are provided to combine multiple tables of data.

pandas has great support for time series and has an extensive set of tools for working with dates, times, and time-indexed data.

Data sets do not only contain numerical data. pandas provides a wide range of functions to clean textual data and extract useful information from it.

Handling Import Errors:
If you encounter an ImportError, it usually means that Python couldn’t find pandas in the list of available libraries. Python internally has a list of directories it searches through, to find packages. You can obtain these directories with:
import sys 
sys.path

Dependencies 
Package             Minimum supported version
setuptools              24.2.0
NumPy                   1.16.5
python-dateutil         2.7.3
pytz                    2017.3

Package Overview:
-----------------
pandas is a Python package providing fast, flexible, and expressive data structures designed to make working with “relational” or “labeled” data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real-world data analysis in Python. Additionally, it has the broader goal of becoming the most powerful and flexible open source data analysis/manipulation tool available in any language. It is already well on its way towards this goal.
pandas is well suited for many different kinds of data:
• Tabular data with heterogeneously-typed columns, as in a SQL table or Excel spreadsheet
• Ordered and unordered (not necessarily fixed-frequency) time series data.
• Arbitrary matrix data (homogeneously typed or heterogeneous) with row and column labels
• Any other form of observational / statistical data sets. The data need not be labeled at all to be placed into a pandas data structure.

The two primary data structures of pandas, Series (1-dimensional) and DataFrame (2-dimensional), handle the vast majority of typical use cases in finance, statistics, social science, and many areas of engineering. pandas is built on top of NumPy and is intended to integrate well within a scientific computing environment with many other 3rd party libraries.

Here are just a few of the things that pandas does well:
Easy handling of missing data (represented as NaN) in floating point as well as non-floating point data 
Size mutability: columns can be inserted and deleted from DataFrame and higher dimensional objects
Automatic and explicit data alignment: objects can be explicitly aligned to a set of labels,or the user can simply ignore the labels and let Series, DataFrame, etc. automatically align the data for you in computations.
Powerful, flexible group by functionality to perform split-apply-combine operations on data sets, for both aggregating and transforming data
Make it easy to convert ragged, differently-indexed data in other Python and NumPy data structures into DataFrame objects
Intelligent label-based slicing, fancy indexing, and subsetting of large data sets Intuitive merging and joining data sets
Flexible reshaping and pivoting of data sets
Hierarchical labeling of axes (possible to have multiple labels per tick)
Robust IO tools for loading data from flat files (CSV and delimited), Excel files, databases, and saving / loading data from the ultrafast HDF5 format
Time series-specific functionality: date range generation and frequency conversion, moving window statistics, date shifting, and lagging.

Many of these principles are here to address the shortcomings frequently experienced using other languages / scientific research environments. For data scientists, working with data is typically divided into multiple stages: munging and cleaning data, analyzing / modeling it, then organizing the results of the analysis into a form suitable for plotting or tabular display. pandas is the ideal tool for all of these tasks.
Some other notes
• pandas is fast. Many of the low-level algorithmic bits have been extensively tweaked in Cython code. However, as with anything else generalization usually sacrifices performance. So if you focus on one feature for your application you may be able to create a faster specialized tool.
• pandas is a dependency of statsmodels, making it an important part of the statistical computing ecosystem in Python.
• pandas has been used extensively in production in financial applications.

DataStructures
df = DataFrame({
    'Dimensions':[1,2],
    'Name':['Series', 'DataFrame'],
    'Description':['1D labeled homogeneously-typed array',
                  'General 2D labeled, size-mutable tabular structure with potentially heterogeneously-typed column']
})
df


Dimensions	Name	    Description
	1	    Series	    1D labeled homogeneously-typed array
	2	    DataFrame	General 2D labeled, size-mutable tabular structure  with
                         potentially heterogeneously-typed column

Why more than one data structure?
The best way to think about the pandas data structures is as flexible containers for lower dimensional data. For example, DataFrame is a container for Series, and Series is a container for scalars. We would like to be able to insert and remove objects from these containers in a dictionary-like fashion.
Also, we would like sensible default behaviors for the common API functions which take into account the typical orientation of time series and cross-sectional data sets. When using the N-dimensional array (ndarrays) to store 2- and 3-dimensional data, a burden is placed on the user to consider the orientation of the data set when writing functions; axes are considered more or less equivalent (except when C- or Fortran-contiguousness matters for performance). In pandas, the axes are intended to lend more semantic meaning to the data; i.e., for a particular data set, there is likely to be a “right” way to orient the data. The goal, then, is to reduce the amount of mental effort required to code up data transformations in downstream functions.
For example, with tabular data (DataFrame) it is more semantically helpful to think of the index (the rows) and the columns rather than axis 0 and axis 1. Iterating through the columns of the DataFrame thus results in more readable code:

for col in df.columns:
    series = df[col]
    # do something with series

Mutability and copying of data
All pandas data structures are value-mutable (the values they contain can be altered) but not always size-mutable. The length of a Series cannot be changed, but, for example, columns can be inserted into a DataFrame. However, the vast majority of methods produce new objects and leave the input data untouched. In general we like to favor immutability where sensible.

1.4.3 Getting started tutorials
WHAT KIND OF DATA DOES PANDAS HANDLE?
---------------------------------------
I want to start using pandas
import pandas as pd

To load the pandas package and start working with it, import the package. The community agreed alias for pandas is pd, so loading pandas as pd is assumed standard practice for all of the pandas documentation.

pandas data table representation
I want to store passenger data of the Titanic. For a number of passengers, I know the name (characters), age (integers) and sex (male/female) data.

df = pd.DataFrame({
    'Names' : ['Ramesh Kumar', 'Suresh Kumar', 'Rupesh Kumar'],
    'Age' : [22, 35, 58],
    'Sex' : ['male', 'male', 'female']
})
df

To manually store data in a table, create a DataFrame. When using a Python dictionary of lists, the dictionary keys will be used as column headers and the values in each list as columns of the DataFrame.

A DataFrame is a 2-dimensional data structure that can store data of different types (including characters, integers, floating point values, categorical data and more) in columns. It is similar to a spreadsheet, a SQL table 

• The table has 3 columns, each of them with a column label. The column labels are respectively Name, Age and Sex.
• The columnName consists of textual data with each value a string,the column Age are numbers and the column Sex is textual data.

EACH COLUMN IN A DATAFRAME IS A SERIES
I’m just interested in working with the data in the column Age
df['Age]

When selecting a single column of a pandas DataFrame, the result is a pandas Series. To select the column, use the column label in between square brackets [].

Note: If you are familiar to Python dictionaries, the selection of a single column is very similar to selection of dictionary values based on the key.

You can create a Series from scratch as well:

ages = pd.Series([22, 35, 58])
ages

0    22
1    35
2    58
dtype: int64

A pandas Series has no column labels, as it is just a single column of a DataFrame. A Series does have row labels.

Do something with a DataFrame or Series
I want to know the maximum Age of the passengers
We can do this on the DataFrame by selecting the Age column and applying max():
df['Age'].max() # DataFrame
ages.max() # Series

As illustrated by the max() method, you can do things with a DataFrame or Series. pandas provides a lot of functionalities, each of them a method you can apply to a DataFrame or Series. As methods are functions, do not forget to use parentheses ().
I’m interested in some basic statistics of the numerical data of my data table
df.describe()

The describe() method provides a quick overview of the numerical data in a DataFrame. As the Name and Sex columns are textual data, these are by default not taken into account by the describe() method.
Many pandas operations return a DataFrame or a Series. The describe() method is an example of a pandas operation returning a pandas Series.

Bullet Points:
--------------
• Import the package, aka import pandas as pd
• A table of data is stored as a pandas DataFrame
• Each column in a DataFrame is a Series
• You can do things by applying a method to a DataFrame or Series

This tutorial uses the Titanic data set, stored as CSV. The data consists of the following data columns: 
• PassengerId: Id of every passenger.
• Survived: This feature have value 0 and 1. 0 for not survived and 1 for survived.
• Pclass: There are 3 classes: Class 1, Class 2 and Class 3.
• Name: Name of passenger.
• Sex: Gender of passenger.
• Age: Age of passenger.
• SibSp: Indication that passenger have siblings and spouse. 
• Parch: Whether a passenger is alone or have family.
• Ticket: Ticket number of passenger. 
• Fare: Indicating the fare.
• Cabin: The cabin of passenger.
• Embarked: The embarked category.

HOW DO I READ AND WRITE TABULAR DATA?
--------------------------------------
I want to analyze the Titanic passenger data, available as a CSV file.
titanic = pd.read_csv('data/titanic.csv')
titanic

pandas provides the read_csv() function to read data stored as a csv file into a pandas DataFrame. pandas supports many different file formats or data sources out of the box (csv, excel, sql, json, parquet, . . . ), each of them with the prefix read_*.
Make sure to always have a check on the data after reading in the data. When displaying a DataFrame, the first and last 5 rows will be shown by default:

I want to see the first 8 rows of a pandas DataFrame.
titanic.head(8)
To see the first N rows of a DataFrame, use the head() method with the required number of rows (in this case 8) as argument.

Interested in the last N rows instead? pandas also provides a tail() method. For example, titanic.tail(10) will return the last 10 rows of the DataFrame.

A check on how pandas interpreted each of the column data types can be done by requesting the pandas dtypes attribute:
titanic.dtypes
PassengerId      int64
Pclass           int64
Name            object
Sex             object
Age            float64

For each of the columns, the used data type is enlisted. The data types in this DataFrame are integers (int64), floats (float64) and strings (object).

NOTE: 
When asking for the dtypes, no brackets are used! dtypes is an attribute of a DataFrame and Series. Attributes of DataFrame or Series do not need brackets. Attributes represent a characteristic of a DataFrame/Series, whereas a method (which requires brackets) do something with the DataFrame/Series.

My colleague requested the Titanic data as a spreadsheet.
titanic.to_excel("titanic.xlsx", sheet_name="passengers", index=False)

Whereas read_* functions are used to read data to pandas, the to_* methods are used to store data. The to_excel() method stores the data as an excel file. In the example here, the sheet_name is named passengers instead of the default Sheet1. By setting index=False the row index labels are not saved in the spreadsheet.
The equivalent read function read_excel() will reload the data to a DataFrame:
titanic = pd.read_excel("titanic.xlsx", sheet_name="passengers")

I’m interested in a technical summary of a DataFrame

titanic.info()

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   PassengerId  891 non-null    int64  
 1   Survived     891 non-null    int64  
 2   Pclass       891 non-null    int64  
 3   Name         891 non-null    object

dtypes: float64(2), int64(4), object(5)
memory usage: 36.0+ KB

The method info() provides technical information about a DataFrame, so let’s explain the output in more detail:
In [1]: import pandas as pd

This tutorial uses the Titanic data set, stored as CSV. The data consists of the following data columns:
• PassengerId: Id of every passenger.
It is indeed a DataFrame.
There are 891 entries, i.e. 891 rows.
Each row has a row label (aka the index) with values ranging from 0 to 890.
The table has 12 columns. Most columns have a value for each of the rows (all 891 values are non-null). Some columns do have missing values and less than 891 non-null values.
The columns Name, Sex, Cabin and Embarked consists of textual data (strings, aka object). The other columns are numerical data with some of them whole numbers (aka integer) and others are real numbers (aka float).
* The kind of data (characters, integers,. . . ) in the different columns are summarized by listing the dtypes. The approximate amount of RAM used to hold the DataFrame is provided as well.
* Getting data in to pandas from many different file formats or data sources is supported by read_* functions. Exporting data out of pandas is provided by different to_* methods.
* The head/tail/info methods and the dtypes attribute are convenient for a first check.

HOW DO I SELECT A SUBSET OF A TABLE?
HOW DO I SELECT A SUBSET OF A DATAFRAME?
HOW DO I SELECT SPECIFIC COLUMNS FROM A DATAFRAME?
I’m interested in the age of the Titanic passengers.
ages = titanic['Age']
ages.head()

To select a single column, use square brackets [] with the column name of the column of interest.
Each column in a DataFrame is a Series. As a single column is selected, the returned object is a pandas Series.
We can verify this by checking the type of the output:
type(titanic['Age']) # pandas.core.series.Series

And have a look at the shape of the output:
titanic['Age'].shape # (891,)
titanic.shape # (891, 12)

DataFrame.shape is an attribute of a pandas Series and DataFrame containing the number of rows and columns: (nrows, ncolumns). A pandas Series is 1-dimensional and only the number of rows is returned.

I’m interested in the age and sex of the Titanic passengers.
age_sex = titanic[['Age', 'Sex']]
age_sex.head()
To select multiple columns, use a list of column names within the selection brackets [].
Note: The inner square brackets define a Python list with columnnames,whereas the outer brackets are used to select the data from a pandas DataFrame as seen in the previous example.

The returned data type is a pandas DataFrame:
age_sex = titanic[['Age', 'Sex']]
age_sex.head()
type(age_sex) # pandas.core.frame.DataFrame
age_sex.shape # (891, 2)
The selection returned a DataFrame with 891 rows and 2 columns. Remember, a DataFrame is 2-dimensional with both a row and column dimension.

How do I filter specific rows from a DataFrame?
I’m interested in the passengers older than 35 years.
above_35 = titanic[titanic['Age'] > 35]
above_35.head()

To select rows based on a conditional expression, use a condition inside the selection brackets []. The condition inside the selection brackets 
titanic["Age"] > 35
checks for which rows the Age column has a value greater than 35
(titanic['Age']>35).head() 

0    False
1     True
2     True
3    False
4    False
Name: Age, dtype: bool

The output of the conditional expression (>, but also ==, !=, <, <=,. . . would work) is actually a pandas Series of boolean values (either True or False) with the same number of rows as the original DataFrame. Such a Series of boolean values can be used to filter the DataFrame by putting it in between the selection brackets []. Only rows for which the value is True will be selected.

We know from before that the original Titanic DataFrame consists of 418 rows. Let’s have a look at the number of rows which satisfy the condition by checking the shape attribute of the resulting DataFrame above_35:
above_35.shape # (105, 11)

I’m interested in the Titanic passengers from cabin class 2 and 3.
class_23 = titanic[titanic['Pclass'].isin([2,3])]
class_23.head()

Similar to the conditional expression, the isin() conditional function returns a True for each row the values are in the provided list. To filter the rows based on such a function, use the conditional function inside the selection brackets []. In this case, the condition inside the selection brackets titanic["Pclass"].isin([2, 3]) checks for which rows the Pclass column is either 2 or 3.
The above is equivalent to filtering by rows for which the class is either 2 or 3 and combining the two statements with an | (or) operator:

class_32 = titanic[(titanic['Pclass'] == 2 ) | (titanic['Pclass'] == 3 )]

Note: When combining multiple conditional statements, each condition must be surrounded by parentheses (). Moreover, you can not use or/and but need to use the or operator | and the and operator &.

I want to work with passenger data for which the age is known.
age_not_na = titanic[titanic['Age'].notna()]
age_not_na

The notna() conditional function returns a True for each row the values are not an Null value. As such, this can be combined with the selection brackets [] to filter the data table.
You might wonder what actually changed, as the first 5 lines are still the same values. One way to verify is to check if the shape has changed:
age_not_na = titanic[titanic['Age'].notna()]
age_not_na.shape # (714, 12)

How do I select specific rows and columns from a DataFrame?
I’m interested in the names of the passengers older than 35 years.
adult_names = titanic.loc[titanic['Age']>35, 'Name']
adult_names.head()

In this case, a subset of both rows and columns is made in one go and just using selection brackets [] is not sufficient anymore. The loc/iloc operators are required in front of the selection brackets []. WHEN USING LOC/ILOC, THE PART BEFORE THE COMMA IS THE ROWS YOU WANT, AND THE PART AFTER THE COMMA IS THE COLUMNS YOU WANT TO SELECT.
When using the column names, row labels or a condition expression, use the loc operator in front of the selection brackets []. For both the part before and after the comma, you can use a single label, a list of labels, a slice of labels, a conditional expression or a colon. USING A COLON SPECIFIES YOU WANT TO SELECT ALL ROWS OR COLUMNS.

I’m interested in rows 10 till 25 and columns 3 to 5.
titanic.iloc[9:25, 2:5]

Again, a subset of both rows and columns is made in one go and just using selection brackets [] is not sufficient anymore. When specifically interested in certain rows and/or columns based on their position in the table, use the iloc operator in front of the selection brackets [].
When selecting specific rows and/or columns with loc or iloc, new values can be assigned to the selected data. For example, to assign the name anonymous to the first 3 elements of the third column:
titanic.iloc[:3,2] = 'anonymous'
titanic

Bullet Points for Indexing:
• When selecting subsets of data, square brackets [] are used.
• Inside these brackets, you can use a single column/row label, a list of column/row labels, a slice of labels, a conditional expression or a colon.
• Select specific rows and/or columns using loc when using the row and column names
• Select specific rows and/or columns using iloc when using the positions in the table
• You can assign new values to a selection based on loc/iloc.

For this tutorial, air quality data about 𝑁𝑂2 is used, made available by openaq and using the py-openaq package. The air_quality_no2.csv data set provides 𝑁𝑂2 values for the measurement stations FR04014, BETR801 and London Westminster in respectively Paris, Antwerp and London.
air_quality = pd.read_csv("data/air_quality_no2.csv", index_col=0, parse_dates=True)
air_quality.head()

	                station_antwerp	    station_paris	station_london
datetime			
2019-05-07 02:00:00	    NaN	                NaN	            23.0
2019-05-07 03:00:00	    50.5	            25.0	        19.0
2019-05-07 04:00:00	    45.0	            27.7	        19.0
2019-05-07 05:00:00	    NaN	                50.4	        16.0
2019-05-07 06:00:00	    NaN	                61.9        	NaN

air_quality.info()
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 1035 entries, 2019-05-07 02:00:00 to 2019-06-21 02:00:00
Data columns (total 3 columns):
 #   Column           Non-Null Count  Dtype  
---  ------           --------------  -----  
 0   station_antwerp  95 non-null     float64
 1   station_paris    1004 non-null   float64
 2   station_london   969 non-null    float64
dtypes: float64(3)
memory usage: 32.3 KB

Note: The usage of the index_col and parse_dates parameters of the read_csv function to define the first (0th) column as index of the resulting DataFrame and convert the dates in the column to Timestamp objects, respectively.

HOW TO CREATE PLOTS IN PANDAS?
------------------------------
I want a quick visual check of the data.
air_quality.plot()

With a DataFrame, pandas creates by default one 'line plot' for each of the columns with numeric data.

I want to plot only the columns of the data table with the data from Paris.
air_quality['station_paris'].plot()

To plot a specific column, use the selection method of the subset data in combination with the plot() method. Hence, the plot() method works on both Series and DataFrame.

air_quality.plot.scatter(x='station_london', y='station_paris', alpha=0.7)
# alpha make scatters solid as it reaches 1.

Apart from the default line plot when using the plot function, a number of alternatives are available to plot data. Let’s use some standard Python to get an overview of the available plot methods:
code:
[
    method_name
    for method_name in dir(air_quality.plot)
    if not method_name.startswith("_")
]
['area',
 'bar',
 'barh',
 'box',
 'density',
 'hexbin',
 'hist',
 'kde',
 'line',
 'pie',
 'scatter']

Note: In many development environments as well as IPython and Jupyter Notebook, use the TAB button to get an overview of the available methods, for example air_quality.plot. + TAB.

One of the options is DataFrame.plot.box(), which refers to a boxplot. The box method is applicable on the air quality example data:
air_quality.plot.box()
air_quality.describe() # to understand the boxplot.

I want each of the columns in a separate subplot.
axs = air_quality.plot.area(figsize=(12,6) , subplots=True)

Separate subplots for each of the data columns are supported by the subplots argument of the plot functions. The builtin options available in each of the pandas plot functions that are worthwhile to have a look.

I want to further customize, extend or save the resulting plot.
fig, axs = plt.subplots(figsize = (12, 4))
air_quality.plot.area(ax=axs)
axs.set_ylabel('NO$_2$ Concentration')
fig.savefig("no2_concentration.png")

Each of the plot objects created by pandas is a matplotlib object. As Matplotlib provides plenty of options to customize plots, making the link between pandas and Matplotlib explicit enables all the power of matplotlib to the plot. This strategy is applied in the previous example:

fig, axs = plt.subplots(figsize=(12, 4)) ## Create an empty matplotlib Figure and Axes
air_quality.plot.area(ax=axs) ## Use pandas to put the area plot on the prepared Figure/Axes
axs.set_ylabel("NO$_2$ concentration") ## Do any matplotlib customization you like
fig.savefig("no2_concentrations.png") ## Save the Figure/Axes using the existing matplotlib method.

Bullet Points on plots:
• The .plot.* methods are applicable on both Series and DataFrames
• By default, each of the columns is plotted as a different element (line, boxplot,. .) • Any plot created by pandas is a Matplotlib object.

HOW TO CREATE NEW COLUMNS DERIVED FROM EXISTING COLUMNS?
--------------------------------------------------------
I WANT TO EXPRESS THE 𝑁𝑂2 CONCENTRATION OF THE STATION IN LONDON IN mg/m3

(If we assume temperature of 25 degrees Celsius and pressure of 1013 hPa, the conversion factor is 1.882)

air_quality['london_mg_per_cubic'] = air_quality['station_london']*1.882
air_quality.head()

To create a new column, use the [] brackets with the new column name at the left side of the assignment.
Note: The calculation of the values is done element_wise.This means all values in the given column are multiplied by the value 1.882 at once. You do not need to use a loop to iterate each of the rows!

I WANT TO CHECK THE RATIO OF THE VALUES IN PARIS VERSUS ANTWERP AND SAVE THE RESULT IN A NEW COLUMN
air_quality['ratio_paris_antwerp'] = (
    air_quality['station_paris'] / air_quality['station_antwerp']
)
air_quality.head()

The calculation is again element-wise, so the / is applied for the values in each row.
Also other mathematical operators (+, -, *, /) or logical operators (<, >, =,. . . ) work element wise.

I WANT TO RENAME THE DATA COLUMNS TO THE CORRESPONDING STATION IDENTIFIERS USED BY openAQ
air_quality_renamed = air_quality.rename(
    columns={
        'station_antwerp': 'BETR801',
        "station_paris": "FR04014",
        "station_london": "London Westminster",
    }
)
air_quality_renamed.head()

The rename() function can be used for both row labels and column labels. Provide a dictionary with the keys as the current names and the values as the new names to update the corresponding names.

The mapping should not be restricted to fixed names only, but can be a mapping function as well. For example, converting the column names to lowercase letters can be done using a function as well:

air_quality_renamed = air_quality_renamed.rename(columns=str.lower)
air_quality_renamed.head()

REMEMBER
* Create a new column by assigning the output to the DataFrame with a new column name in between the [].
* Operations are element-wise, no need to loop over rows.
* Use rename with a dictionary or function to rename row labels or column names.

HOW TO CALCULATE SUMMARY STATISTICS?
This tutorial uses the Titanic data set, stored as CSV. The data consists of the following data columns: • PassengerId: Id of every passenger.
• Survived: This feature have value 0 and 1. 0 for not survived and 1 for survived.
• Pclass: There are 3 classes: Class 1, Class 2 and Class 3.
• Name: Name of passenger.
• Sex: Gender of passenger.
• Age: Age of passenger.
• SibSp: Indication that passenger have siblings and spouse. • Parch: Whether a passenger is alone or have family.
• Ticket: Ticket number of passenger. • Fare: Indicating the fare.
• Cabin: The cabin of passenger.
• Embarked: The embarked category.

titanic = pd.read_csv('data/titanic.csv')
titanic.head()

AGGREGATING STATISTICS
-----------------------
What is the average age of the Titanic passengers?
titanic['Age'].mean() # 29.699

Different statistics are available and can be applied to columns with numerical data. Operations in general exclude missing data and operate across rows by default.

What is the median age and ticket fare price of the Titanic passengers?
titanic[['Age', 'Fare']].median()
Age     28.0000
Fare    14.4542
dtype: float64

The statistic applied to multiple columns of a DataFrame is calculated for each numeric column.
The aggregating statistic can be calculated for multiple columns at the same time. Remember the describe function.
titanic[['Age', 'Fare']].describe()

Instead of the predefined statistics, specific combinations of aggregating statistics for given columns can be defined using the DataFrame.agg() method:
titanic.agg({
    'Age':['min', 'max', 'median', 'skew', 'mean'],
    'Fare':['min', 'max', 'median', 'skew', 'mean', 'std'],
})

AGGREGATING STATISTICS GROUPED BY CATEGORY
-------------------------------------------
What is the average age for male versus female Titanic passengers?
titanic[['Sex','Age']].groupby('Sex').mean()
	        Age
Sex	
female	27.915709
male	30.726645

As our interest is the average age for each gender, a subselection on these two columns is made first: titanic[[ "Sex", "Age"]]. Next, the groupby() method is applied on the Sex column to make a group per category. The average age for each gender is calculated and returned.
Calculating a given statistic (e.g. mean age) for each category in a column (e.g. male/female in the Sex column) is a common pattern. The groupby method is used to support this type of operations. More general, this fits in the more general split-apply-combine pattern:
• Split the data into groups
• Apply a function to each group independently 
• Combine the results into a data structure

Split - Sex in Male and Female
Apply - Mean function on each group splitted
Combine - Combbine the result and present

The apply and combine steps are typically done together in pandas.

In the previous example, we explicitly selected the 2 columns first. If not, the mean method is applied to each column containing numerical columns:
titanic.groupby('Sex').mean()


It does not make much sense to get the average value of the Pclass. if we are only interested in the average age for each gender, the selection of columns (rectangular brackets [] as usual) is supported on the grouped data as well:
titanic.groupby('Sex')['Age'].mean()

Why option is Faster?
%timeit titanic.groupby('Sex')['Age'].mean() # 801 µs ± 45.4 µs
%timeit titanic[['Sex','Age']].groupby('Sex').mean() # 24.1 ms ± 282 µs

Note: ThePclass column contains numerical data butactually represents 3 categories(orfactors) respectively the labels ‘1’, ‘2’ and ‘3’. Calculating statistics on these does not make much sense. Therefore, pandas provides a Categorical data type to handle this type of data.

What is the mean ticket fare price for each of the sex and cabin class combinations?
titanic.groupby(['Sex', 'Pclass'])['Fare'].mean()
Grouping can be done by multiple columns at the same time. Provide the column names as a list to the groupby() method.

COUNT NUMBER OF RECORDS BY CATEGORY
What is the number of passengers in each of the cabin classes?
titanic.Pclass.value_counts()
The function is a shortcut, as it is actually a groupby operation in combination with counting of the number of records within each group:
titanic.groupby('Pclass')['Pclass'].count()

Why one is faster?
%timeit titanic.Pclass.value_counts() # 389 µs ± 44.5 µs per loop
%timeit titanic.groupby('Pclass')['Pclass'].count() # 530 µs ± 38.6 µs per loop

Note: Both size and count can be used in combination with groupby.Where as size includes NaN values and just provides the number of rows (size of the table), count excludes the missing values. In the value_counts method, use the dropna argument to include or exclude the NaN value

REMEMBER
* Aggregation statistics can be calculated on entire columns or rows
* groupby provides the power of the split-apply-combine pattern
* value_counts is a convenient shortcut to count the number of entries in each category of a variable.  

HOW TO RESHAPE THE LAYOUT OF TABLES?
------------------------------------
This tutorial uses air quality data about 𝑁𝑂2 and Particulate matter less than 2.5 micrometers, made available by openaq and using the py-openaq package. The air_quality_long.csv data set provides 𝑁 𝑂2 and 𝑃 𝑀25 values for the measurement stations FR04014, BETR801 and London Westminster in respectively Paris, Antwerp and London.
The air-quality data set has the following columns:
• city: city where the sensor is used, either Paris, Antwerp or London
• country: country where the sensor is used, either FR, BE or GB
• location: the id of the sensor, either FR04014, BETR801 or London Westminster
• parameter: the parameter measured by the sensor, either 𝑁 𝑂2 or Particulate matter • value: the measured value
• unit: the unit of the measured parameter, in this case ‘μg/m3’
and the index of the DataFrame is datetime, the datetime of the measurement.

Note: The air-quality data is provided in a so-called long format data representation with each observation on a separate row and each variable a separate column of the data table. The long/narrow format is also known as the tidy data format.

tidy data - https://www.jstatsoft.org/article/view/v059i10
(A huge amount of effort is spent cleaning data to get it ready for analysis, but there has been little research on how to make data cleaning as easy and effective as possible. This paper tackles a small, but important, component of data cleaning: data tidying. Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets. This structure also makes it easier to develop tidy tools for data analysis, tools that both input and output tidy datasets. The advantages of a consistent data structure and matching tools are demonstrated with a case study free from mundane data manipulation chores.)

air_quality = pd.read_csv('data/air_quality_long.csv', index_col='date.utc', parse_dates=True) # date.utc is a column in dataset
air_quality.head()

How to reshape the layout of tables? 

SORT TABLE ROWS

I want to sort the Titanic data according to the age of the passengers.
titanic.sort_values(by='Age').head() ## KDB equivalent - `age xasc select from ('',(,)",")0:`titanic.csv

I want to sort the Titanic data according to the cabin class and age in descending order.
titanic.sort_values(by=['Pclass','Age'], ascending=False)

With Series.sort_values(), the rows in the table are sorted according to the defined column(s). The index will follow the row order.

LONG TO WIDE TABLE FORMAT
-------------------------
Let’s use a small subset of the air quality data set. We focus on 𝑁 𝑂2 data and only use the first two measurements of each location (i.e. the head of each group). The subset of data will be called no2_subset

 # filter for no2 data only
no2 = air_quality[air_quality['parameter'] == 'no2']
no2.head()

# use 2 measurements (head) for each location (groupby)
no2_subset = no2.sort_index().groupby(['location']).head(2)
no2_subset

I want the values for the three stations(location) as separate columns next to each other
no2_subset.pivot(columns='location', values='value') ## OR
no2_subset.pivot(columns='location')['value']

Which one is faster?
%timeit no2_subset.pivot(columns='location', values='value') # 2.34 ms ± 306 µs per loop
%timeit no2_subset.pivot(columns='location')['value'] # 5.6 ms ± 204 µs per loop

The pivot() function is purely reshaping of the data: a single value for each index/column combination is required.

As pandas support plotting of multiple columns out of the box, the conversion from long to wide table format enables the plotting of the different time series at the same time.
no2 = air_quality[air_quality['parameter'] == 'no2']
no2.head()
no2.pivot(columns='location', values='value').plot(figsize = (12, 6))

Note: When the index parameter is not defined, the existing index (rowlabels) is used.

PIVOT TABLE
I want the mean concentrations for 𝑁𝑂2 and 𝑃𝑀2.5 in each of the stations(locations) in table form.
air_quality = pd.read_csv('data/air_quality_long.csv', index_col='date.utc', parse_dates=True) # date.utc is a column in dataset

air_quality.pivot_table(columns='parameter', values='value', index='location', aggfunc='mean')

Difference between pivot() and pivot_table()?
In the case of pivot(), the data is only rearranged. When multiple values need to be aggregated (in this specific case, the values on different time steps) pivot_table() can be used, providing an aggregation function (e.g. mean) on how to combine these values.

Pivot table is a well known concept in spreadsheet software. When interested in summary columns for each variable separately as well, put the margin parameter to True:
air_quality.pivot_table(columns='parameter', values='value', 
                        index='location', aggfunc='mean', margins=True,)

Note: In case you are wondering, pivot_table() is indeed directly linked to groupby().The same result can be derived by grouping on both parameter and location:
air_quality.groupby(['parameter', 'location']).mean()

WIDE TO LONG FORMAT
no2_pivoted = no2.pivot(columns="location", values='value').reset_index()
no2_pivoted.head()

I want to collect all air quality 𝑁𝑂2 measurements in a single column (long format)
no2_pivoted.melt(id_vars='date.utc')

The pandas.melt() method on a DataFrame converts the data table from wide format to long format. The column headers become the variable names in a newly created column.

The solution is the short version on how to apply pandas.melt(). The method will melt all columns NOT mentioned in id_vars together into two columns: A column with the column header names and a column with the values itself. The latter column gets by default the name value.

The pandas.melt() method can be defined in more detail
no2 = no2_pivoted.melt( id_vars='date.utc', value_vars=['BETR801','FR04014','London Westminster',], value_name='NO2', var_name='id_location', )
no2.head()

The result in the same, but in more detail defined:
• value_vars  - defines explicitly which columns to melt together
• value_name  - provides a custom column name for the values column instead of the default column name value
• var_name  - provides a custom column name for the column collecting the column header names. Otherwise it takes the index name or a default variable

Hence, the arguments value_name and var_name are just user-defined names for the two generated columns. The columns to melt are defined by id_vars and value_vars.

REMEMBER
* Sorting by one or more columns is supported by sort_values
* The pivot function is purely restructuring of the data, pivot_table supports aggregations
* The reverse of pivot (long to wide format) is melt (wide to long format)

HOW TO COMBINE DATA FROM MULTIPLE TABLES?
------------------------------------------
DataSets:
air_quality_no2 = pd.read_csv("data/air_quality_no2_long.csv", parse_dates=True)
air_quality_no2.head()
air_quality_no2 = air_quality_no2[[ 'date.utc', 'location',
                                  'parameter', 'value', ]]
air_quality_no2.head()

The air_quality_pm25_long.csv data set provides 𝑃𝑀25 values for the measurement stations FR04014, BETR801 and London Westminster in respectively Paris, Antwerp and London.
air_quality_pm25 = pd.read_csv("data/air_quality_pm25_long.csv",parse_dates=True)
air_quality_pm25.head()
air_quality_pm25 = air_quality_pm25[[ 'date.utc', 'location',
                                  'parameter', 'value', ]]
air_quality_pm25.head()

How to combine data from multiple tables? 
CONCATENATING OBJECTS
I want to combine the measurements of 𝑁𝑂2 and 𝑃𝑀25, two tables with a similar structure, in a single table

air_quality = pd.concat([air_quality_pm25, air_quality_no2], axis=0)
air_quality

The concat() function performs concatenation operations of multiple tables along one of the axis (row-wise or column-wise).
By default concatenation is along axis 0, so the resulting table combines the rows of the input tables. Let’s check the shape of the original and the concatenated tables to verify the operation:

print('Shape of the ``air_quality_pm25`` table', air_quality_pm25.shape)
print('Shape of the ``air_quality_no2`` table', air_quality_no2.shape)
print('Shape of resulting ``air_quality`` table', air_quality.shape)

Shape of the ``air_quality_pm25`` table (1110, 4)
Shape of the ``air_quality_no2`` table (2068, 4)
Shape of resulting ``air_quality`` table (3178, 4)

Hence, the resulting table has 3178 = 1110 + 2068 rows

Note: The axis argument will return in a number of pandas methods that can be applied along an axis. A DataFrame has two corresponding axes: the first running vertically downwards across rows (axis 0), and the second running horizontally across columns (axis 1). Most operations like concatenation or summary statistics are by default across rows (axis 0), but can be applied across columns as well.

Sorting the table on the datetime information illustrates also the combination of both tables, with the parameter column defining the origin of the table (either no2 from table air_quality_no2 or pm25 from table air_quality_pm25):

air_quality.sort_values('date.utc')

In this specific example, the parameter column provided by the data ensures that each of the original tables can be identified. This is not always the case. the concat function provides a convenient solution with the keys argument, adding an additional (hierarchical) row index. For example:

air_quality_ = pd.concat([air_quality_pm25, air_quality_no2], keys=["PM25", 'NO2',])
air_quality_

Note: The existence of multiple row/column indices at the same time has not been mentioned within these tutorials. Hierarchical indexing or MultiIndex is an advanced and powerful pandas feature to analyze higher dimensional data.
Multi-indexing is out of scope for this pandas introduction. For the moment, remember that the func- tion reset_index can be used to convert any level of an index to a column, e.g. air_quality. reset_index(level=0)

air_quality_ = pd.concat([air_quality_pm25, air_quality_no2], keys=["PM25", 'NO2',])
air_quality_
air_quality_.reset_index(level=0)

JOIN TABLES USING A COMMON IDENTIFIER

We have to Add the station coordinates, provided by the stations metadata table, to the corresponding rows in the measurements table.

Note: The stations used in this example (FR04014,BETR801 and LondonWestminster)are just three entries enlisted in the metadata table. We only want to add the coordinates of these three to the measurements table, each on the corresponding rows of the air_quality table.
stations_coord = pd.read_csv("data/air_quality_stations.csv")
stations_coord.head()

air_quality.head()

air_quality = pd.merge(air_quality, stations_coord, how='left', on='location')
air_quality.head()

Using the merge() function, for each of the rows in the air_quality table, the corresponding coordinates are added from the air_quality_stations_coord table. Both tables have the column location in common which is used as a key to combine the information. By choosing the left join, only the locations available in the air_quality (left) table, i.e. FR04014, BETR801 and London Westminster, end up in the resulting table. The merge function supports multiple join options similar to database-style operations.

I want to Add the parameter full description and name, provided by the parameters metadata table, to the measurements table

air_quality_parameters = pd.read_csv("data/air_quality_parameters.csv")
air_quality_parameters.head()

air_quality.head()

air_quality = pd.merge(air_quality, air_quality_parameters, how='left',
                      left_on='parameter', right_on='id')
air_quality.head()

Compared to the previous example, there is no common column name. However, the parameter column in the air_quality table and the id column in the air_quality_parameters_name both provide the measured variable in a common format. The left_on and right_on arguments are used here (instead of just on) to make the link between the two tables.
pandas supports also inner, outer, and right joins.

REMEMBER
* Multiple tables can be concatenated both column-wise and row-wise using the concat function.
* For database-like merging/joining of tables, use the merge function.

HOW TO HANDLE TIME SERIES DATA WITH EASE?
=========================================
For this tutorial, air quality data about 𝑁𝑂2 and Particulate matter less than 2.5 micrometers is used, made available by openaq and downloaded using the py-openaq package. The air_quality_no2_long.csv" data set provides 𝑁 𝑂2 values for the measurement stations FR04014, BETR801 and London Westminster in respectively Paris, Antwerp and London.

air_quality = pd.read_csv("data/air_quality_no2_long.csv")
air_quality.rename(columns = {'date.utc': 'datetime'})
air_quality.head()
air_quality.city.unique()

air_quality.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 2068 entries, 0 to 2067
Data columns (total 7 columns):
 #   Column     Non-Null Count  Dtype  
---  ------     --------------  -----  
 0   city       2068 non-null   object 
 1   country    2068 non-null   object 
 2   datetime   2068 non-null   object

Currently, datetime is object type.

Using pandas datetime properties
---------------------------------
I want to work with the dates in the column datetime as datetime objects instead of plain text.

air_quality['datetime'] = pd.to_datetime(air_quality.datetime)
air_quality.datetime.dtype # datetime64[ns, UTC]

Initially, the values in datetime are character strings and do not provide any datetime operations (e.g. extract the year, day of the week,. . . ). By applying the to_datetime function, pandas interprets the strings and convert these to datetime (i.e. datetime64[ns, UTC]) objects. In pandas we call these datetime objects similar to datetime. datetime from the standard library as pandas.Timestamp.

Note: Asmanydatasetsdocontaindatetimeinformationinoneofthecolumns,pandasinputfunctionlikepandas. read_csv() and pandas.read_json() can do the transformation to dates when reading the data using the parse_dates parameter with a list of the columns to read as Timestamp:
pd.read_csv("../data/air_quality_no2_long.csv", parse_dates=["datetime"])

Why are these pandas.Timestamp objects useful? 
Let’s illustrate the added value with some example cases. What is the start and end date of the time series data set we are working with?
air_quality.datetime.min(), air_quality.datetime.max()
(Timestamp('2019-05-07 01:00:00+0000', tz='UTC'),
 Timestamp('2019-06-21 00:00:00+0000', tz='UTC'))

 Using pandas.Timestamp for datetimes enables us to calculate with date information and make them comparable. Hence, we can use this to get the length of our time series:
 air_quality.datetime.max() -  air_quality.datetime.min()
 # Timedelta('44 days 23:00:00')

 The result is a pandas.Timedelta object, similar to datetime.timedelta from the standard Python library and defining a time duration.

 I want to add a new column to the DataFrame containing only the month of the measurement
air_quality['month'] = air_quality.datetime.dt.month
air_quality.head()

By using Timestamp objects for dates, a lot of time-related properties are provided by pandas. For example the month, but also year, weekofyear, quarter,. . . All of these properties are accessible by the dt accessor.

What is the average 𝑁𝑂2 concentration for each day of the week for each of the measurement locations?
air_quality.groupby([air_quality.datetime.dt.weekday, 'location'])['value'].mean()

Here, we want to calculate a given statistic (e.g. mean 𝑁𝑂2) for each weekday and for each measurement location. To group on weekdays, we use the datetime property weekday (with Monday=0 and Sunday=6) of pandas Timestamp, which is also accessible by the dt accessor. The grouping on both locations and weekdays can be done to split the calculation of the mean on each of these combinations.

Plot the typical 𝑁𝑂2 pattern during the day of our time series of all stations together. In other words, what is the average value for each hour of the day?

fig, axs = plt.subplots(figsize=(12,4))
air_quality.groupby(air_quality.datetime.dt.hour)['value'].mean().plot(
    kind='bar', rot=270, ax=axs)
plt.xlabel('Hour of the day')
plt.ylabel('$NO_2 (μg/m^3)$')
fig.savefig('hourlyPollution.png')

Similar to the previous case, we want to calculate a given statistic (e.g. mean 𝑁𝑂2) for each hour of the day and we can use the split-apply-combine approach again. For this case, we use the datetime property hour of pandas Timestamp, which is also accessible by the dt accessor.

Datetime as index
------------------
In the tutorial on reshaping, pivot() was introduced to reshape the data table with each of the measurements locations as a separate column:
no_2 = air_quality.pivot(index='datetime', columns='location', values='value')
no_2.head()

Note: By pivoting the data, the datetime information became the index of the table. In general, setting a column as an index can be achieved by the set_index function.

Working with a datetime index (i.e. DatetimeIndex) provides powerful functionalities. For example, we do not need the dt accessor to get the time series properties, but have these properties available on the index directly:
no_2.index.year, no_2.index.month_name

Some other advantages are the convenient subsetting of time period or the adapted time scale on plots. Let’s apply this on our data.

I want to Create a plot of the 𝑁𝑂2 values in the different stations from the 20th of May till the end of 21st of May
no_2['2019-05-20':'2019-05-21'].plot(figsize=(12,6))

By providing a string that parses to a datetime, a specific subset of the data can be selected on a DatetimeIndex.

RESAMPLE A TIME SERIES TO ANOTHER FREQUENCY
--------------------------------------------
Aggregate the current hourly(currently the dataset consists of hourly values) time series values to the monthly maximum value in each of the stations.

monthly_max = no_2.resample('M').max()
monthly_max

A very powerful method on time series data with a datetime index, is the ability to resample() time series to another frequency (e.g., converting secondly data into 5-minutely data).
The resample() method is similar to a groupby operation:
• it provides a time-based grouping, by using a string (e.g. M, 5H,. . . ) that defines the target frequency 
• it requires an aggregation function such as mean, max,. . .

no_2.resample('6H').mean().plot(figsize=(12,6))

An overview of the aliases used to define time series frequencies is given in the offset aliases overview table. When defined, the frequency of the time series is provided by the freq attribute:

monthly_max.index.freq # <MonthEnd>
monthly_max.index.freqstr # 'M'

I want to Make a plot of the daily mean 𝑁𝑂2 value in each of the stations.
no_2.resample('D').mean().plot(style='-o', figsize=(12,6))

REMEMBER
* Valid date strings can be converted to datetime objects using to_datetime function or as part of read functions.
* Datetime objects in pandas support calculations, logical operations and convenient date-related properties using the dt accessor.
* A DatetimeIndex contains these date-related properties and supports convenient slicing.
* Resample is a powerful method to change the frequency of a time series.

HOW TO MANIPULATE TEXTUAL DATA?
This tutorial uses the Titanic data set, stored as CSV.
titanic = pd.read_csv("data/titanic.csv")

Make all name characters lowercase.
To make each of the strings in the Name column lowercase, select the Name column, add the str accessor and apply the lower method. As such, each of the strings is converted element-wise.

titanic.Name.str.lower()

Create a new column Surname that contains the surname of the passengers by extracting the part before the comma.

titanic.Name.str.split(',')

Using the Series.str.split() method, each of the values is returned as a list of 2 elements. The first element is the part before the comma and the second element is the part after the comma.

titanic['Surname'] = titanic.Name.str.split(',').str.get(0)
titanic.head()

As we are only interested in the first part representing the surname (element 0), we can again use the str accessor and apply Series.str.get() to extract the relevant part. Indeed, these string functions can be concatenated to combine multiple functions at once!

I want to Extract the passenger data about the countesses on board of the Titanic.

titanic[titanic.Name.str.contains('Countess')]

The string method Series.str.contains() checks for each of the values in the column Name if the string contains the word Countess and returns for each of the values True (Countess is part of the name) or False (Countess is not part of the name). This output can be used to subselect the data using conditional (boolean) indexing introduced in the subsetting of data tutorial. As there was only one countess on the Titanic, we get one row as a result.

Note: More powerful extractions on strings are supported, as the Series.str.contains() and Series. str.extract() methods accept regular expressions,

Which passenger of the Titanic has the longest name?
To get the longest name we first have to get the lengths of each of the names in the Name column. By using pandas string methods, the Series.str.len() function is applied to each of the names individually (element-wise).

titanic.Name.str.len().idxmax()

Next, we need to get the corresponding location, preferably the index label, in the table for which the name length is the largest. The idxmax() method does exactly that. It is not a string method and is applied to integers, so no str is used.

titanic.iloc[titanic.Name.str.len().idxmax()]['Name']
titanic.loc[titanic.Name.str.len().idxmax()]['Name']

Based on the index name of the row (307) and the column (Name), we can do a selection using the loc or iloc operator.

In the “Sex” column, replace values of “male” by “M” and values of “female” by “F”.

titanic['Sex_short'] = titanic.Sex.replace({'male':'M', 'female':'F'})
titanic.head(2)

Whereas replace() is not a string method, it provides a convenient way to use mappings or vocabularies to translate certainvalues.It requires a dictionary to define the mapping{from : to}.

Warning: There is also a replace() method available to replace a specific set of characters. However, when having a mapping of multiple values, this would become:
titanic["Sex_short"] = titanic["Sex"].str.replace("female", "F")
titanic["Sex_short"] = titanic["Sex_short"].str.replace("male", "M")
This would become cumbersome and easily lead to mistakes. Just think (or try out yourself) what would happen if those two statements are applied in the opposite order.

REMEMBER
* String methods are available using the str accessor.
* String methods work element-wise and can be used for conditional indexing.
* The replace method is a convenient method to convert values according to a given dictionary.


=================================Out of the box================================
||Accessors||
=============
Pandas is a widely-used data analysis and manipulation library in Python. It provides numerous functions and methods to work with any type of data. There are also methods that work only with a specific data type. These methods are accessed through 4 accessors.
The accessors extend the capabilities of Pandas and provide specific operations. For instance, extracting the month from the date can be done using the dt accessor.
In this post, we will see various operations with 4 accessors of Pandas which are:
str: String data type
    Eg: Get only the row which have alphabets from a series.
            a = pd.Series(['a',10,'bcd1',20,'efs'])
            a[a.str.isalpha().replace(np.nan, False)]

cat: Categorical data type
    For categorical data it is more efficient to work with categorical datatype than using the object datatype. It makes a significant difference in terms of memory and speed especially when the data has low cardinality(i.e number of categories is low compared to the number of observations)

    cate = pd.Series(['A','B','A','A','B','C'], dtype='category')
    cate.cat.categories # Index(['A', 'B', 'C'], dtype='object')
    cate.cat.rename_categories({'A':1, 'B':2, 'C':3})
    #cate[0]='D' # ValueError: Cannot setitem on a Categorical with a new category, set the categories first
    cate.cat.add_categories('D', inplace=True)
    cate[0]='D'
    cate

dt: Datetime, Timedelta, Period data types
    dts = pd.Series(pd.date_range('2021.01.01', periods=5, freq='10D'))
        dts.dt.day
        dts.dt.year
        dts.dt.month
        dts.dt.date
        dts.dt.hour
        dts.dt.minute
        dts.dt.second
        dts.dt.weekday
        dts.dt.isocalendar().week
        dts.dt.is_month_start

sparse: Sparse data type

=====================================================================================

CHAPTER 2 - USER GUIDE
------------------------
2.1.1 Object creation

Creating a Series by passing a list of values, letting pandas create a default integer index:
s = pd.Series([1,3,5,np.nan,6,8])

pd.date_range() - Return a fixed frequency DatetimeIndex.
    start=None, end=None, periods=None, freq=None, tz=None, normalize=False, name=None, closed=None, **kwarg

Creating a DataFrame by passing a NumPy array, with a datetime index and labeled columns:
dates = pd.date_range("20210101", periods=6, freq='2D')
df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list("ABCD"))
df

df2 = pd.DataFrame({
    'A':1.0,
    'B':pd.Timestamp('20210102'),
    'C':pd.Series(1, index=list(range(4)), dtype='float32'),
    'D':np.array([3]*4, dtype='int32'),
    'E':pd.Categorical(['test', 'train', 'test', 'train', ]),
    'F':'foo',
})
df2.info()

The columns of the resulting DataFrame have different dtypes.
df2.dtypes


DataFrame.to_numpy() gives a NumPy representation of the underlying data. Note that this can be an expensive operation when your DataFrame has columns with different data types, which comes down to a fundamental differ- ence between pandas and NumPy: NumPy arrays have one dtype for the entire array, while pandas DataFrames have one dtype per column. When you call DataFrame.to_numpy(), pandas will find the NumPy dtype that can hold all of the dtypes in the DataFrame. This may end up being object, which requires casting every value to a Python object.

For df, our DataFrame of all floating-point values, DataFrame.to_numpy() is fast and doesn’t require copying data.
df.to_numpy()

For df2, the DataFrame with multiple dtypes, DataFrame.to_numpy() is relatively expensive.
df2.to_numpy()
array([[1.0, Timestamp('2021-01-02 00:00:00'), 1.0, 3, 'test', 'foo'],
       [1.0, Timestamp('2021-01-02 00:00:00'), 1.0, 3, 'train', 'foo'],
       [1.0, Timestamp('2021-01-02 00:00:00'), 1.0, 3, 'test', 'foo'],
       [1.0, Timestamp('2021-01-02 00:00:00'), 1.0, 3, 'train', 'foo']],
      dtype=object)

Note: DataFrame.to_numpy()does not include the index orc olumn labels in the output.

2.1.2 Viewing data
-------------------
Transposing your data:
df.T

Sorting by an axis:
df.sort_index(axis=1, ascending=False)

                D	        C	        B	        A
2021-01-01	-0.030514	-0.875529	-0.055480	-0.503135
2021-01-03	0.122823	-0.488960	-1.026299	-0.982295

df.sort_index(ascending=False)

                A	        B	        C	        D
2021-01-11	-0.537115	0.030939	-0.311905	0.598476
2021-01-09	-0.873883	0.678836	-0.255343	-0.826411

Sorting by values:
df.sort_values('B')

2.1.3 Selection
----------------
Note: While standard Python / Numpy expressions for selecting and setting are intuitive and come in handy for interactive work, for production code, we recommend the optimized pandas data access methods, .at, .iat, .loc and .iloc.

Getting
  Selecting a single column, which yields a Series, equivalent to df.A:
  df['A']

 Selecting via [], which slices the rows.
  df[0:3]
  df["20210102":"20210106"]

 Selection by label
 ------------------
 For getting a cross section using a label:
 df.loc[dates[0]]

Selecting on a multi-axis by label:
df[['A','B']] ~ df.loc[:,['A','B']]

Showing label slicing, both endpoints are included:
df.loc['20210101':'20210105',['A','C']]

Reduction in the dimensions of the returned object:
df.loc['2021-01-03',['A','C']]

For getting a scalar value
df.loc[dates[0], 'A'] # -0.5031

For getting fast access to a scalar (equivalent to the prior method):
df.at[dates[0], 'A'] # -0.5031

which one is faster?
%timeit df.loc[dates[0], 'A'] # 30.3 µs ± 1.27
%timeit df.at[dates[0], 'A'] # 23.4 µs ± 5.19 µs

Selection by position(index)
-----------------------------
Select via the position of the passed integers:
df.iloc[3]

By integer slices, acting similar to numpy/Python:
df.iloc[2:5, 2:4]

By lists of integer position locations, similar to the NumPy/Python style:
df.iloc[[1,2,4], [0,2]]

For slicing rows explicitly:
df.iloc[2:5,]

For slicing columns explicitly:
df.iloc[:, 2:4]

For getting a value explicitly:
df.iloc[2,2] # 0.79

For getting fast access to a scalar (equivalent to the prior method):
df.iat[2,2] # 0.790

Boolean indexing
-----------------
Using a single column’s values to select data.
df[df['A']>0]

Selecting values from a DataFrame where a boolean condition is met.
df[df>0]

Using the isin() method for filtering:
df2 = df.copy()
df2['E'] = [ 'one', 'one', 'two', 'three', 'four', 'three', ]
df2[df2.E.isin(['two', 'four'])]

Setting
-------
Setting a new column automatically aligns the data by the indexes.
s1 = pd.Series(np.arange(1,7), index=pd.date_range('20210101', periods=6))
s1

Setting values by label:
df.at[dates[0], 'A'] = 0

Setting values by position:
df.iat[0, 1] = 0

Setting by assigning with a NumPy array:
df['D'] = np.array([3] * len(df))

A where operation with setting.
df2 = df.copy()
df2[df2>0] = -df2

df2 = df.copy()
df2 = df2.where(df2<0,-df2)

2.1.4 Missing data
-------------------
pandas primarily uses the value np.nan to represent missing data. It is by default not included in computations.

Reindexing allows you to change/add/delete the index on a specified axis. This returns a copy of the data.

reindex - assign new index to existing dataframe.

df1 = df.reindex(index=dates[0:4], columns=list(df.columns)+['E'])
df1.loc[dates[0:2], 'E'] = 1

To drop any rows that have missing data.
df.dropna(how = 'any') ~ df.dropna()

which one is faster?
%timeit df.dropna() # 1.59 ms ± 13.5 µs per loop
%timeit df.dropna(how = 'any') # 1.67 ms ± 50.9 µs per loop

%timeit titanic.dropna() # 1.97 ms ± 117 µs per loop
%timeit titanic.dropna(how='any') # 2.2 ms ± 196 µs per loop

Filling missing data.
df1.fillna(value=5) ~ df1.fillna(5)

To get the boolean mask where values are nan.
pd.isna(df1)

2.1.5 Operations
----------------
Stats
Operations in general exclude missing data.
df.mean() # To get mean of each column

Same operation on the other axis:
df.mean(axis=1) ~ df.mean(1) # To get mean of each row
df.mean.mean() # To get a scaler which is mean of complete dataframe

Operating with objects that have different dimensionality and need alignment. In addition, pandas automatically broadcasts along the specified dimension.

s = pd.Series([1,3,5,np.nan,6,8], index=dates).shift(2)
df.sub(s, axis='index')
df.rsub(s, axis='index')

Apply
------
Applying functions to the data:
df.apply(np.cumprod)

df.apply(lambda x: x.max() - x.min())
df.apply(lambda x: x.max() - x.min(), axis=1)

Histogramming:
get the frequency.

s = pd.Series(np.random.randint(0,7,size=10))
s.value_counts()

String Methods
--------------
Series is equipped with a set of string processing methods in the str accessor that make it easy to operate on each element of the array, as in the code snippet below. Note that pattern-matching in str generally uses regular expressions by default (and in some cases always uses them).

s = pd.Series(['A','B','V','CABA',np.nan, 'RAVA'])
s.str.lower()

2.1.6 Merge
------------
Concat
------
pandas provides various facilities for easily combining together Series and DataFrame objects with various kinds of set logic for the indexes and relational algebra functionality in the case of join / merge-type operations.
Concatenating pandas objects together with concat():

df = pd.DataFrame(np.random.randn(10,4))
# let's break it into pieces
pieces = [df[:3], df[3:7], df[7:]]
pd.concat(pieces)

Note: Adding a column to a DataFrame is relatively fast. However, adding a row requires a copy, and may be expensive. We recommend passing a pre-built list of records to the DataFrame constructor instead of building a DataFrame by iteratively appending records to it.

Join
-----
SQL style merges
left = pd.DataFrame({"key": ["foo", "foo"], "lval": [1, 2]})
right = pd.DataFrame({"key": ["foo", "foo"], "rval": [4, 5]})
pd.merge(left, right, on="key")

2.1.7 Grouping
--------------
By “group by” we are referring to a process involving one or more of the following steps: • Splitting the data into groups based on some criteria
• Applying a function to each group independently
• Combining the results into a data structure

df = pd.DataFrame(
    {
    "A": ["foo", "bar", "foo", "bar", "foo", "bar", "foo", "foo"],
    "B": ["one", "one", "two", "three", "two", "two", "one", "three"],
    "C": np.random.randn(8),
    "D": np.random.randn(8),
    }
)
Grouping and then applying the sum() function to the resulting groups.
df.groupby(["A", "B"]).sum()

2.1.8 Reshaping
---------------
tuples = list(zip(*[
    ["bar", "bar", "baz", "baz", "foo", "foo", "qux", "qux"],
    ["one", "two", "one", "two", "one", "two", "one", "two"],
]))

index = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])
df = pd.DataFrame(np.random.randn(8,2), index=index, columns=['A','B'])
df2 = df[:4]
df2

The stack() method “compresses” a level in the DataFrame’s columns.
stacked = df2.stack()
stacked
(It is similar to melt but it maintains index as well)
first  second   
bar    one     A    0.709423
               B   -0.276359
       two     A    0.771991
               B   -0.523291
baz    one     A    0.033710
               B   -0.099103
       two     A   -0.046292
               B    0.984227
dtype: float64

With a “stacked” DataFrame or Series (having a MultiIndex as the index), the inverse operation of stack() is unstack(), which by default unstacks the last level:

stacked.unstack()
                		A	       B
first	second		
bar	    one	        0.709423	-0.276359
        two	        0.771991	-0.523291
baz	    one	        0.033710	-0.099103
        two	       -0.046292	0.984227

stacked.unstack([0,1])
first	bar	baz
second	one	two	one	two
A	0.709423	0.771991	0.033710	-0.046292
B	-0.276359	-0.523291	-0.099103	0.984227

Pivot tables
------------
Reshape - Long to Wide

df = pd.DataFrame({
    "A": ["one", "one", "two", "three"] * 3,
    "B": ["A", "B", "C"] * 4,
    "C": ["foo", "foo", "foo", "bar", "bar", "bar"] * 2,
    "D": np.random.randn(12),
    "E": np.random.randn(12),
})
df.pivot_table(columns=['C'], values='D', index=['A','B'])

2.1.9 Time series
------------------
pandas has simple, powerful, and efficient functionality for performing resampling operations during frequency con- version (e.g., converting secondly data into 5-minutely data). This is extremely common in, but not limited to, financial applications.

rng = pd.date_range('2021/01/01', periods=100, freq='S')
ts = pd.Series(np.random.randint(0,500,len(rng)), index=rng)
ts.resample('5M').mean()

Time zone representation:
rng = pd.date_range('3/6/2020 00:00', periods=5)
ts = pd.Series(np.random.randn(len(rng)), index=rng)
ts_utc = ts.tz_localize('UTC')
Converting to another time zone:
ts_utc.tz_convert('US/Eastern')

Converting between time span representations:
rng = pd.date_range('3/6/2020 00:00', periods=5, freq='M')
ts = pd.Series(np.random.randn(len(rng)), index=rng)
ps = ts.to_period()
ps.to_timestamp()

Converting between period and timestamp enables some convenient arithmetic functions to be used. In the following example, we convert a quarterly frequency with year ending in November to 9am of the end of the month following the quarter end:

prng = pd.period_range('1990Q1', '2000Q4', freq='Q-NOV')
ts = pd.Series(np.random.randn(len(prng)), index=prng)
ts.index = (prng.asfreq('M','e')+1).asfreq('H','s')+9
ts

Get the first date od provided month.
pd.Timestamp('2020-01', freq='M').to_period().to_timestamp()

2.1.10 Categoricals
-------------------
pandas can include categorical data in a DataFrame.

df = pd.DataFrame({
    'id':np.arange(1,7),
    'raw_grade': [ 'a', 'b', 'b', 'a', 'a', 'e', ],
})

Convert the raw grades to a categorical data type.
df['grade']=df['raw_grade'].astype('category')

Rename the categories to more meaningful names (assigning to Series.cat.categories() is in place!).
df['grade'].cat.categories = ['vg', 'g', 'b']

Reorder the categories and simultaneously add the missing categories (methods under Series.cat() return a new Series by default).
df['grade'] = df['grade'].cat.set_categories(['vb','b','m','g','vg'])

Sorting is per order in the categories, not lexical order.
df.sort_values('grade')

Grouping by a categorical column also shows empty categories.
df.groupby('grade').count()

2.1.11 Plotting
----------------
We use the standard convention for referencing the matplotlib API:
ts = pd.Series(np.random.randn(1000), index=pd.date_range(
    '2019.01.01', periods=1000, ))
ts = ts.cumsum()
ts.plot()

On a DataFrame, the plot() method is a convenience to plot all of the columns with labels:
df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, 
                  columns=['A','B','C','D'])
df = df.cumsum()

plt.figure()
df.plot(figsize=(12,6))
plt.legend(loc='best')

2.1.12 Getting data in/out
CSV
Writing to a csv file. - df.to_csv("foo.csv")
Reading from a csv file. - pd.read_csv("foo.csv")

2.2 Intro to data structures
============================
We’ll start with a quick, non-comprehensive overview of the fundamental data structures in pandas to get you started. The fundamental behavior about data types, indexing, and axis labeling / alignment apply across all of the objects.

Here is a basic tenet to keep in mind: DATA ALIGNMENT IS INTRINSIC. The link between labels and data will not be broken unless done so explicitly by you.

===========What is Vectorization ===========================
https://realpython.com/numpy-array-programming/#getting-into-shape-intro-to-numpy-arrays

This practice of replacing explicit loops with array expressions is commonly referred to as vectorization. In general, vectorized array operations will often be one or two (or more) orders of magnitude faster than their pure Python equivalents, with the biggest impact [seen] in any kind of numerical computations.

When looping over an array or any data structure in Python, there’s a lot of overhead involved. Vectorized operations in NumPy delegate the looping internally to highly optimized C and Fortran functions, making for cleaner and faster Python code.

consider a 1-dimensional vector of True and False for which you want to count the number of “False to True” transitions in the sequence:
In simple words we need to count the number of cases in which True comes after False.
Eg:
x = np.array([False, False, True, True,False])
np.cunt_nonzero(x[:-1]<x[1:]) # 1 - only once True comes after false at position 3

Classic Python:
def get_count(x) -> int:
    count = 0
    for i,j in list(zip(x[:-1], x[1:])):
        if j and not i:
            count += 1
    return count
%timeit get_count(x) # 1.1 ms ± 19.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

numpy:
%timeit np.count_nonzero(x[:-1]<x[1:]) # 1.1 ms ± 19.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

Numpy solution is around 157 times faster.
from timeit import timeit

setup = 'from __main__ import count_transitions, x; import numpy as np'
num = 100
t1 = timeit('count_transitions(x)', setup = setup, number=num)
print(t1)
t2 = timeit('np.count_nonzero(x[:-1]<x[1:])', setup = setup, number=num)
print(t2)
print(f'Speed difference: {(t1/t2)}')

17.00445253200087
0.10794881399851874
Speed difference: 157.52329184676546

Question: Given a stock’s price history as a sequence, and assuming that you are only allowed to make one purchase and one sale, what is the maximum profit that can be obtained? For example, given prices = (20, 18, 14, 17, 20, 21, 15), the max profit would be 7, from buying at 14 and selling at 21.

6 solutions:
p = np.array([20, 18, 14, 17, 20, 21, 15])

def bruteforce(a) -> int:
    max_diff = 0
    for i in p:
        for j in p[1:]:
            t = i - j
            if t > max_diff:
                max_diff = t
    return max_diff

def ordern(a) -> int:
    max_val  = min_val = p[0]
    for i in p[1:]:
        if i>max_val:
            max_val = i
        elif i< min_val:
            min_val = i
    return max_val - min_val

def realordern(p) -> int:
    diff_px = 0
    min_px = p[0]
    for px in p[1:]:
        min_px = min(min_px, px)
        diff_px = max(px-min_px, diff_px)
    return diff_px

def basic(p) -> int:
    return max(p)-min(p)

from timeit import timeit

setup = 'from __main__ import realordern, ordern, p, basic, bruteforce; import numpy as np'
num = 10000
t6 = timeit('np.max(p - np.minimum.accumulate(p))', setup = setup, number=num)
print('numpy accumulate', t6)
t5 = timeit('np.max(p) - np.min(p)', setup = setup, number=num)
print('numpy max min', t5)
t4 = timeit('basic(p)', setup = setup, number=num)
print('python max min', t4)
t1 = timeit('realordern(p)', setup = setup, number=num)
print('solution suggested by realPython', t1)
t2 = timeit('ordern(p)', setup = setup, number=num)
print('My if else solution', t2)
t3 = timeit('bruteforce(p)', setup = setup, number=num)
print('BruteForce way ', t3)
print(min(t1, t2, t3, t4, t5 ,t6))

numpy accumulate 0.13581958799841232
numpy max min 0.10029465600018739
python max min 0.03708621999976458
solution suggested by realPython 0.051151576000847854
My if else solution 0.02522610599771724
BruteForce way  0.21499461300118128
0.02522610599771724

AXES can be confusing term so in short - summing an array for axis=0 collapses the rows of the array with a column-wise computation.

df = DataFrame({
    'a':[10,20],
    'b':[100, 200]
})
print(df)
df.sum(axis=0)

    a    b
0  10  100
1  20  200
a     30
b    300

random.normal(loc=0.0, scale=1.0, size=None)
Draw random samples from a normal (Gaussian) distribution.
The probability density function of the normal distribution, first derived by De Moivre and 200 years later by both Gauss and Laplace independently [2], is often called the bell curve because of its characteristic shape (see the example below).
The normal distributions occurs often in nature. For example, it describes the commonly occurring distribution of samples influenced by a large number of tiny, random disturbances, each with its own unique distribution [2].

http://scipy-lectures.org/intro/numpy/array_object.html#what-are-numpy-and-numpy-arrays

Interactive help for numpy
np.array? --> Interactive help
------------
Docstring:
array(object, dtype=None, *, copy=True, order='K', subok=False, ndmin=0)
Create an array.

Looking for something:
np.lookfor('create a random array')
-----------------------------------
Search results for 'create a random array'
------------------------------------------
numpy.matlib.rand
    Return a matrix of random values with given shape.

np.con*?
-----------
np.concatenate
np.conj
np.conjugate
np.convolve
===================================================================================



2.2.1 Series
=============
Series is a one-dimensional labeled array capable of holding any data type (integers, strings, floating point numbers, Python objects, etc.). The axis labels are collectively referred to as the index. The basic method to create a Series is to call:

s = pd.Series(data, index=index)

Here, data can be many different things: • a Python dict
• an ndarray
• a scalar value (like 5)
The passed index is a list of axis labels. Thus, this separates into a few cases depending on what data is:

From ndarray
If data is an ndarray, index must be the same length as data. If no index is passed, one will be created having values [0, ..., len(data) - 1].

s = pd.Series(np.arange(5), index=['a','b','c','d','e'])
s.index

pd.Series(np.arange(5))

Note: pandas supports non-unique index values. If an operation that does not support duplicate index values is attempted, an exception will be raised at that time. The reason for being lazy is nearly all performance-based (there are many instances in computations, like parts of GroupBy, where the index is not used).

## DUPLICATE INDEX
s = pd.Series(np.arange(5), index=['a','b','a','d','e']) 

From dict
Series can be instantiated from dicts:
s = pd.Series({
    'b':1,
    'a':0,
    'c':2,
})

Note: When the data is a dict, and an index is not passed, the Series index will be ordered by the dict’s insertion order, if you’re using Python version >= 3.6 and pandas version >= 0.23.

pd.Series(s, index=['b','c','d','a'])

Note: NaN (not a number)is the standard missing data marker used in pandas.

From scalar value
If data is a scalar value, an index can be provided. The value will be repeated to match the length of index.
pd.Series(5)
pd.Series(5, index=['b','c','d','a'])

Series is ndarray-like
Series acts very similarly to a ndarray, and is a valid argument to most NumPy functions. However, operations such as slicing will also slice the index.

s[0] is s['a'] ## False
s[0] == s['a'] ## True

print(id(s[0])) # 4949691408
id(s['a']) # 4949692112

s[:3]

s[s>s.median()]

s[[4,3,1]]

np.exp(s)

Like a NumPy array, a pandas Series has a dtype.
s.dtype # dtype('float64')

This is often a NumPy dtype. However, pandas and 3rd-party libraries extend NumPy’s type system in a few places, in which case the dtype would be an ExtensionDtype. Some examples within pandas are Categorical data and Nullable integer data type. 

If you need the actual array backing a Series, use Series.array.
s.array
<PandasArray>
[   0.9662603314256619,  -0.10565599919230215, -0.011807389416360094,
   -0.3584626404849454,   -1.4087244659737785]
Length: 5, dtype: float64

Accessing the array can be useful when you need to do some operation without the index (to disable automatic alignment, for example).
type(s.array) # pandas.core.arrays.numpy_.PandasArray

Series.array will always be an ExtensionArray. Briefly, an ExtensionArray is a thin wrapper around one or more concrete arrays like a numpy.ndarray. pandas knows how to take an ExtensionArray and store it in a Series or a column of a DataFrame.

While Series is ndarray-like, if you need an actual ndarray, then use Series.to_numpy().
s.to_numpy()

Even if the Series is backed by a ExtensionArray, Series.to_numpy() will return a NumPy ndarray.

Series is dict-like
--------------------
A Series is like a fixed-size dict in that you can get and set values by index label:

s['a'] # 0.9662603314256619
'e' in s # True
'f' in s # False

If a label is not contained, an exception is raised:
s['f'] # KeyError: 'f'

Using the get method, a missing label will return None or specified default:
s.get('f')
s.get('f', 10) # 10
s.get('f', np.nan) nan

Vectorized operations and label alignment with Series
------------------------------------------------------
When working with raw NumPy arrays, looping through value-by-value is usually not necessary. The same is true when working with Series in pandas. Series can also be passed into most NumPy methods expecting an ndarray.

s+s
s * 2
np.exp(s)

A key difference between Series and ndarray is that operations between Series automatically align the data based on label. Thus, you can write computations without giving consideration to whether the Series involved have the same labels.

s1 = pd.Series(np.arange(5), index=['a','b','c','d','e'])
print(s1[:-1])
print(s1[1:])
s1[:-1] + s1[1:]
a    0
b    1
c    2
d    3
dtype: int64
b    1
c    2
d    3
e    4
dtype: int64
a    NaN
b    2.0
c    4.0
d    6.0
e    NaN
dtype: float64

a = np.arange(5)
print(a)
a[:-1] + a[1:]

[0 1 2 3 4]
array([1, 3, 5, 7])


The result of an operation between unaligned Series will have the union of the indexes involved. If a label is not found in one Series or the other, the result will be marked as missing NaN. Being able to write code without doing any explicit data alignment grants immense freedom and flexibility in interactive data analysis and research. The integrated data alignment features of the pandas data structures set pandas apart from the majority of related tools for working with labeled data.


Note: In general, we chose to make the default result of operations between differently indexed objects yield the union of the indexes in order to avoid loss of information. Having an index label, though the data is missing, is typically important information as part of a computation. You of course have the option of dropping labels with missing data via the dropna function.

Name attribute
---------------
Series can also have a name attribute:

s1 = pd.Series(np.arange(5), index=['a','b','c','d','e'], name='something')
print(s1)
print(s1.name) # something

The Series name will be assigned automatically in many cases, in particular when taking 1D slices of DataFrame as you will see below.
You can rename a Series with the pandas.Series.rename() method.

s2 = s1.rename('different')
s2.name # 'different'
print(id(s1)) # 4939539792
print(id(s2)) # 4939597952

Note that s1 and s2 refer to different objects.

You can also name an existing series with rename method.
s3 = pd.Series(np.arange(5), index=['a','b','c','d','e'],)
s3.rename('Ramesha') # Name: Ramesha, dtype: int64

2.2.2 DataFrame
----------------
DataFrame is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet or SQL table, or a dict of Series objects. It is generally the most commonly used pandas object. Like Series, DataFrame accepts many different kinds of input:
• Dict of 1D ndarrays, lists, dicts, or Series 
• 2-D numpy.ndarray
• Structured or record ndarray
• A Series
• Another DataFrame

--------------------------------------------------------------------------------
Structured or record ndarray - 

Structured arrays are ndarrays whose datatype is a composition of simpler datatypes organized as a sequence of named fields. For example,

x = np.array([
    ('Rex', 9, 81.0,),
    ('Fido', 3, 27.0,),
], dtype=[('name', 'U10'), ('age','i4'), ('weight', 'f4')]
)
x.dtype

Here x is a one-dimensional array of length two whose datatype is a structure with three fields: 
1. A string of length 10 or less named ‘name’, 
2. a 32-bit integer named ‘age’, and 
3. a 32-bit float named ‘weight’.

If you index x at position 1 you get a structure:
x[1] # ('Fido', 3, 27.)
x[1]['age'] # 3
You can access and modify individual fields of a structured array by indexing with the field name:
x[1]['age'] = 30
x[1] #  ('Fido', 30, 27.)

## Dataframe from structured array
df = DataFrame(x)
df

Structured datatypes are designed to be able to mimic ‘structs’ in the C language, and share a similar memory layout. They are meant for interfacing with C code and for low-level manipulation of structured buffers, for example for interpreting binary blobs. For these purposes they support specialized features such as subarrays, nested datatypes, and unions, and allow control over the memory layout of the structure.

Users looking to manipulate tabular data, such as stored in csv files, may find other pydata projects more suitable, such as xarray, pandas, or DataArray. These provide a high-level interface for tabular data analysis and are better optimized for that use. For instance, the C-struct-like memory layout of structured arrays in numpy can lead to poor cache behavior in comparison
---------------------------------------------------------------------------------

back to dataframe - 
Along with the data, you can optionally pass index (row labels) and columns (column labels) arguments. If you pass an index and / or columns, you are guaranteeing the index and / or columns of the resulting DataFrame. Thus, a dict of Series plus a specific index will discard all data not matching up to the passed index.
If axis labels are not passed, they will be constructed from the input data based on common sense rules.

df = DataFrame({
    'a':[10,20],
    'b':[100, 200]
}, index=['chandu', 'maghu']
)
df

Note: When the data is a dict,and columns is not specified,the DataFrame columns will be ordered by the dict’s insertion order, if you are using Python version >= 3.6 and pandas >= 0.23.

If you want to create a dataframe with a dict but different names:
d = {
    'a':[10,20],
    'b':[100, 200]
}
df = DataFrame(list(d.values()), columns=['Ramesh', 'Suresh'])
df

From dict of Series or dicts
----------------------------
The resulting index will be the union of the indexes of the various Series. If there are any nested dicts, these will first be converted to Series. If no columns are passed, the columns will be the ordered list of dict keys.

d = {
    'one': pd.Series(np.arange(1,4).astype(float), index=['a','b','c']),
    'two': pd.Series(np.arange(1,5).astype(float), index=['a','b','c','d']),
}
df = pd.DataFrame(d)
df

df = pd.DataFrame(d, index=['d','b','a'])

df = pd.DataFrame(d, index=['d','b','a'], columns=['two', 'three'])

The row and column labels can be accessed respectively by accessing the index and columns attributes:

Note: When a particular set of columns is passed along with a dict of data, the passed columns override the keys in the dict.

df.index # Index(['d', 'b', 'a'], dtype='object')
df.columns # Index(['two', 'three'], dtype='object')

From dict of ndarrays / lists
-----------------------------
The ndarrays must all be the same length. If an index is passed, it must clearly also be the same length as the arrays. If no index is passed, the result will be range(n), where n is the array length.

d = {"one": [1.0, 2.0, 3.0, 4.0], "two": [4.0, 3.0, 2.0, 1.0]}
pd.DataFrame(d, index=["a", "b", "c", "d"])

From structured or record array
-------------------------------
This case is handled identically to a dict of arrays.
data = np.zeros((2,), dtype=[("A", "i4"), ("B", "f4"), ("C", "a10")])
data[:] = [(1, 2.0, "Hello"), (2, 3.0, "World")]
pd.DataFrame(data)
pd.DataFrame(data, index=['on', 'tw'])
pd.DataFrame(data, columns=list('cab'.upper()))

Note: DataFrame is not intended to work exactly like a 2-dimensional NumPy ndarray.

From a list of dicts
---------------------
data2 = [{"a": 1, "b": 2}, {"a": 5, "b": 10, "c": 20}]
pd.DataFrame(data2)
pd.DataFrame(data2, index=["first", "second"])
pd.DataFrame(data2, columns=["a", "b"])

From a dict of tuples
---------------------
You can automatically create a MultiIndexed frame by passing a tuples dictionary.
d = {
    ("a", "b"): {("A", "B"): 1, ("A", "C"): 2},
    ("a", "a"): {("A", "C"): 3, ("A", "B"): 4},
    ("a", "c"): {("A", "B"): 5, ("A", "C"): 6},
    ("b", "a"): {("A", "C"): 7, ("A", "B"): 8},
    ("b", "b"): {("A", "D"): 9, ("A", "B"): 10},
}
DataFrame(d)

From a Series
-------------
The result will be a DataFrame with the same index as the input Series, and with one column whose name is the original name of the Series (only if no other column name provided).

From a list of namedtuples
---------------------------
The field names of the first namedtuple in the list determine the columns of the DataFrame. The remaining namedtuples (or tuples) are simply unpacked and their values are fed into the rows of the DataFrame. If any of those tuples is shorter than the first namedtuple then the later columns in the corresponding row are marked as missing values. If any are longer than the first namedtuple, a ValueError is raised.

Point = namedtuple('Point', 'x y')
DataFrame([Point(0,0), Point(0,1), Point(0,2)])

Point = namedtuple('Point3D', 'x y z')
DataFrame([Point(0,0,0), Point(0,1,1), Point(0,1,2)])

From a list of dataclasses
--------------------------
Passing a list of dataclasses is equivalent to passing a list of dictionaries.
Please be aware, that all values in the list should be dataclasses, mixing types in the list would result in a TypeError.

from dataclasses import make_dataclass
Point = make_dataclass('Point', [('x', int), ('y', int)])
pd.DataFrame([Point(0, 0), Point(0,1), Point(0,2)])

Missing data
Much more will be said on this topic in the Missing data section. To construct a DataFrame with missing data, we use np.nan to represent missing values. Alternatively, you may pass a numpy.MaskedArray as the data argument to the DataFrame constructor, and its masked entries will be considered missing.

Alternate constructors
-----------------------
DataFrame.from_dict --
----------------------
DataFrame.from_dict takes a dict of dicts or a dict of array-like sequences and returns a DataFrame. It operates like the DataFrame constructor except for the orient parameter which is 'columns' by default, but which can be set to 'index' in order to use the dict keys as row labels
pd.DataFrame.from_dict(dict([('A', np.arange(1,4)), ('B', np.arange(4,7))]))

If you pass orient='index', the keys will be the row labels. In this case, you can also pass the desired column names:
pd.DataFrame.from_dict(
    dict([('A', np.arange(1,4)), ('B', np.arange(4,7))]),
    orient = 'index',
    columns=['one', 'two', 'three'])

DataFrame.from_records
------------------------
DataFrame.from_records takes a list of tuples or an ndarray with structured dtype. It works analogously to the normal DataFrame constructor, except that the resulting DataFrame index may be a specific field of the structured dtype. For example:
x = np.array([
    ('Rex', 9, 81.0,),
    ('Fido', 3, 27.0,),
], dtype=[('name', 'U10'), ('age','i4'), ('weight', 'f4')]
)

pd.DataFrame.from_records(x)

Column selection, addition, deletion
-------------------------------------
You can treat a DataFrame semantically like a dict of like-indexed Series objects. Getting, setting, and deleting columns works with the same syntax as the analogous dict operations:

d = {
    'one': pd.Series(np.arange(1,4).astype(float), index=['a','b','c']),
    'two': pd.Series(np.arange(1,5).astype(float), index=['a','b','c','d']),
}
df = pd.DataFrame(d)
df['one']   ## Selection
df['three'] = df['one'] * df['two'] ## Column Addition
df['flag'] = df['one'] > 2  ## Column Addition
df

Columns can be deleted or popped like with a dict:
del df['two']   ## permanently deleted
three = df.pop('three')  ## Poped and deleated

When inserting a scalar value, it will naturally be propagated to fill the column:
df['foo'] = 'bar'

When inserting a Series that does not have the same index as the DataFrame, it will be conformed to the DataFrame’s index:
df['one_trunc'] = df["one"][:2]

If the index of the series to be added does not match with df, then np.nan is filled.
s = pd.Series(np.arange(1,4), index=['z','y','x'])
df1 = df
print(df1)
df1['trunc_one'] = s
df1

one	flag	    foo	trunc_one
a	1.0	False	bar	NaN
b	2.0	False	bar	NaN
c	3.0	True	bar	NaN
d	NaN	False	bar	NaN

You can insert raw ndarrays but their length must match the length of the DataFrame’s index.
By default, columns get inserted at the end. The insert function is available to insert at a particular location in the columns:
df.insert(1, 'bar', df["one"])

Assigning new columns in method chains
--------------------------------------
assign - DataFrame has an assign() method that allows you to easily create new columns that are potentially derived from existing columns.
assign always returns a copy of the data, leaving the original DataFrame untouched.
iris = pd.read_csv("data/iris.data")
iris.head()
iris.assign(SepalRation = iris.SepalWidth / iris.SepalLength).head()

In the example above, we inserted a precomputed value. We can also pass in a function of one argument to be evaluated on the DataFrame being assigned to.
iris.assign(SepalRatio=lambda x:(x['SepalWidth']/x['SepalLength'])).head()

Passing a callable, as opposed to an actual value to be inserted, is useful when you don’t have a reference to the DataFrame at hand. This is common when using assign in a chain of operations. For example, we can limit the DataFrame to just those observations with a Sepal Length greater than 5, calculate the ratio, and plot:

iris.query("SepalLength > 5").assign(
    SepalRatio = iris.SepalWidth / iris.SepalLength,
    PetalRatio = lambda x: x.PetalWidth / x.PetalLength
).plot(kind="scatter", x="SepalRatio", y="PetalRatio")

Since a function is passed in, the function is computed on the DataFrame being assigned to. Importantly, this is the DataFrame that’s been filtered to those rows with sepal length greater than 5. The filtering happens first, and then the ratio calculations. This is an example where we didn’t have a reference to the filtered DataFrame available.

The function signature for assign is simply **kwargs. The keys are the column names for the new fields, and the values are either a value to be inserted (for example, a Series or NumPy array), or a function of one argument to be called on the DataFrame. A copy of the original DataFrame is returned, with the new values inserted.
Starting with Python 3.6 the order of **kwargs is preserved. This allows for dependent assignment, where an expression later in **kwargs can refer to a column created earlier in the same assign().

dfa = pd.DataFrame({"A": [1, 2, 3], "B": [4, 5, 6]})
dfa.assign(C=lambda x:x.A+x.B, D=lambda x:x.A+x.C)

In the second expression, x['C'] will refer to the newly created column,that’s equal to dfa['A'] + dfa['B'].

Indexing / selection
---------------------
The basics of indexing are as follows:
Operation                           Syntax                      Result
Select column                       df[col]                     Series
Select row by label                 df.loc[label]               Series
Select row by integer location      df.iloc[loc]                Series
Slice rows                          df[5:10]                    DataFrame
Select rows by boolean vector       df[bool_vec]                DataFrame

Row selection, for example, returns a Series whose index is the columns of the DataFrame:
df.loc["b"]
df.iloc[2]

Data alignment and arithmetic
------------------------------

Data alignment between DataFrame objects automatically align on BOTH THE COLUMNS AND THE INDEX (ROW LABELS). Again, the resulting object will have the union of the column and row labels.
df1 = pd.DataFrame(np.random.randn(10, 4), columns=['A', 'B', 'C', 'D'])
df2 = pd.DataFrame(np.random.randn(7,3), columns=['A', 'B', 'C'])
df1 + df2

When doing an operation between DataFrame and Series, the default behavior is to align the Series index on the DataFrame columns, thus broadcasting row-wise. For example:
df1 - df1.iloc[0]

Operations with scalars are just as you would expect:
df = pd.DataFrame(np.arange(20).reshape(5,4), columns=['A', 'B', 'C', 'D'])
df*5+2
1/df
df**4

Boolean operators work as well:
df1 = pd.DataFrame({'a':[1,0,1], 'b':[0,1,1]}, dtype=bool)
df2 = pd.DataFrame({'a':[0,1,1], 'b':[1,1,0]}, dtype=bool)
df1 & df2
df1 | df2
df1 ^ df2
-df1

Transposing
To transpose, access the T attribute (also the transpose function), similar to an ndarray:
df[:3].T

DataFrame interoperability with NumPy functions
------------------------------------------------
Elementwise NumPy ufuncs (log, exp, sqrt, . . . ) and various other NumPy functions can be used with no issues on Series and DataFrame, assuming the data within are numeric:
np.exp(df)
np.asanyarray(df)

DataFrame is not intended to be a drop-in replacement for ndarray as its indexing semantics and data model are quite different in places from an n-dimensional array.
Series implements __array_ufunc__, which allows it to work with NumPy’s universal functions. The ufunc is applied to the underlying array in a Series.
s = pd.Series(np.arange(1,5).astype(int))
np.exp(s)

Changed in version 0.25.0: When multiple Series are passed to a ufunc, they are aligned before performing the operation.
Like other parts of the library, pandas will automatically align labeled inputs as part of a ufunc with multiple inputs. For example, using numpy.remainder() on two Series with differently ordered labels will align before the operation.
s1 = pd.Series(np.arange(1,4), index=['a', "b", 'c'])
s2 = pd.Series([1,3,5], index=['b', 'a', 'c'])
s1+s2
np.remainder(s1,s2)

As usual, the union of the two indices is taken, and non-overlapping values are filled with missing values.
s1 = pd.Series(np.arange(1,4), index=['a', "b", 'c'])
s2 = pd.Series([1,3,5], index=['b', 'a', 'c'])
s3 = pd.Series(np.arange(2,7,2), index=['b', 'c', 'd'])
s1+s2
np.remainder(s1, s2)
np.remainder(s1, s3)

When a binary ufunc is applied to a Series and Index, the Series implementation takes precedence and a Series is returned.
ser = pd.Series([1,2,3])
idx = pd.Index([4,5,6])
type(np.maximum(ser, idx)) # pandas.core.series.Series

NumPy ufuncs are safe to apply to Series backed by non-ndarray arrays, for example arrays.SparseArray. If possible, the ufunc is applied without converting the underlying data to an ndarray.

Console display
---------------
Very large DataFrames will be truncated to display them in the console. You can also get a summary using info().
However, using to_string will return a string representation of the DataFrame in tabular form, though it won’t always fit the console width:
baseball = pd.read_csv("data/baseball.csv")
baseball
baseball.info()
print(baseball.iloc[-20:,:12].to_string())

Wide DataFrames will be printed across multiple rows by default:
pd.DataFrame(np.random.randn(3,12))

You can change how much to print on a single row by setting the display.width option:
pd.set_option('display.width', 40)
pd.DataFrame(np.random.randn(3, 12))

You can adjust the max width of the individual columns by setting display.max_colwidth
pd.set_option("display.max_colwidth", 30)

You can also disable this feature via the expand_frame_repr option. This will print the table in one block.

DataFrame column attribute access and IPython completion
---------------------------------------------------------
If a DataFrame column label is a valid Python variable name, the column can be accessed like an attribute:
df = pd.DataFrame({"foo1": np.random.randn(5), "foo2": np.random.randn(5)})
df.foo1
The columns are also connected to the IPython completion mechanism so they can be tab-completed:
df.fo<TAB>

2.3 Essential basic functionality
==================================
Here we discuss a lot of the essential functionality common to the pandas data structures. To begin, let’s create some example objects like we did in the 10 minutes to pandas section:
s = pd.Series(np.random.randn(5), index=['a','b','c','d','e'])
index = pd.date_range('1.1.2000', periods=8)
df = pd.DataFrame(np.random.randn(8,3), index=index, columns=['A', 'B', 'C'])

2.3.1 Head and tail
-------------------
To view a small sample of a Series or DataFrame object, use the head() and tail() methods. The default number of elements to display is five, but you may pass a custom number.
long_series = pd.Series(np.random.randn(1000))
long_series.head()
long_series.tail(3)

2.3.2 Attributes and underlying data
-------------------------------------
pandas objects have a number of attributes enabling you to access the metadata • shape: gives the axis dimensions of the object, consistent with ndarray
• Axis labels
– Series: index (only axis)
– DataFrame: index (rows) and columns Note, these attributes can be safely assigned to!
df[:2]
df.columns = [col.lower() for col in df.columns]

pandas objects (Index, Series, DataFrame) can be thought of as containers for arrays, which hold the actual data and do the actual computation. For many types, the underlying array is a numpy.ndarray. However, pandas and 3rd party libraries may extend NumPy’s type system to add support for custom arrays (see dtypes).
To get the actual data inside a Index or Series, use the .array property
s.array
df.index.array

array will always be an ExtensionArray. The exact details of what an ExtensionArray is and why pandas uses them are a bit beyond the scope of this introduction. We will see it in future chapter.
s.to_numpy()
np.asarray(s)

--------------Extension Array-----------------

3.15.6 pandas.api.extensions.ExtensionArray
class pandas.api.extensions.ExtensionArray Abstract base class for custom 1-D array types.
pandas will recognize instances of this class as proper arrays with a custom type and will not attempt to coerce them to objects. They may be stored directly inside a DataFrame or Series.

----------------------------------------------

When the Series or Index is backed by an ExtensionArray, to_numpy() may involve copying data and coercing values. See dtypes for more.
to_numpy() gives some control over the dtype of the resulting numpy.ndarray. For example, consider date- times with timezones. NumPy doesn’t have a dtype to represent timezone-aware datetimes, so there are two possibly useful representations:
1. An object-dtype numpy.ndarray with Timestamp objects, each with the correct tz
2. A datetime64[ns] -dtype numpy.ndarray, where the values have been converted to UTC and the time-
zone discarded
Timezones may be preserved with dtype=object
ser = pd.Series(pd.date_range('2000', periods=2, tz='CET'))
ser # dtype: datetime64[ns, CET]
ser.to_numpy() # dtype=object
ser.to_numpy(dtype=object) # array([Timestamp('2000-01-01 00:00:00+0100', tz='CET', freq='D'),
                           # Timestamp('2000-01-02 00:00:00+0100', tz='CET', freq='D')],
                           # dtype=object)

Or Timezones can be thrown away with dtype='datetime64[ns]'
ser.to_numpy(dtype='datetime64[ns]') # array(['1999-12-31T23:00:00.000000000', '2000-01-01T23:00:00.000000000'],
                                      #dtype='datetime64[ns]')

Getting the “raw data” inside a DataFrame is possibly a bit more complex. When your DataFrame only has a single data type for all the columns, DataFrame.to_numpy() will return the underlying data:
df = pd.DataFrame(np.random.randn(8,3), index=index, columns=['A', 'B', 'C'])
df.columns = [col.lower() for col in df.columns]
df.to_numpy()

If a DataFrame contains homogeneously-typed data, the ndarray can actually be modified in-place, and the changes will be reflected in the data structure. For heterogeneous data (e.g. some of the DataFrame’s columns are not all the same dtype), this will not be the case. The values attribute itself, unlike the axis labels, cannot be assigned to.
Note: When working with heterogeneous data,the dtype of the resulting ndarray will be chosen to accommodate all of the data involved. For example, if strings are involved, the result will be of object dtype. If there are only floats and integers, the resulting array will be of float dtype.

In the past, pandas recommended Series.values or DataFrame.values for extracting the data from a Series or DataFrame. You’ll still find references to these in old code bases and online. Going forward, we recommend avoiding .values and using .array or .to_numpy(). .values has the following drawbacks:
  1. When your Series contains an extension type, it’s unclear whether Series.values returns a NumPy array or the extension array. Series.array will always return an ExtensionArray, and will never copy data. Series.to_numpy() will always return a NumPy array, potentially at the cost of copying / coercing values.
  2. When your DataFrame contains a mixture of data types, DataFrame.values may involve copying data and coercing values to a common dtype, a relatively expensive operation. DataFrame.to_numpy(), being a method, makes it clearer that the returned NumPy array may not be a view on the same data in the DataFrame.
--------------- Extension Types --------------------------

4.7.2 Extension types
pandas defines an interface for implementing data types and arrays that extend NumPy’s type system. pandas itself uses the extension system for some types that aren’t built into NumPy (categorical, period, interval, datetime with timezone).
Libraries can define a custom array and data type. When pandas encounters these objects, they will be handled properly (i.e. not converted to an ndarray of objects). Many methods like pandas.isna() will dispatch to the extension type’s implementation.
If you’re building a library that implements the interface, please publicize it on ecosystem.extensions. The interface consists of two classes.

ExtensionDtype
A pandas.api.extensions.ExtensionDtype is similar to a numpy.dtype object. It describes the data type. Implementors are responsible for a few unique items like the name.
One particularly important item is the type property. This should be the class that is the scalar type for your data. For example, if you were writing an extension array for IP Address data, this might be ipaddress.IPv4Address.
See the extension dtype source for interface definition.
New in version 0.24.0.
pandas.api.extension.ExtensionDtype can be registered to pandas to allow creation via a string dtype name. This allows one to instantiate Series and .astype() with a registered string name, for example 'category' is a registered string accessor for the CategoricalDtype.
See the extension dtype dtypes for more on how to register dtypes. ExtensionArray
This class provides all the array-like functionality. ExtensionArrays are limited to 1 dimension. An ExtensionArray is linked to an ExtensionDtype via the dtype attribute.
pandas makes no restrictions on how an extension array is created via its __new__ or __init__, and puts no restrictions on how you store your data. We do require that your array be convertible to a NumPy array, even if this is relatively expensive (as it is for Categorical).
They may be backed by none, one, or many NumPy arrays. For example, pandas.Categorical is an extension array backed by two arrays, one for codes and one for categories. An array of IPv6 addresses may be backed by a NumPy structured array with two fields, one for the lower 64 bits and one for the upper 64 bits. Or they may be backed by some other storage type, like Python lists.

----------------------------------------------------------

2.3.3 Accelerated operations
-----------------------------
pandas has support for accelerating certain types of binary numerical and boolean operations using the numexpr library and the bottleneck libraries.
These libraries are especially useful when dealing with large data sets, and provide large speedups. numexpr uses smart chunking, caching, and multiple cores. bottleneck is a set of specialized cython routines that are especially fast when dealing with arrays that have nans.
You are highly encouraged to install both libraries.
These are both enabled to be used by default, you can control this by setting the options:
 pd.set_option("compute.use_bottleneck", False)
 pd.set_option("compute.use_numexpr", False)

2.3.4 Flexible binary operations
---------------------------------

With binary operations between pandas data structures, there are two key points of interest:
• Broadcasting behavior between higher- (e.g. DataFrame) and lower-dimensional (e.g. Series) objects.
• Missing data in computations.
We will demonstrate how to manage these issues independently, though they can be handled simultaneously.

Matching / broadcasting behavior
--------------------------------
DataFrame has the methods add(), sub(), mul(), div() and related functions radd(), rsub(), ... for carrying out binary operations. For broadcasting behavior, Series input is of primary interest. Using these functions, you can use to either match on the index or columns via the axis keyword:
df = pd.DataFrame({
    'one':pd.Series(np.arange(1,4), index=['a','b','c',]),
    'two':pd.Series(np.arange(1,5), index=['a','b','c','d',]),
    'three':pd.Series(np.arange(1,4), index=['b','c','d']),
})
print(df)
row = df.iloc[1] # 2, 2, 1
column = df['two'] # 1, 2, 3, 4
print(df.sub(row, axis='columns'))
print(df.sub(row, axis=1))

df.sub(column, axis='index')
df.sub(column, axis=0)

Furthermore you can align a level of a MultiIndexed DataFrame with a Series.
dfmi = df.copy()
dfmi.index = pd.MultiIndex.from_tuples(
    [(1,'a'), (1,'b'), (1,'c'), (2, 'a')],
    names=['first', 'second']
    )
dfmi
column = df['two'] # 1, 2, 3, 4
dfmi.sub(column, axis=0, level=1)
dfmi.sub(column, axis=0, level="second")

Series and Index also support the divmod() builtin. This function takes the floor division and modulo operation at the same time returning a two-tuple of the same type as the left hand side. For example:
s = pd.Series(np.arange(0,31,10))
quo, rem = dm = divmod(s, 3)
print(dm)
print('*'*8)
print(rem)
print('*'*8)
print(quo)

idx = pd.Index(np.arange(0,31,10))
quo, rem = divmod(idx, 3)
quo
rem

We can also do elementwise divmod():
quo, rem = divmod(s, [1,2,3,5])

Missing data / operations with fill values
-------------------------------------------
In Series and DataFrame, the arithmetic functions have the option of inputting a fill_value, namely a value to substitute when at most one of the values at a location are missing. For example, when adding two DataFrame objects, you may wish to treat NaN as 0 unless both DataFrames are missing that value, in which case the result will be NaN (you can later replace NaN with some other value using fillna if you wish).
df1 = DataFrame({
    'a':[1,2,3,],
    'b':[11,22,33]
})

df2 = DataFrame({
    'a':[1,2,3,],
    'b':[11,22,33],
    'c':[10, 20, 30]
})
print(df1 + df2)
df1.add(df2, fill_value=0)
df1.add(df2, fill_value=1)
   a   b   c
0  2  22 NaN
1  4  44 NaN
2  6  66 NaN
a	b	c
0	2	22	10.0
1	4	44	20.0
2	6	66	30.0
	a	b	c
0	2	22	11.0
1	4	44	21.0
2	6	66	31.0

Flexible comparisons
---------------------
Series and DataFrame have the binary comparison methods eq, ne, lt, gt, le, and ge whose behavior is analogous to the binary arithmetic operations described above:
df1.gt(df2)
df1.ne(df2)

These operations produce a pandas object of the same type as the left-hand-side input that is of dtype bool. These boolean objects can be used in indexing operations.

Boolean reductions
-------------------
You can apply the reductions: empty, any(), all(), and bool() to provide a way to summarize a boolean result.
df1.gt(2).all(axis=1)
df1.gt(2).any(axis=1)

You can reduce to a final boolean value.
df1.gt(2).any(axis=1).any() # True

You can test if a pandas object is empty, via the empty property.
df1.empty # False
pd.DataFrame(columns=list('ABC')).empty # True

To evaluate single-element pandas objects in a boolean context, use the method bool():
pd.Series([True]).bool() # True
pd.DataFrame([False]).bool() # False

Warning: You might be tempted to do the following:
if df:
    pass

df1 and df2

These will both raise errors, as you are trying to compare multiple values.:
ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().

Comparing if objects are equivalent
------------------------------------
Often you may find that there is more than one way to compute the same result. As a simple example, consider df + df and df * 2. To test that these two computations produce the same result, given the tools shown above, you might imagine using(df + df == df * 2).all().But in fact,this expression is False:
( (df1 + df1) == (df1 * 2) ).all()

To get a single boolean value use
( (df1 + df1) == (df1 * 2) ).all().all()

Notice that the boolean DataFrame df + df == df * 2 contains some False values! This is because NaNs do not compare as equals:
np.nan == np.nan # False
So, NDFrames (such as Series and DataFrames) have an equals() method for testing equality, with NaNs in corre- sponding locations treated as equal.
pd.Series(np.nan).equals(pd.Series(np.nan)) # True
(df1*2).equals(df1 + df1) # True

Note that the Series or DataFrame index needs to be in the same order for equality to be True:
df1 = DataFrame({'a':['foo', 0, np.nan]})
df2 = DataFrame({'a':[0, 'foo', np.nan]}, index=[1,0,2])

df1.equals(df2) # False
df1.equals(df2.sort_index()) # True

Comparing array-like objects
-----------------------------
You can conveniently perform element-wise comparisons when comparing a pandas data structure with a scalar value:
pd.Series(["foo", "bar", "baz"]) == "foo"
pd.Index(["foo", "bar", "baz"]) == "foo"

pandas also handles element-wise comparisons between different array-like objects of the same length:
pd.Series(["foo", "bar", "baz"]) == pd.Index(["foo", "bar", "qux"])
pd.Series(["foo", "bar", "baz"]) == np.array(["foo", "bar", "qux"]) ## True True False
pd.Series(["foo", "bar", "baz"]).equals( np.array(["foo", "bar", "qux"]) ) # False

Trying to compare Index or Series objects of different lengths will raise a ValueError:
pd.Series(['foo', 'bar', 'baz']) == pd.Series(['foo', 'bar']) ValueError: Series lengths must match to compare
pd.Series(['foo', 'bar', 'baz']) == pd.Series(['foo']) ValueError: Series lengths must match to compare

Note that this is different from the NumPy behavior where a comparison can be broadcast:
np.array([1, 2, 3]) == np.array([2]) #  array([False, True, False])
or it can return False if broadcasting can not be done:
np.array([1, 2, 3]) == np.array([1, 2]) # False

Combining overlapping data sets
--------------------------------
A problem occasionally arising is the combination of two similar data sets where values in one are preferred over the other. An example would be two data series representing a particular economic indicator where one is considered to be of “higher quality”. However, the lower quality series might extend further back in history or have more complete data coverage. As such, we would like to combine two DataFrame objects where missing values in one DataFrame are conditionally filled with like-labeled values from the other DataFrame. The function implementing this operation is combine_first(), which we illustrate:
df1 = pd.DataFrame({
    'A':[ 1, np.nan, 3, 5, np.nan, ],
    'B':[ np.nan, 2, 3, np.nan, 6, ],
})

df2 = pd.DataFrame({
    "A": [5.0, 2.0, 4.0, np.nan, 3.0, 7.0],
    "B": [np.nan, np.nan, 3.0, 4.0, 6.0, 8.0],
})

df1.combine_first(df2)

General DataFrame combine
--------------------------
The combine_first() method above calls the more general DataFrame.combine(). This method takes another DataFrame and a combiner function, aligns the input DataFrame and then passes the combiner function pairs of Series (i.e., columns whose names are the same).
So, for instance, to reproduce combine_first() as above:
def combiner(x,y):
    return np.where(pd.isna(x),y,x)

df1.combine(df2, combiner)

2.3.5 Descriptive statistics
-----------------------------
There exists a large number of methods for computing descriptive statistics and other related operations on Series, DataFrame. Most of these are aggregations (hence producing a lower-dimensional result) like sum(), mean(), and quantile(), but some of them, like cumsum() and cumprod(), produce an object of the same size. Generally speaking, these methods take an axis argument, just like ndarray.{sum, std, . . . }, but the axis can be specified by name or integer:

• Series: no axis argument needed
• DataFrame: “index” (axis=0, default), “columns” (axis=1)

df.mean(0)
df.mean(1)

All such methods have a skipna option signaling whether to exclude missing data (True by default):
df.sum(0, skipna=False)
df.sum(0, skipna=True)

Combined with the broadcasting / arithmetic behavior, one can describe various statistical procedures, like standard- ization (rendering data zero mean and standard deviation of 1), very concisely:
standardization formula z = x - µ / std

ts_stand = (df - df.mean()) / df.std()
ts_stand.std()
ts_stand.mean()

xs_stand = df.sub(df.mean(1), axis=0).div(df.std(1), axis=0)
xs_stand.std(1)
xs_stand.mean(1)

Note that methods like cumsum() and cumprod() preserve the location of NaN values. This is somewhat different from expanding() and rolling() since NaN behavior is furthermore dictated by a min_periods parameter.
df.cumsum()

Here is a quick reference summary table of common functions. Each also takes an optional level parameter which applies only if the object has a hierarchical index.
Function             Description
count               Number of non-NA observations
sum                 Sum of values
mean                Mean of values
mad                 Mean absolute deviation
median              Arithmetic median of values
min                 Minimum
max                 Maximum
mode                Mode
abs                 Absolute Value
prod                Product of values
std                 Bessel-corrected sample standard deviation
var                 Unbiased variance
sem                 Standard error of the mean
skew                Sample skewness (3rd moment)
kurt                Sample kurtosis (4th moment)
quantile            Sample quantile (value at %)
cumsum              Cumulative sum
cumprod             Cumulative product
cummax              Cumulative maximum
cummin              Cumulative minimum

Note that by chance some NumPy methods, like mean, std, and sum, will exclude NAs on Series input by default:
np.mean(df.one) # 2.0
np.mean(df.one.to_numpy()) # nan

Series.nunique() will return the number of unique non-NA values in a Series:
series = pd.Series(np.arange(1,11))
series[6:] = np.nan
series[3:6] = 5
series.nunique()

Summarizing data: describe
----------------------------
There is a convenient describe() function which computes a variety of summary statistics about a Series or the columns of a DataFrame (excluding NAs of course):
series = pd.Series(np.random.randn(1000))
series[::2] = np.nan
series.describe()

frame = pd.DataFrame(np.random.randn(1000, 5), columns=["a", "b", "c", "d","e"])
frame.iloc[::2] = np.nan
frame.describe()

You can select specific percentiles to include in the output:
series.describe(percentiles=[0.05, 0.25, 0.75, 0.95])

By default, the median is always included.
For a non-numerical Series object, describe() will give a simple summary of the number of unique values and most frequently occurring values:
s = pd.Series(["a", "a", "b", "b", "a", "a", np.nan, "c", "d", "a"])
s.describe()

Note that on a mixed-type DataFrame object, describe() will restrict the summary to include only numerical columns or, if none are, only categorical columns:
frame = pd.DataFrame({"a": ["Yes", "Yes", "No", "No"], "b": range(4)})
frame.describe()

This behavior can be controlled by providing a list of types as include/exclude arguments. The special value all can also be used:
frame.describe(include=['object'])
frame.describe(include=['number'])
frame.describe(include='all')

That feature relies on select_dtypes.

Index of min/max values
------------------------
The idxmin() and idxmax() functions on Series and DataFrame compute the index labels with the minimum and maximum corresponding values:
s = pd.Series(np.random.choice(10, 5, replace=False))
0    4
1    6
2    7
3    8
4    0
s.idxmin(), s.idxmax() # (4, 3)

df1 = pd.DataFrame(
    np.random.choice(20, 15, replace=False).reshape(5,3),
    columns=['A', 'B', 'C']
)
df1
df1.idxmax(), df1.idxmin()
df1.idxmax(axis=1), df1.idxmin(axis=1)

When there are multiple rows (or columns) matching the minimum or maximum value, idxmin() and idxmax() return the first matching index:
df1 = pd.DataFrame(
    np.random.choice(20, 15).reshape(5,3),
    columns=['A', 'B', 'C']
)
df1

    A	B	C
0	4	5	13
1	19	12	16
2	1	4	13
3	19	19	8
4	13	6	15
df1['A'].idxmax(), df1.idxmin()
df1.idxmax().idxmax() - To get the index of the max value in a df

Note: idxmin and idxmax are called argmin and argmax in NumPy.

Value counts (histogramming) / mode
-------------------------------------
The value_counts() Series method and top-level function computes a histogram of a 1D array of values. It can also be used as a function on regular arrays:
data = np.random.randint(0,4, size=50)
s = pd.Series(data)
s.value_counts()
pd.value_counts(s)

The value_counts() method can be used to count combinations across multiple columns. By default all columns
are used but a subset can be selected using the subset argument.
data = {"a": [1, 2, 3, 4,1], "b": ["x", "x", "y", "y",'x']}
frame = pd.DataFrame(data)
frame.value_counts()
a  b
1  x    2
2  x    1

To find if a row is duplicated in a dataframe then we can go with duplicated method.
frame[frame.duplicated()]

So, if we just want to rows which is duplicated then we can go with duplicated method, if we want to counts also then we can use value_counts().
frame.value_counts()[frame.value_counts() > 1]

Similarly, you can get the most frequently occurring value(s), i.e. the mode, of the values in a Series or DataFrame:
s5 = pd.Series([1, 1, 3, 3, 3, 3, 5, 5, 7, 7, 7,])
s5.mode() #     0    3

df = pd.DataFrame({
    'A': np.random.randint(0,7, size=5),
    'B': np.random.randint(-10, 15, size=5)
})
df.mode()

Discretization and quantiling
-------------------------------
Continuous values can be discretized using the cut() (bins based on values) and qcut() (bins based on sample quantiles) functions:
arr = np.arange(1,9).astype(float)
factor = pd.cut(arr, [-5, -1, 0, 1, 5])
factor

qcut() computes sample quantiles. For example, we could slice up some normally distributed data into equal-size quartiles like so:
arr = np.arange(1,9).astype(float)
factor = pd.qcut(arr, [0, .25, .5, .75, 1])
factor
pd.value_counts(factor)

We can also pass infinite values to define the bins:
arr = np.arange(1,9).astype(float)
factor = pd.cut(arr, [-np.inf, 0, np.inf])
factor

2.3.6 Function application
==========================
To apply your own or another library’s functions to pandas objects, you should be aware of the three methods below. The appropriate method to use depends on whether your function expects to operate on an entire DataFrame or Series, row- or column-wise, or elementwise.
1. Tablewise Function Application: pipe()
2. Row or Column-wise Function Application: apply()
3. Aggregation API: agg() and transform()
4. Applying Elementwise Functions: applymap()

Tablewise function application
-------------------------------
DataFrames and Series can be passed into functions. However, if the function needs to be called in a chain, consider using the pipe() method.

df_p = pd.DataFrame({'city_and_code': ['Chicago, IL']})
df_p

def extract_city_name(df):
    """
        Chicago, IL -> Chicago for city_name column
    """
    df['city_name'] = df['city_and_code'].str.split(",").str.get(0)
    return df

def add_country_name(df, country_name=None):
    """
        Chicago -> Chicago-US for city_name column
    """
    col = 'city_name'
    df['city_and_country'] = df[col] + country_name
    return df

extract_city_name and add_country_name are functions taking and returning DataFrames.

Now compare the following:
add_country_name(extract_city_name(df_p), country_name='US')

Is equivalent to:
df_p.pipe(extract_city_name).pipe(add_country_name, country_name='US')

pandas encourages the second style, which is known as method chaining. pipe makes it easy to use your own or another library’s functions in method chains, alongside pandas’ methods.

In the example above, the functions extract_city_name and add_country_name each expected a DataFrame as the first positional argument. What if the function you wish to apply takes its data as, say, the second argument? In this case, provide pipe with a tuple of (callable, data_keyword). .pipe will route the DataFrame to the argument specified in the tuple.

For example, we can fit a regression using statsmodels. Their API expects a formula first and a DataFrame as the second argument,data.We pass in the function, keyword pair(sm.ols, 'data')to pipe:
bb.query("h>0").assign(ln_h=lambda df:np.log(df.h)).pipe(
    (sm.ols, 'data'), "hr ~ ln_h + year + g + C(lg)").fit().summary()

The pipe method is inspired by unix pipes and more recently dplyr and magrittr, which have introduced the popular (%>%) (read pipe) operator for R. The implementation of pipe here is quite clean and feels right at home in Python. We encourage you to view the source code of pipe().

Row or column-wise function application
----------------------------------------
Arbitrary functions can be applied along the axes of a DataFrame using the apply() method, which, like the descriptive statistics methods, takes an optional axis argument:

df.apply(np.mean)
df.apply(np.mean)
df.apply(np.mean, axis=1)
df.apply(lambda x:x.max() - x.min())
df.apply(np.cumsum)
df.apply(np.exp)

The apply() method will also dispatch on a string method name.
df.apply("mean")
df.apply("mean", axis=1)

The return type of the function passed to apply() affects the type of the final output from DataFrame.apply for the default behaviour:
• If the applied function returns a Series, the final output is a DataFrame. The columns match the index of the Series returned by the applied function.
• If the applied function returns any other type, the final output is a Series.

This default behaviour can be overridden using the result_type, which accepts three options: reduce,
broadcast, and expand. These will determine how list-likes return values expand (or not) to a DataFrame.

apply() combined with some cleverness can be used to answer many questions about a data set. For example,
suppose we wanted to extract the date where the maximum value for each column occurred:

tsdf.apply("idxmax")

You may also pass additional arguments and keyword arguments to the apply() method. For instance, consider the following function you would like to apply:
def sub_and_div(x, sub, div=1):
    return (x - sub)/div
df.apply(sub_and_div, args=(3,),div=2)

Another useful feature is the ability to pass Series methods to carry out some Series operation on each column or row:
df.apply(pd.Series.interpolate)
-----------------------------------------------------------------------
pd.Series.interpolate - Fill NaN values using an interpolation method.
-----------------------------------------------------------------------
Finally, apply() takes an argument raw which is False by default, which converts each row or column into a Series before applying the function. When set to True, the passed function will instead receive an ndarray object, which has positive performance implications if you do not need the indexing functionality.
df.apply(np.mean, raw=True)

Which one is faster?
%timeit df.apply(np.mean, raw=True) ## 312 µs ± 28.5 µs per loop
%timeit df.apply('mean') ## 727 µs ± 4.89 µs per loop

np is way faster, another example
%timeit df.apply(np.median, raw=True) # 423 µs ± 35.3 µs per loop, (mean ± std. dev. of 7 runs, 1000 loops each)
%timeit df.apply('median') ## 1.6 ms ± 49.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

Aggregation API
----------------
The aggregation API allows one to express possibly multiple aggregation operations in a single concise way. This API is similar across pandas objects, see groupby API, the window API, and the resample API. The entry point for aggregation is DataFrame.aggregate(), or the alias DataFrame.agg().
Using a single function is equivalent to apply(). You can also pass named methods as strings. These will return a Series of the aggregated output:
We will use a similar starting frame from above:
tsdf = pd.DataFrame(np.random.choice(20, 15, replace=False).reshape(5,3),
                    columns=['A', 'B', 'C'],
                   index = pd.date_range('2020.01.01', periods=5))
tsdf.iloc[2:4] = np.nan
tsdf.agg(np.sum)
tsdf.agg('sum')
# these are equivalent to a ``.sum()`` because we are aggregating # on a single function
tsdf.sum()

Single aggregations on a Series this will return a scalar value:
tsdf["A"].agg("sum") # 31.0

Aggregating with multiple functions
-----------------------------------
You can pass multiple aggregation arguments as a list. The results of each of the passed functions will be a row in the resulting DataFrame. These are naturally named from the aggregation function.
Multiple functions yield multiple rows:
tsdf.agg(['sum'])
tsdf.agg(['sum', 'mean'])

On a Series, multiple functions return a Series, indexed by the function names:
tsdf['A'].agg(['sum', 'mean'])

Passing a lambda function will yield a <lambda> named row:
tsdf['A'].agg(['sum', lambda x: x.mean()])
sum         31.000000
<lambda>    10.333333

Passing a named function will yield that name for the row:
def my_mean(x):
    return x.mean()
tsdf['A'].agg(['sum', my_mean])
sum        31.000000
my_mean    10.333333

Aggregating with a dict
------------------------
Passing a dictionary of column names to a scalar or a list of scalars, to DataFrame.agg allows you to customize which functions are applied to which columns. Note that the results are not in any particular order, you can use an OrderedDict instead to guarantee ordering.
tsdf.agg({
    'A':'mean',
    'B':'sum'
})

Passing a list-like will generate a DataFrame output. You will get a matrix-like output of all of the aggregators. The output will consist of all unique functions. Those that are not noted for a particular column will be NaN:
tsdf.agg({
    'A':['max', 'min'],
    'B':'sum'
})

Mixed dtypes
------------
When presented with mixed dtypes that cannot aggregate, .agg will only take the valid aggregations. This is similar to how .groupby.agg works.
mdf = pd.DataFrame({
    'A':[1,2,3],
    'B':[1.0,2.0,3.0],
    'C':['foo', 'bar', 'baz'],
    'D':pd.date_range('2020.01.01', periods=3, freq='2D')
})
mdf.dtypes
mdf.agg(['min', 'sum'])

Custom describe
-----------------
With .agg() it is possible to easily create a custom describe function, similar to the built in describe function.
from functools import partial
from functools import partial
q_25 = partial(pd.Series.quantile, q=0.25)
q_25.__name__ = "25%"
q_75 = partial(pd.Series.quantile, q=0.75)
q_75.__name__ = "75%"
tsdf.agg(["count", "mean", "std", "min", q_25, "median", q_75, "max"])

Transform API
===============
The transform() METHOD RETURNS AN OBJECT THAT IS INDEXED THE SAME (SAME SIZE) AS THE ORIGINAL. This API allows you to provide multiple operations at the same time rather than one-by-one. Its API is quite similar to the .agg API.
Transform the entire frame. .transform() allows input functions as: a NumPy function, a string function name or a user defined function.
tsdf.transform(np.abs)
tsdf.transform('abs')
tsdf.transform(lambda x: x.abs())

Here transform() received a single function; this is equivalent to a ufunc application.
np.abs(tsdf)

Passing a single function to .transform() with a Series will yield a single Series in return.
tsdf['A'].transform('abs')
tsdf['A'].agg('abs') # transform is similar to agg.

Transform with multiple functions
Passing multiple functions will yield a column MultiIndexed DataFrame. The first level will be the original frame column names; the second level will be the names of the transforming functions.
tsdf.transform([np.abs, lambda x:x+1])

                        A	                B	            C
            absolute	<lambda>	absolute	<lambda>	absolute	<lambda>
2021-01-01	4.0	            5.0	        12.0	13.0	        18.0	19.0
2021-01-04	7.0	            8.0	        3.0	    4.0	            5.0     12.0

Passing multiple functions to a Series will yield a DataFrame. The resulting column names will be the transforming functions.
tsdf['A'].transform([np.abs, lambda x:x+1])

Transforming with a dict
Passing a dict of functions will allow selective transforming per column.
tsdf.transform({"A": np.abs, "B": lambda x: x + 1})

Passing a dict of lists will generate a MultiIndexed DataFrame with these selective transforms.
tsdf.transform({"A": np.abs, "B": [lambda x: x + 1, "sqrt"]})

Difference between agg and transform?
https://stackoverflow.com/questions/40957932/transform-vs-aggregate-in-pandas#:~:text=1%20Answer&text=maybe%20you%20want%20these%20values,as%20what%20you%20started%20with.&text=agg%20is%20used%20when%20you,run%20on%20the%20same%20column.

consider the dataframe df
dft = pd.DataFrame(dict(A=list('aabb'), B=[1, 2, 3, 4], C=[0, 9, 0, 9]))

groupby is the standard use aggregater
dft.groupby('A').mean()

	B	C
A
a	1.5	4.5
b	3.5	4.5

maybe you want these values broadcast across the whole group and return something with the same index as what you started with.
use transform
dft.groupby('A').transform('mean')
	B	C
0	1.5	4.5
1	1.5	4.5
2	3.5	4.5
3	3.5	4.5

dft.set_index('A').groupby(level='A').transform('mean')
	B	C
A
a	1.5	4.5
a	1.5	4.5
b	3.5	4.5
b	3.5	4.5

agg is used when you have specific things you want to run for different columns or more than one thing run on the same column.
df.groupby('A').agg(['mean', 'std'])

        B	            C
    mean	std	    mean	std
A
a	1.5	0.707107	4.5	6.363961
b	3.5	0.707107	4.5	6.363961

dft.groupby('A').agg(dict(B='sum', C=['mean', 'prod']))

        B	        C
        sum	    mean	prod
A
a	    3	    4.5	    0
b	    7	    4.5	    0

Applying elementwise functions
===============================
Since not all functions can be vectorized (accept NumPy arrays and return another array or value), the methods applymap() on DataFrame and analogously map() on Series accept any Python function taking a single value and returning a single value. For example:
def f(x):
    return len(str(x))

tsdf.A.map(f)
tsdf.applymap(f)

Series.map() has an additional feature; it can be used to easily “link” or “map” values defined by a secondary series. This is closely related to merging/joining functionality:
s = pd.Series(["six", "seven", "six", "seven", "six"], index=["a", "b", "c", "d", "e"])
t = pd.Series({"six": 6.0, "seven": 7.0})
s.map(t)

2.3.7 Reindexing and altering labels
=====================================
reindex() is the fundamental data alignment method in pandas. It is used to implement nearly all other features relying on label-alignment functionality. To reindex means to conform the data to match a given set of labels along a particular axis. This accomplishes several things:
• Reorders the existing data to match a new set of labels
• Inserts missing value (NA) markers in label locations where no data for that label existed
• If specified, fill data for missing labels using logic (highly relevant to working with time series data)
Here is a simple example:
s = pd.Series(np.random.randn(5), index=["a", "b", "c", "d", "e"])
s.reindex(["e", "b", "f", "d"])

Here, the f label was not contained in the Series and hence appears as NaN in the result.
With a DataFrame, you can simultaneously reindex the index and columns:
df = DataFrame(
    np.random.randn(4, 3),
    columns=['one', 'two', 'three'],
    index=['a', 'b', 'c', 'd']
)
df.loc['a']['three'] = np.nan
df.loc['d']['one'] = np.nan
df

df.reindex(index=['c', 'f', 'a'], columns=['three', 'two', 'one'])

which one is faster to rearrange columns?
%timeit df.reindex(columns=['three', 'two', 'one']) # 435 µs ± 17.1 µs per loop
    %timeit df[['three', 'two', 'one']] # 572 µs ± 26.2 µs per loop

You may also use reindex with an axis keyword:
df.reindex(["c", "f", "b"], axis="index")

Note that the Index objects containing the actual axis labels can be shared between objects. So if we have a Series and a DataFrame, the following can be done:
rs = s.reindex(df.index)
rs.index is df.index # True

This means that the reindexed Series’s index is the same Python object as the DataFrame’s index.
DataFrame.reindex() also supports an “axis-style” calling convention, where you specify a single labels argument and the axis it applies to.
df.reindex(["c", "f", "b"], axis="index")
df.reindex(["three", "two", "one"], axis="columns")

Note: When writing performance-sensitive code, there is a good reason to spend some time becoming a reindexing ninja: many operations are faster on pre-aligned data. Adding two unaligned DataFrames internally triggers a reindexing step. For exploratory analysis you will hardly notice the difference (because reindex has been heavily optimized), but when CPU cycles matter sprinkling a few explicit reindex calls here and there can have an impact.

Reindexing to align with another object
----------------------------------------
You may wish to take an object and reindex its axes to be labeled the same as another object. While the syntax for this is straightforward albeit verbose, it is a common enough operation that the reindex_like() method is available to make this simpler:
This means, you can change the shape of the dataframe as per another dataframe.
In below example we are changing the shape of df(4,3) similar to df2(3,2)
df2 = DataFrame(
    np.random.randn(3,2),
    columns=['one', 'two'],
    index = ['a', 'b', 'c']
)
df2

df.reindex_like(df2)

Aligning objects with each other with align:
--------------------------------------------
The align() method is the fastest way to simultaneously align two objects. It supports a join argument (related to joining and merging):
• join='outer': take the union of the indexes (default)
• join='left': use the calling object’s index
• join='right': use the passed object’s index
• join='inner': intersect the indexes

It returns a tuple with both of the reindexed Series:
s = pd.Series(np.arange(1,6), index=['a', 'b', 'c', 'd', 'e'])
s1 = s[:4]
s2 = s[1:]
s1.align(s2)
s1.align(s2, join='inner')
s1.align(s2, join='left')

For DataFrames, the join method will be applied to both the index and the columns by default:
df2.align(df)
df2.align(df, join='inner')
df2.align(df, join='right')

You can also pass an axis option to only align on the specified axis:
df.align(df2, join='inner', axis=0)

If you pass a Series to DataFrame.align(), you can choose to align both objects either on the DataFrame’s index or columns using the axis argument:
df.align(df.iloc[0], axis=1)

Filling while reindexing
-------------------------
reindex() takes an optional parameter method which is a filling method chosen from the following table:
Method              Action
pad / ffill         Fill values forward
bfill / backfill    Fill values backward
nearest             Fill from the nearest index value

We illustrate these fill methods on a simple Series:
rng = pd.date_range('2021.01.01', periods=8)
ts = pd.Series(np.arange(1,9), index=rng)
ts2 = ts[[0, 3, 6]]
ts2.reindex(ts.index)
ts2.reindex(ts.index, method='ffill')
ts2.reindex(ts.index, method='bfill')
ts2.reindex(ts.index, method='nearest')

These methods require that the indexes are ordered increasing or decreasing.
Note that the same result could have been achieved using fillna (except for method='nearest') or interpolate:
ts2.reindex(ts.index).fillna(method="ffill")
ts2.reindex(ts.index).fillna(method="bfill")

reindex() will raise a ValueError if the index is not monotonically increasing or decreasing. fillna() and interpolate() will not perform any checks on the order of the index.
tst = pd.Series(np.arange(1,5), index=['4', '1', '3', '2'])
tst.reindex(index = ['2', '8', '4'], method='ffill') # ValueError
tst.reindex(index = ['2', '8', '4']).fillna(method='ffill')

Limits on filling while reindexing
------------------------------------
The limit and tolerance arguments provide additional control over filling while reindexing. Limit specifies the maximum count of consecutive matches:
ts2.reindex(ts.index, method='ffill', limit=1)

In contrast, tolerance specifies the maximum distance between the index and indexer value
ts2.reindex(ts.index, method='ffill', tolerance='1 day')

Notice that when used on a DatetimeIndex, TimedeltaIndex or PeriodIndex, tolerance will coerced into a Timedelta if possible. This allows you to specify tolerance with appropriate strings.

Dropping labels from an axis
-----------------------------
A method closely related to reindex is the drop() function. It removes a set of labels from an axis:
df.drop(['a', 'd'], axis=0)
df.drop(['two'], axis=1)
Note that the following also works, but is a bit less obvious / clean:
df.reindex(df.index.difference(['a', 'd']))

Renaming / mapping labels
--------------------------
The rename() method allows you to relabel an axis based on some mapping (a dict or Series) or an arbitrary function.
s = pd.Series(['ram', 'sur', 'cha', 'mag', 'pho'], index=['a', 'b', 'c', 'd', 'e'])
s.rename(str.upper) # renames the index

If you pass a function, it must return a value when called with any of the labels (and must produce a set of unique values). A dict or Series can also be used:
df.rename(columns={'one': 'foo', 'two':'bar'},
         index={'a':'aam', 'b':'badaaam', 'd':'doorhaiaam'}
         )

If the mapping doesn’t include a column/index label, it isn’t renamed. Note that extra labels in the mapping don’t throw an error.
Added 'e' to column but it didn't throw an error
df.rename(columns={'one': 'foo', 'two':'bar'},
         index={'a':'aam', 'b':'badaaam', 'd':'doorhaiaam', 'e':'eaam'}
         )
DataFrame.rename() also supports an “axis-style” calling convention, where you specify a single mapper and the axis to apply that mapping to.
df.rename({"one": "foo", "two": "bar"}, axis="columns")
df.rename({"a": "apple", "b": "banana", "d": "durian"}, axis="index")

The rename() method also provides an inplace named parameter that is by default False and copies the under- lying data. Pass inplace=True to rename the data in place.
Finally, rename() also accepts a scalar or list-like for altering the Series.name attribute.
s.rename('scaler-name')

The methods DataFrame.rename_axis() and Series.rename_axis() allow specific names of a MultiIndex to be changed (as opposed to the labels).
df = pd.DataFrame({
    'x':np.arange(1,7), 'y':np.arange(10,61,10)
}, index=pd.MultiIndex.from_product(
    [['a', 'b', 'c'], [1, 2]], names=['let', 'num']),
)
df.rename_axis(index={'let':'abc'})
df.rename_axis(index=str.upper)

================
2.3.8 Iteration
================
The behavior of basic iteration over pandas objects depends on the type. When iterating over a Series, it is regarded as array-like, and basic iteration produces the values. DataFrames follow the dict-like convention of iterating over the “keys” of the objects.
Inshort,basic iteration(for i in object)produces:
• Series: values
• DataFrame: column labels
Thus, for example, iterating over a DataFrame gives you the column names:
df = pd.DataFrame(
    np.arange(6).reshape(3,2),
    index = ['a', 'b', 'c'],
    columns=['col1', 'col2']
)
for col in df:
    print (col)

for val in s:
    print(val)

pandas objects also have the dict-like items() method to iterate over the (key, value) pairs. To iterate over the rows of a DataFrame, you can use the following methods:
• iterrows(): Iterate over the rows of a DataFrame as (index, Series) pairs. This converts the rows to Series objects, which can change the dtypes and has some performance implications.
• itertuples(): Iterate over the rows of a DataFrame as namedtuples of the values. This is a lot faster than iterrows(), and is in most cases preferable to use to iterate over the values of a DataFrame.
 Warning: Iterating through pandas objects is generally slow.In many cases,iterating manually over the rows is not needed and can be avoided with one of the following approaches:
• Look for a vectorized solution: many operations can be performed using built-in methods or NumPy functions, (boolean) indexing etc
• When you have a function that cannot work on the full Data Frame/Series at once,it is better to use apply() instead of iterating over the values.
• If you need to do iterative manipulations on the values but performance is important,consider writing the inner loop with cython or numba.

Warning: You should never modify something you are iterating over.This is not guaranteed to work in all cases. Depending on the data types, the iterator returns a copy and not a view, and writing to it will have no effect!
For example, in the following case setting the value has no effect:
df = pd.DataFrame({"a": [1, 2, 3], "b": ["a", "b", "c"]})
for index, row in df.iterrows():
    print(index)
    print(row)
    row['a'] = 10
df

items
------
Consistent with the dict-like interface, items() iterates through key-value pairs:
• Series: (index, scalar value) pairs
• DataFrame: (column, Series) pairs

for label, ser in df.items():
    print(ser)
    print(label)

iterrows():
-----------
iterrows() allows you to iterate through the rows of a DataFrame as Series objects. It returns an iterator yielding each index value along with a Series containing the data in each row:
for index,colNameValue in df.iterrows():
    print(index)
    print(colNameValue)

Note: Because iterrows() returns a Series for each row, it does not preserve dtypes across the rows (dtypes are preserved across columns for DataFrames). For example,
df_orig = pd.DataFrame([[1, 1.5]], columns=["int", "float"])
df_orig
df_orig.dtypes
row = next(df_orig.iterrows())[1]
print(row)
type(row) # pandas.core.series.Series
row['int'].dtype # dtype('float64')
df_orig['int'].dtype # dtype('int64')

---------next()--------------
The next() function returns the next item in an iterator.
You can add a default return value, to return if the iterable has reached to its end.
Syntax - next(iterable, default)
mylist = iter(["apple", "banana", "cherry"])
x = next(mylist, "orange")
print(x)
------------------------------

To preserve dtypes while iterating over the rows, it is better to use itertuples() which returns namedtuples of the values and which is generally much faster than iterrows().

For instance, a contrived way to transpose the DataFrame would be:
df2 = pd.DataFrame({"x": [1, 2, 3], "y": [4, 5, 6]})
df2.T

pd.DataFrame({index: colSeries for index, colSeries in df2.iterrows()}) ~ df2.T

itertuples
----------
The itertuples() method will return an iterator yielding a namedtuple for each row in the DataFrame. The first element of the tuple will be the row’s corresponding index value, while the remaining values are the row values.
For instance:
for row in df.itertuples():
    print(row)
Pandas(Index=0, a=1, b='a')
Pandas(Index=1, a=2, b='b')
Pandas(Index=2, a=3, b='c')

This method does not convert the row to a Series object; it merely returns the values inside a namedtuple. Therefore, itertuples() preserves the data type of the values and is generally faster as iterrows().
Note: The column names will be renamed to positional names if they are invalid Python identifiers,repeated,or start with an underscore. With a large number of columns (>255), regular tuples are returned.

---------------------
2.3.9 .dt accessor
---------------------
Series has an accessor to succinctly return datetime like properties for the values of the Series, if it is a date- time/period like Series. This will return a Series, indexed like the existing Series.
s = pd.Series(pd.date_range('2021.01.01 10:30:15', periods=4))
s.dt.second
This enables nice expressions like this:
s[s.dt.day == 2]
You can easily produces tz aware transformations:
s.dt.tz_localize('US/Eastern')
You can also chain these types of operations:
s.dt.tz_localize('UTC').dt.tz_convert('US/Eastern')

You can also format datetime values as strings with Series.dt.strftime() which supports the same format as the standard strftime().
datetime to object(str)
s = pd.Series(pd.date_range("20130101", periods=4))
s.dt.strftime('%Y/%m/%d')

The .dt accessor works for period and timedelta dtypes.
period to object(Str)
s = pd.Series(pd.period_range("20130101", periods=4))
s.dt.strftime('%Y/%m/%d')
s.dt.month
s.dt.day

timedelta:
s = pd.Series(pd.timedelta_range('1 day 00:05:10', periods=4, freq="s"))
s.dt.days
s.dt.seconds
s.dt.components

Hence different forms of dateTime for timeseries
* DatetimeIndex
* PeriodIndex
* Period
* timedelta

Note: Series.dt will raise a TypeError if you access with a non-datetime-like values.

2.3.10 Vectorized string methods
---------------------------------
Series is equipped with a set of string processing methods that make it easy to operate on each element of the array. Perhaps most importantly, these methods exclude missing/NA values automatically. These are accessed via the Series’s str attribute and generally have names matching the equivalent (scalar) built-in string methods. For example:
s = pd.Series( ["A", "B", "C", "Aaba", "Baca", np.nan, "CABA", "dog", "cat"], dtype="string")
s.str.lower()

Powerful pattern-matching methods are provided as well, but note that pattern-matching generally uses regular expres- sions by default (and in some cases always uses them).

2.3.11 Sorting
---------------
pandas supports three kinds of sorting: sorting by index labels, sorting by column values, and sorting by a combination of both.

By index
--------
The Series.sort_index() and DataFrame.sort_index() methods are used to sort a pandas object by its index levels.

df = pd.DataFrame({
    "one": pd.Series(np.random.choice(10, 3, replace=False), index=["a", "b", "c"]),
    "two": pd.Series(np.random.choice(10, 4, replace=False), index=["a", "b", "c", "d"]),
    "three": pd.Series(np.random.choice(10, 3, replace=False), index=["b", "c", "d"]),
})
df

unsorted_df = df.reindex(index=["a", "d", "c", "b"], columns=["three", "two", "one"])
print(unsorted_df)

unsorted_df.sort_index()
unsorted_df.sort_index(ascending=False)
unsorted_df.sort_index(axis=1) ## sorted column name alphabetically
unsorted_df["three"].sort_index() # Series

Sorting by index also supports a key parameter that takes a callable function to apply to the index being sorted. For
MultiIndex objects, the key is applied per-level to the levels specified by level.
s1 = pd.DataFrame({"a": ["B", "a", "C"], "b": [1, 2, 3], "c": [2, 3, 4]}
                 ).set_index(list('ab'))
s1.sort_index(level='a')
s1.sort_index(level="a", key=lambda idx: idx.str.lower())

By values
----------
The Series.sort_values() method is used to sort a Series by its values. The DataFrame. sort_values() method is used to sort a DataFrame by its column or row values. The optional by parameter to DataFrame.sort_values() may used to specify one or more columns to use to determine the sorted order.
df1 = pd.DataFrame({"one": [2, 1, 1, 1], "two": [1, 3, 2, 4], "three": [5, 4, 3, 2]})
df1.sort_values(by='two')

The by parameter can take a list of column names, e.g.:
df1.sort_values(by=['one', 'two'])

These methods have special treatment of NA values via the na_position argument:
s[2] = np.nan
s.sort_values() # places NAN at the end
s.sort_values(na_position='first')

Sorting also supports a key parameter that takes a callable function to apply to the values being sorted.
s1 = pd.Series(["B", "a", "C"])
s1.sort_values()
s1.sort_values(key=lambda x: x.str.lower())

key will be given the Series of values and should return a Series or array of the same shape with the transformed values. For DataFrame objects, the key is applied per column, so the key should still expect a Series and return a Series, e.g.
df = pd.DataFrame({"a": ["B", "a", "C"], "b": [1, 2, 3]})
df.sort_values(by="a", key=lambda col: col.str.lower())
The name or type of each column can be used to apply different functions to different columns.

By indexes and values
---------------------
Strings passed as the by parameter to DataFrame.sort_values() may refer to either columns or index level names.
idx = pd.MultiIndex.from_tuples(
[("a", 1), ("a", 2), ("a", 2), ("b", 2), ("b", 1), ("b", 1)])
idx.names=["first", "second"]
df_multi = pd.DataFrame({"A":np.arange(6,0,-1)}, index=idx)
df_multi.sort_values(by=['second', 'A'])

Note: If a string matches both a column name and an index level name then a warning is issued and the column takes precedence. This will result in an ambiguity error in a future version.

searchsorted
-------------
Series has the searchsorted() method, which works similarly to numpy.ndarray.searchsorted().

-----------np.searchsorted()------------------------
numpy.searchsorted(a, v, side='left', sorter=None)[source]
Find indices where elements should be inserted to maintain order.
Binary search is used to find the required insertion points.
np.searchsorted([1,2,3,4,5], 3) # 2
np.searchsorted([1,2,3,4,5], 3, side='right') # 3
np.searchsorted([1,2,3,4,5], [-10, 10, 2, 3]) # array([0, 5, 1, 2])
-----------------------------------------------------

ser = pd.Series([1, 2, 9])
ser.searchsorted(4) # 2, this is the index where 4 can be placed to maintain the sorted order of Series
ser.searchsorted([0, 6]) # array([0, 2])
ser.searchsorted([1,9]) # array([0, 2])
ser.searchsorted([1,9], side='right') # array([1, 3]), default being left
ser = pd.Series([3, 1, 2])
np.argsort([3,1,2]) # array([1, 2, 0]), smallest value at 1nth position
ser.searchsorted([0, 3], sorter=np.argsort(ser)) # array([0, 2])

smallest / largest values
--------------------------
Series has the nsmallest() and nlargest() methods which return the smallest or largest 𝑛 values. For a large Series this can be much faster than sorting the entire Series and calling head(n) on the result.
s = pd.Series(np.random.permutation(10))
s.sort_values()
s.nsmallest(3)
s.nlargest(3)
s.nlargest(3).sort_values()

DataFrame also has the nlargest and nsmallest methods.
df = pd.DataFrame({
    "a": [1,5,3,5,7,9,8],
    "b": list("abdceff"),
    "c": [1.0, 2.0, 4.0, 3.2, np.nan, 3.0, 4.0],
})
print(df.nlargest(5,'a'))
df.nlargest(5,['a', 'c'])
df.nsmallest(3,'c')

Sorting by a MultiIndex column
-------------------------------
You must be explicit about sorting when the column is a MultiIndex, and fully specify all levels to by.
df1.columns = pd.MultiIndex.from_tuples([("a", "one"), ("a", "two"), ("b", "three")])
df1.sort_values(by=("a", "two"))

2.3.12 Copying
---------------
The copy() method on pandas objects copies the underlying data (though not the axis indexes, since they are im- mutable) and returns a new object. Note that it is seldom necessary to copy objects. For example, there are only a handful of ways to alter a DataFrame in-place:
• Inserting, deleting, or modifying a column.
• Assigning to the index or columns attributes.
• For homogeneous data, directly modifying the values via the values attribute or advanced indexing.
To be clear, no pandas method has the side effect of modifying your data; almost every method returns a new object, leaving the original object untouched. If the data is modified, it is because you did so explicitly.

2.3.13 dtypes
-------------
For the most part, pandas uses NumPy arrays and dtypes for Series or individual columns of a DataFrame. NumPy provides support for float, int, bool, timedelta64[ns] and datetime64[ns] (note that NumPy does not support timezone-aware datetimes).
pandas and third-party libraries extend NumPy’s type system in a few places. This section describes the extensions pandas has made internally.
The following table lists all of pandas extension types. For methods requiring dtype arguments, strings can be specified as indicated. See the respective documentation sections for more on each type.

Kind of Data        Data Type       Scalar      Array                       String Aliases          Documentation
tz-aware datetime   DatetimeTZDtype Timestamp   arrays.DatetimeArray    'datetime64[ns, <tz>]'  Time zone handling
Categorical         CategoricalDtype (none)     Categorical             'category'               Categorical data
period (time spans) PeriodDtype     Period      arrays.PeriodArray  'period[<freq>]', 'Period[<freq>]'  Time span representation
sparse              SparseDtype     (none)      arrays.SparseArray      'Sparse', 'Sparse[int]', 'Sparse[float]'
Sparse data structures
intervals           IntervalDtype   Interval    arrays.IntervalArray
'interval', 'Interval', 'Interval[<numpy_dtype>]', 'Interval[datetime64[ns, <tz>]]', 'Interval[timedelta64[<freq>]]'
IntervalIndex

nullable integer    Int64Dtype, …   (none)      arrays.IntegerArray 'Int8', 'Int16', 'Int32', 'Int64', 'UInt8', 'UInt16', 'UInt32', 'UInt64'        Nullable integer data type

Strings             StringDtype         str     arrays.StringArray          'string'        Working with text data
Boolean (with NA)   BooleanDtype        bool    arrays.BooleanArray         'boolean'   Boolean data with missing values

pandas has two ways to store strings.
1. object dtype, which can hold any Python object, including strings.
2. StringDtype, which is dedicated to strings.
Generally, we recommend using StringDtype.

Finally, arbitrary objects may be stored using the object dtype, but should be avoided to the extent possible (for
performance and interoperability with other libraries and methods.

A convenient dtypes attribute for DataFrame returns a Series with the data type of each column.

dft = pd.DataFrame({
    "A": np.random.rand(3),
    "B": 1,
    "C": "foo",
    "D": pd.Timestamp("20010102"),
    "E": pd.Series([1.0] * 3).astype("float32"),
    "F": False,
    "G": pd.Series([1] * 3, dtype="int8"),
})
dft.dtypes

On a Series object, use the dtype attribute.
dft['A'].dtype

If a pandas object contains data with multiple dtypes in a single column, the dtype of the column will be chosen to accommodate all of the data types (object is the most general).
# these ints are coerced to floats
pd.Series([1, 2, 3, 4, 5, 6.0])

# string data forces an ``object`` dtype
pd.Series([1, 2, 3, 6.0, "foo"])

The number of columns of each type in a DataFrame can be found by calling DataFrame.dtypes. value_counts().
dft.dtypes.value_counts()

Numeric dtypes will propagate and can coexist in DataFrames. If a dtype is passed (either directly via the dtype keyword, a passed ndarray, or a passed Series), then it will be preserved in DataFrame operations. Furthermore, different numeric dtypes will NOT be combined. The following example will give you a taste.

df1 = pd.DataFrame(np.random.randn(8, 1), columns=["A"], dtype="float32")
df1.dtypes

df2 = pd.DataFrame({
    "A": pd.Series(np.random.randn(8), dtype="float16"),
    "B": pd.Series(np.random.randn(8)),
    "C": pd.Series(np.array(np.random.randn(8), dtype="uint8")),
})
df2.dtypes
## float64 is not combined with float16 in above example.

defaults
---------
By default integer types are int64 and float types are float64, regardless of platform (32-bit or 64-bit). The following will all result in int64 dtypes.

pd.DataFrame([1, 2], columns=["a"]).dtypes # a    int64
pd.DataFrame({"a": [1, 2]}).dtypes # a    int64
pd.DataFrame({"a": 1}, index=list(range(2))).dtypes # a    int64

Note that Numpy will choose platform-dependent types when creating arrays. The following WILL result in int32 on 32-bit platform.
frame = pd.DataFrame(np.array([1, 2]))

upcasting:
----------
Types can potentially be upcasted when combined with other types, meaning they are promoted from the current type (e.g. int to float).
print(df1.head())
print(df1.dtypes)
print('*'*10)
print(df2.head())
print(df2.dtypes)
df3 = df1.reindex_like(df2).fillna(value=0.0) + df2
df3.dtypes

DataFrame.to_numpy() will return the lower-common-denominator of the dtypes, meaning the dtype that can accommodate ALL of the types in the resulting homogeneous dtyped NumPy array. This can force some upcasting.
df3.to_numpy().dtype # dtype('float64')

astype
------
You can use the astype() method to explicitly convert dtypes from one to another. These will by default return a copy, even if the dtype was unchanged (pass copy=False to change this behavior). In addition, they will raise an exception if the astype operation is invalid.
Upcasting is always according to the NumPy rules. If two different dtypes are involved in an operation, then the more general one will be used as the result of the operation.
# conversion of dtypes
df3.astype('float32').dtypes

Convert a subset of columns to a specified type using astype().
dft = pd.DataFrame({"a": [1, 2, 3], "b": [4, 5, 6], "c": [7, 8, 9]})
dft.dtypes
dft[["a", "b"]] = dft[["a", "b"]].astype(np.uint8)
dft.dtypes

Convert certain columns to a specific dtype by passing a dict to astype().
dft1 = pd.DataFrame({"a": [1, 0, 1], "b": [4, 5, 6], "c": [7, 8, 9]})
dft1 = dft1.astype({"a": np.bool_, "c": np.float64})
dft1.dtypes

Note: When trying to convert a subset of columns to a specified type using astype() and loc(),upcasting occurs. loc() tries to fit in what we are assigning to the current dtypes, while [] will overwrite them taking the dtype from
the right hand side. Therefore the following piece of code produces the unintended result.
dft = pd.DataFrame({"a": [1, 2, 3], "b": [4, 5, 6], "c": [7, 8, 9]})
dft.loc[:, ["a", "b"]].astype(np.uint8).dtypes
dft.loc[:, ["a", "b"]] = dft.loc[:, ["a", "b"]].astype(np.uint8)
dft.dtypes

object conversion
------------------
pandas offers various functions to try to force conversion of types from the object dtype to other types. In cases where the data is already of the correct type, but stored in an object array, the DataFrame.infer_objects() and Series.infer_objects() methods can be used to soft convert to the correct type.
import datetime
df = pd.DataFrame([
    [1, 2,],
    ["a", "b"],
    [datetime.datetime(2019,1,2), datetime.datetime(2019,2,3)]
])
df = df.T
df.dtypes

0            object
1            object
2    datetime64[ns]

Because the data was transposed the original inference stored all columns as object, which infer_objects will correct.
df.infer_objects().dtypes
0             int64
1            object
2    datetime64[ns]

The following functions are available for one dimensional object arrays or scalars to perform hard conversion of objects to a specified type:
• to_numeric() (conversion to numeric dtypes)
m = [1.1, 2, 3]
pd.to_numeric(m)

• to_datetime() (conversion to datetime objects)
import datetime
m = ['2021.01.02', datetime.datetime(2021,1,3)]
pd.to_datetime(m)

• to_timedelta() (conversion to timedelta objects)
m = ['5µs', pd.Timedelta("1day")]
pd.to_timedelta(m)

To force a conversion, we can pass in an errors argument, which specifies how pandas should deal with elements that cannot be converted to desired dtype or object. By default, errors='raise', meaning that any errors encountered will be raised during the conversion process. However, if errors='coerce', these errors will be ignored and pandas will convert problematic elements to pd.NaT (for datetime and timedelta) or np.nan (for numeric). This might be useful if you are reading in data which is mostly of the desired dtype (e.g. numeric, datetime), but occasionally has non-conforming elements intermixed that you want to represent as missing:
import datetime
m = ['apple', datetime.datetime(2020, 4, 8)]
pd.to_datetime(m, errors='coerce') # DatetimeIndex(['NaT', '2020-04-08'],

m = ['apple', 2, 3.3]
pd.to_numeric(m, errors='coerce') # array([nan, 2. , 3.3])

m = ['apple', pd.Timedelta("1day")]
pd.to_timedelta(m, errors='coerce') # TimedeltaIndex([NaT, '1 days']

The errors parameter has a third option of errors='ignore', which will simply return the passed in data if it encounters any errors with the conversion to a desired data type:
import datetime
m = ['apple', datetime.datetime(2020, 4, 8)]
pd.to_datetime(m, errors='ignore') # Index(['apple', 2020-04-08 00:00:00], dtype='object')

m = ['apple', 2, 3.3]
pd.to_numeric(m, errors='ignore') # array(['apple', 2, 3.3], dtype=object)

m = ['apple', pd.Timedelta("1day")]
pd.to_timedelta(m, errors='ignore') # array(['apple', Timedelta('1 days 00:00:00')], dtype=object)

In addition to object conversion, to_numeric() provides another argument downcast, which gives the option of downcasting the newly (or already) numeric data to a smaller dtype, which can conserve memory:
In [410]: m = ["1", 2, 3]
In [411]: pd.to_numeric(m, downcast="integer") # smallest signed int dtype
Out[411]: array([1, 2, 3], dtype=int8)
In [412]: pd.to_numeric(m, downcast="signed") # same as 'integer'
Out[412]: array([1, 2, 3], dtype=int8)
In [413]: pd.to_numeric(m, downcast="unsigned") # smallest unsigned int dtype
Out[413]: array([1, 2, 3], dtype=uint8)
In [414]: pd.to_numeric(m, downcast="float") # smallest float dtype Out[414]: array([1., 2., 3.], dtype=float32)

As these methods apply only to one-dimensional arrays, lists or scalars; they cannot be used directly on multi- dimensional objects such as DataFrames. However, with apply(), we can “apply” the function over each column efficiently:

import datetime
df = pd.DataFrame([['2021.02.21', datetime.datetime(2021,3,2)]]*2)
print(df.info())
df.apply(pd.to_datetime).info()

df = pd.DataFrame([["1.1", 2, 3]] * 2, dtype="O")
print(df.dtypes)
df.apply(pd.to_numeric).dtypes

df = pd.DataFrame([["5us", pd.Timedelta("1day")]] * 2, dtype="O")
print(df.dtypes)
df.apply(pd.to_timedelta).dtypes

gotchas
-------
Performing selection operations on integer type data can easily upcast the data to floating. The dtype of the input data will be preserved in cases where nans are not introduced.
df3
print(df3.dtypes)
dfi = df3.astype("int32")
print(dfi.dtypes)
dfi['E'] = 1
print(dfi.dtypes)
dfi[dfi>0].dtypes

While float dtypes are unchanged.
dfa = df3.copy()
print(dfa.dtypes)
dfa['A'] = dfa['A'].astype('float32')
dfa[dfa>0].dtypes

2.3.14 Selecting columns based on dtype
---------------------------------------
The select_dtypes() method implements subsetting of columns based on their dtype.
First, let’s create a DataFrame with a slew of different dtypes:
df = pd.DataFrame({
    'string': list('abc'),
    'int64': list(range(1,4)),
    "uint8": np.arange(3, 6).astype("u1"),
    "float64": np.arange(4.0, 7.0),
    "bool1": [True, False, True],
    "bool2": [False, True, False],
    "dates": pd.date_range("now", periods=3),
    "category": pd.Series(list("ABC")).astype("category"),
})
df["tdeltas"] = df.dates.diff()
df["uint64"] = np.arange(3, 6).astype("u8")
df["other_dates"] = pd.date_range("20130101", periods=3)
df["tz_aware_dates"] = pd.date_range("20130101", periods=3, tz="US/Eastern")
df.dtypes

select_dtypes() has two parameters include and exclude that allow you to say “give me the columns with these dtypes” (include) and/or “give the columns without these dtypes” (exclude).
For example, to select bool columns:
df.select_dtypes(include=['bool'])
You can also pass the name of a dtype in the NumPy dtype hierarchy:
df.select_dtypes(include=['datetime64'])

select_dtypes() also works with generic dtypes as well.
For example, to select all numeric and boolean columns while excluding unsigned integers:
df.select_dtypes(include=['number', 'bool'], exclude=['unsignedinteger'])

To select string columns you must use the object dtype:
df.select_dtypes(include=['object'])

To see all the child dtypes of a generic dtype like numpy.number you can define a function that returns a tree of child dtypes:
def subtypes(dtype):
    subs = dtype.__subclasses__()
    if not subs:
        return dtype
    return [dtype, [subtypes(dt) for dt in subs]]
subtypes(np.number)
subtypes(np.integer)
subtypes(np.generic)

Note: pandas also defines the types category, and datetime64[ns, tz], which are not integrated into the normal NumPy hierarchy and won’t show up with the above function.

2.4 IO tools (text, CSV, HDF5, . . . )
---------------------------------------
The pandas I/O API is a set of top level reader functions accessed like pandas.read_csv() that generally return a pandas object. The corresponding writer functions are object methods that are accessed like DataFrame. to_csv(). Below is a table containing available readers and writers.
FormatType      DataDescription             Reader                  Writer
text                CSV                     read_csv                to_csv
text        Fixed-Width Text File           read_fwf
text                JSON                    read_json               to_json
text                HTML                    read_html               to_html
text            Local clipboard             read_clipboard          to_clipboard
binary             MS Excel                 read_excel              to_excel
binary          OpenDocument                read_excel
binary          HDF5 Format                 read_hdf                to_hdf
binary          Feather Format              read_feather            to_feather
binary          Parquet Format              read_parquet            to_parquet
binary          ORC Format                  read_orc
binary          Msgpack                     read_msgpack            to_msgpack
binary          Stata                       read_stata              to_stata
binary          SAS                         read_sas
binary          SPSS                        read_spss
binary      Python Pickle Format            read_pickle             to_pickle
SQL             SQL                         read_sql                to_sql
SQL         Google BigQuery                 read_gbq                to_gbq

Note: For examples that use the StringIO class, make sure you import it with from io import StringIO for Python 3.

2.4.1 CSV & text files
=========================
The workhorse function for reading text files (a.k.a. flat files) is read_csv().

Parsing options
read_csv() accepts the following common arguments:

Basic
filepath_or_buffer [various] Either a path to a file (a str, pathlib.Path, or py._path.local. LocalPath), URL (including http, ftp, and S3 locations), or any object with a read() method (such as an open file or StringIO).

sep [str, defaults to ',' for read_csv(), \t for read_table()] Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python’s builtin sniffer tool, csv.Sniffer. In addition, separators longer than 1 character and different from '\s+' will be interpreted as regular expressions and will also force the use of the Python parsing engine. Note that regex delimiters are prone to ignoring quoted data. Regex example: '\\r\\t'.

delimiter [str, default None] Alternative argument name for sep.

delim_whitespace [boolean, default False] Specifies whether or not whitespace (e.g. ' ' or '\t') will be used as the delimiter. Equivalent to setting sep='\s+'. If this option is set to True, nothing should be passed in for the delimiter parameter.

Column and index locations and names
header [int or list of ints, default 'infer'] Row number(s) to use as the column names, and the start of the data. Default behavior is to infer the column names: if no names are passed the behavior is identical to header=0 and column names are inferred from the first line of the file, if column names are passed explicitly then the behavior is identical to header=None. Explicitly pass header=0 to be able to replace existing names.
The header can be a list of ints that specify row locations for a MultiIndex on the columns e.g. [0,1,3]. Intervening rows that are not specified will be skipped (e.g. 2 in this example is skipped). Note that this parameter ignores commented lines and empty lines if skip_blank_lines=True, so header=0 denotes the first line of data rather than the first line of the file.

names [array-like, default None] List of column names to use. If file contains no header row, then you should explicitly pass header=None. Duplicates in this list are not allowed.

index_col [int, str, sequence of int / str, or False, default None] Column(s) to use as the row labels of the DataFrame, either given as string name or column index. If a sequence of int / str is given, a MultiIndex is used.
Note: index_col=False can be used to force pandas to not use the first column as the index, e.g. when you have a malformed file with delimiters at the end of each line.
The default value of None instructs pandas to guess. If the number of fields in the column header row is equal to the number of fields in the body of the data file, then a default index is used. If it is one larger, then the first field is used as an index.

usecols [list-like or callable, default None] Return a subset of the columns. If list-like, all elements must either be positional (i.e. integer indices into the document columns) or strings that correspond to column names provided either by the user in names or inferred from the document header row(s). For example, a valid list- likeusecolsparameterwouldbe[0, 1, 2]or['foo', 'bar', 'baz'].
Element order is ignored, so usecols=[0, 1] is the same as [1, 0]. To instantiate a DataFrame from data with element order preserved use pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']] for columns in ['foo', 'bar'] order or pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]for['bar', 'foo']order.
If callable, the callable function will be evaluated against the column names, returning names where the callable function evaluates to True:

from io import StringIO
data = 'col1,col2,col3\na,b,1\na,b,2\na,c,3'
pd.read_csv(StringIO(data))
pd.read_csv(StringIO(data), usecols=lambda x: x.upper() in['COL1', 'COL3'])

squeeze [boolean, default False] If the parsed data only contains one column then return a Series.
prefix [str, default None] Prefix to add to column numbers when no header, e.g. ‘X’ for X0, X1, . . .
mangle_dupe_cols [boolean, default True] Duplicate columns will be specified as ‘X’, ‘X.1’. . . ’X.N’, rather than ‘X’. . . ’X’. Passing in False will cause data to be overwritten if there are duplicate names in the columns.

General parsing configuration
dtype [Type name or dict of column -> type, default None] Data type for data or columns. E.g. {'a': np. float64, 'b': np.int32} (unsupported with engine='python'). Use str or object together with suitable na_values settings to preserve and not interpret dtype.
engine [{'c', 'python'}] Parser engine to use. The C engine is faster while the Python engine is currently more feature-complete.
converters [dict,defaultNone]Dictoffunctionsforconvertingvaluesincertaincolumns.Keyscaneitherbeintegers or column labels.
true_values [list, default None] Values to consider as True. false_values [list, default None] Values to consider as False. skipinitialspace [boolean, default False] Skip spaces after delimiter.

skiprows [list-like or integer, default None] Line numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file.
If callable, the callable function will be evaluated against the row indices, returning True if the row should be skipped and False otherwise:
data = "col1,col2,col3\na,b,1\na,b,2\nc,d,3"
pd.read_csv(StringIO(data))
pd.read_csv(StringIO(data), skiprows=lambda x: x % 2 != 0)

na_values [scalar, str, list-like, or dict, default None] Additional strings to recognize as NA/NaN. If dict passed, specific per-column NA values. See na values const below for a list of the values interpreted as NaN by default.
keep_default_na [boolean, default True] Whether or not to include the default NaN values when parsing the data. Depending on whether na_values is passed in, the behavior is as follows:
• Ifkeep_default_naisTrue,andna_valuesarespecified,na_valuesisappendedtothedefault NaN values used for parsing.
• If keep_default_na is True, and na_values are not specified, only the default NaN values are used for parsing.
• If keep_default_na is False, and na_values are specified, only the NaN values specified na_values are used for parsing.
• If keep_default_na is False, and na_values are not specified, no strings will be parsed as NaN. Note that if na_filter is passed in as False, the keep_default_na and na_values parameters will
be ignored.
na_filter [boolean, default True] Detect missing value markers (empty strings and the value of na_values). In data
without any NAs, passing na_filter=False can improve the performance of reading a large file. verbose [boolean, default False] Indicate number of NA values placed in non-numeric columns. skip_blank_lines [boolean, default True] If True, skip over blank lines rather than interpreting as NaN values.

Datetime handling
------------------
parse_dates [boolean or list of ints or names or list of lists or dict, default False.]
• If True -> try parsing the index.
• If [1, 2, 3] -> try parsing columns 1, 2, 3 each as a separate date column.
• If [[1, 3]] -> combine columns 1 and 3 and parse as a single date column.
• If {'foo': [1, 3]} -> parse columns 1, 3 as date and call result ‘foo’. A fast-path exists for iso8601- formatted dates.
infer_datetime_format [boolean, default False] If True and parse_dates is enabled for a column, attempt to infer the datetime format to speed up the processing.
keep_date_col [boolean, default False] If True and parse_dates specifies combining multiple columns then keep the original columns.
date_parser [function, default None] Function to use for converting a sequence of string columns to an array of datetime instances. The default uses dateutil.parser.parser to do the conversion. pandas will try to call date_parser in three different ways, advancing to the next if an exception occurs: 1) Pass one or more arrays (as defined by parse_dates) as arguments; 2) concatenate (row-wise) the string values from the columns defined by parse_dates into a single array and pass that; and 3) call date_parser once for each row using one or more strings (corresponding to the columns defined by parse_dates) as arguments.
dayfirst [boolean, default False] DD/MM format dates, international and European format.
cache_dates [boolean, default True] If True, use a cache of unique, converted dates to apply the datetime conversion.
May produce significant speed-up when parsing duplicate date strings, especially ones with timezone offsets.

Iteration
iterator [boolean, default False] Return TextFileReader object for iteration or getting chunks with get_chunk().
chunksize [int, default None] Return TextFileReader object for iteration

Quoting, compression, and file format
compression [{'infer', 'gzip', 'bz2', 'zip', 'xz', None, dict}, default 'infer'] For on-the-fly de- compression of on-disk data. If ‘infer’, then use gzip, bz2, zip, or xz if filepath_or_buffer is path-like ending in ‘.gz’, ‘.bz2’, ‘.zip’, or ‘.xz’, respectively, and no decompression otherwise. If using ‘zip’, the ZIP file must contain only one data file to be read in. Set to None for no decompression. Can also be a dict with key 'method' set to one of {'zip', 'gzip', 'bz2'} and other key-value pairs are forwarded to zipfile.ZipFile, gzip.GzipFile, or bz2.BZ2File. As an example, the following could be passed for faster compression and to create a reproducible gzip archive: compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}.
thousands [str, default None] Thousands separator.
decimal [str, default '.'] Character to recognize as decimal point. E.g. use ',' for European data.

float_precision [string, default None] Specifies which converter the C engine should use for floating-point values. The options are None for the ordinary converter, high for the high-precision converter, and round_trip for the round-trip converter.
lineterminator [str (length 1), default None] Character to break file into lines. Only valid with C parser.
quotechar [str (length 1)] The character used to denote the start and end of a quoted item. Quoted items can include
the delimiter and it will be ignored.
quoting [int or csv.QUOTE_* instance, default 0] Control field quoting behavior per csv.QUOTE_* constants.
Use one of QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).
doublequote [boolean, default True] When quotechar is specified and quoting is not QUOTE_NONE, indi- cate whether or not to interpret two consecutive quotechar elements inside a field as a single quotechar element.
escapechar [str (length 1), default None] One-character string used to escape delimiter when quoting is QUOTE_NONE.
comment [str, default None] Indicates remainder of line should not be parsed. If found at the beginning of a line, the line will be ignored altogether. This parameter must be a single character. Like empty lines (as long as skip_blank_lines=True), fully commented lines are ignored by the parameter header but not by skiprows. For example, if comment='#', parsing ‘#empty\na,b,c\n1,2,3’ with header=0 will result in ‘a,b,c’ being treated as the header.
encoding [str,defaultNone]EncodingtouseforUTFwhenreading/writing(e.g.'utf-8').ListofPythonstandard encodings.
dialect [str or csv.Dialect instance, default None] If provided, this parameter will override values (default or not) for the following parameters: delimiter, doublequote, escapechar, skipinitialspace, quotechar, and quoting. If it is necessary to override values, a ParserWarning will be issued. See csv. Dialect documentation for more details.

Error handling
error_bad_lines [boolean, default True] Lines with too many fields (e.g. a csv line with too many commas) will by default cause an exception to be raised, and no DataFrame will be returned. If False, then these “bad lines” will dropped from the DataFrame that is returned. See bad lines below.
warn_bad_lines [boolean, default True] If error_bad_lines is False, and warn_bad_lines is True, a warning for each “bad line” will be output.

Specifying column data types
You can indicate the data type for the whole DataFrame or individual columns:
data = "a,b,c,d\n1,2,3,4\n5,6,7,8\n9,10,11"
df = pd.read_csv(StringIO(data), dtype=object)
df = pd.read_csv(StringIO(data), dtype={"b": object, "c": np.float64, "d":"Int64"})

Fortunately, pandas offers more than one way to ensure that your column(s) contain only one dtype. If you’re unfamiliar with these concepts,

For instance, you can use the converters argument of read_csv():
data = "col_1\n1\n2\n'A'\n4.22"
df = pd.read_csv(StringIO(data), converters={"col_1": str})
df["col_1"].apply(type).value_counts()

Or you can use the to_numeric() function to coerce the dtypes after reading in the data,which will convert all valid parsing to floats, leaving the invalid parsing as NaN.
df2 = pd.read_csv(StringIO(data))
df2["col_1"] = pd.to_numeric(df2["col_1"], errors="coerce")
df2["col_1"].apply(type).value_counts()

Ultimately, how you deal with reading in columns containing mixed dtypes depends on your specific needs. In the case above, if you wanted to NaN out the data anomalies, then to_numeric() is probably your best option. However, if you wanted for all the data to be coerced, no matter the type, then using the converters argument of read_csv() would certainly be worth trying.

Note: In some cases, reading in abnormal data with columns containing mixed dtypes will result in an inconsistent dataset. If you rely on pandas to infer the dtypes of your columns, the parsing engine will go and infer the dtypes for different chunks of the data, rather than the whole dataset at once. Consequently, you can end up with column(s) with mixed dtypes. For example,
will result with mixed_df containing an int dtype for certain chunks of the column, and str for others due to the mixed dtypes from the data that was read in. It is important to note that the overall column will be marked with a dtype of object, which is used for columns with mixed dtypes.
col1 = list(range(500000)) + ["a", "b"] + list(range(50000))
df=pd.DataFrame({'col1': col1})
df.to_csv('foo.csv')
mixed_df = pd.read_csv('foo.csv')
mixed_df['col1'].apply(type).value_counts()

Specifying categorical dtype
-----------------------------
Categorical columns can be parsed directly by dtype=CategoricalDtype(categories, ordered).
data = "col1,col2,col3\na,b,1\na,b,2\nc,d,3"
pd.read_csv(StringIO(data)).dtypes
pd.read_csv(StringIO(data), dtype="category").dtypes
Individual columns can be parsed as a Categorical using a dict specification:
pd.read_csv(StringIO(data), dtype={'col1':'category'}).dtypes

Specifying dtype='category' will result in an unordered Categorical whose categories are the unique values observed in the data. For more control on the categories and order, create a CategoricalDtype ahead of time, and pass that for that column’s dtype.
dtype = CategoricalDtype(['d', 'c', 'b', 'a'], ordered=True)
pd.read_csv(StringIO(data), dtype={'col1':dtype}).dtypes

When using dtype=CategoricalDtype, “unexpected” values outside of dtype.categories are treated as missing values.
dtype = CategoricalDtype(["a", "b", "d"]) # No 'c'
pd.read_csv(StringIO(data), dtype={'col1':dtype}).col1

This matches the behavior of Categorical.set_categories().
Note: With dtype='category', the resulting categories will always be parsed as strings (object dtype). If the
categories are numeric they can be converted using the to_numeric() function, or as appropriate, another converter
such as to_datetime().
When dtype is a CategoricalDtype with homogeneous categories ( all numeric, all datetimes, etc.), the
conversion is done automatically.
df = pd.read_csv(StringIO(data), dtype="category")
df.dtypes
df.col3
df["col3"].cat.categories = pd.to_numeric(df['col3'].cat.categories)
df.col3

Naming and using columns
Handling column names
============================
A file may or may not have a header row. pandas assumes the first row should be used as the column names:
data = "a,b,c\n1,2,3\n4,5,6\n7,8,9"
print(data)
pd.read_csv(StringIO(data))
By specifying the names argument in conjunction with header you can indicate other names to use and whether or not to throw away the header row (if any):
pd.read_csv(StringIO(data), names=["foo", "bar", "baz"], header=0)
# Replace Header name with provided names
pd.read_csv(StringIO(data), names=["foo", "bar", "baz"], header=0)
# Add new column names and treat existing data as rows
pd.read_csv(StringIO(data), names=["foo", "bar", "baz"], header=None)
pd.read_csv(StringIO(data), names=["foo", "bar", "baz"])

If the header is in a row other than the first, pass the row number to header. This will skip the preceding rows:
data = "skip this skip it\na,b,c\n1,2,3\n4,5,6\n7,8,9"
pd.read_csv(StringIO(data), header=1)
Note: Default behavior is to infer the column names: if no names are passed the behavior is identical to header=0 and column names are inferred from the first non-blank line of the file, if column names are passed explicitly then the behavior is identical to header=None.

Duplicate names parsing
-----------------------
If the file or header contains duplicate names, pandas will by default distinguish between them so as to prevent overwriting data:
data = "a,b,a\n0,1,2\n3,4,5"
pd.read_csv(StringIO(data))
There is no more duplicate data because mangle_dupe_cols=True by default, which modifies a series of dupli- cate columns ‘X’, . . . , ‘X’ to become ‘X’, ‘X.1’, . . . , ‘X.N’. If mangle_dupe_cols=False, duplicate data can arise:
To prevent users from encountering this problem with duplicate data, a ValueError exception is raised if mangle_dupe_cols != True:
data = 'a,b,a\n0,1,2\n3,4,5'
pd.read_csv(StringIO(data), mangle_dupe_cols=False)
ValueError: Setting mangle_dupe_cols=False is not supported yet

Filtering columns (usecols):
----------------------------
The usecols argument allows you to select any subset of the columns in a file, either using the column names, position numbers or a callable:
data = "a,b,c,d\n1,2,3,foo\n4,5,6,bar\n7,8,9,baz"
pd.read_csv(StringIO(data), usecols=['b', 'd'])
pd.read_csv(StringIO(data), usecols=[0,2,3])
pd.read_csv(StringIO(data), usecols=lambda x: x.upper() in ["A", "C"])

The usecols argument can also be used to specify which columns not to use in the final result:
pd.read_csv(StringIO(data), usecols=lambda x: x not in ["a", "c"])
In this case, the callable is specifying that we exclude the “a” and “c” columns from the output.

querys and empty lines
Ignoring line comments and empty lines
----------------------------------------
If the comment parameter is specified, then completely commented lines will be ignored. By default, completely blank lines will be ignored as well.
data = "\na,b,c\n \n# commented line\n1,2,3\n\n4,5,6"
pd.read_csv(StringIO(data), comment="#")
If skip_blank_lines=False, then read_csv will not ignore blank lines:
data = "a,b,c\n\n1,2,3\n\n\n4,5,6"
pd.read_csv(StringIO(data), skip_blank_lines=False)

Warning: The presence of ignored lines might create ambiguities involving line numbers;the parameter header uses row numbers (ignoring commented/empty lines), while skiprows uses line numbers (including commented/empty lines):
data = "#comment\na,b,c\nA,B,C\n1,2,3"
pd.read_csv(StringIO(data), comment="#", header=1)
data = "A,B,C\n#comment\na,b,c\n1,2,3"
pd.read_csv(StringIO(data), comment="#", skiprows=2)
If both header and skiprows are specified, header will be relative to the end of skiprows. For example:
data = (
 "# empty\n"
 "# second empty line\n"
 "# third emptyline\n"
 "X,Y,Z\n"
 "1,2,3\n"
 "A,B,C\n"
 "1,2.,4.\n"
 "5.,NaN,10.0\n"  )
pd.read_csv(StringIO(data), comment="#", skiprows=4, header=1)

Comments
========
Sometimes comments or meta data may be included in a file:
file - temp.csv
ID,level,category
Patient1,123000,x # really unpleasant
Patient2,23000,y # wouldn't take his medicine
Patient3,1234018,z # awesome

We can suppress the comments using the comment keyword:
df = pd.read_csv('temp.csv', comment='#')
df

Dealing with Unicode data
--------------------------
The encoding argument should be used for encoded unicode data, which will result in byte strings being decoded to unicode in the result:
from io import BytesIO
data = b"word,length\n" b"Tr\xc3\xa4umen,7\n" b"Gr\xc3\xbc\xc3\x9fe,5"
data = data.decode("utf8").encode("latin-1")
df = pd.read_csv(BytesIO(data), encoding="latin-1")
df['word'][1]
Some formats which encode all characters as multiple bytes, like UTF-16, won’t parse correctly at all without specifying the encoding.

Index columns and trailing delimiters
--------------------------------------
If a file has one more column of data than the number of column names, the first column will be used as the DataFrame’s row names i.e index:
data = "a,b,c\n4,apple,bat,5.7\n8,orange,cow,10"
pd.read_csv(StringIO(data))
data = "index,a,b,c\n4,apple,bat,5.7\n8,orange,cow,10"
pd.read_csv(StringIO(data), index_col=0)

Ordinarily, you can achieve this behavior using the index_col option.
There are some exception cases when a file has been prepared with delimiters at the end of each data line, confusing
the parser. To explicitly disable the index column inference and discard the last column, pass index_col=False:
data = "a,b,c\n4,apple,bat,\n8,orange,cow,"
pd.read_csv(StringIO(data))
pd.read_csv(StringIO(data), index_col=False)

if column name also contains an extra comma
data = "a,b,c,\n4,apple,bat,\n8,orange,cow,"
pd.read_csv(StringIO(data), index_col=False, names=['a','b','c'], header=0)

If a subset of data is being parsed using the usecols option, the index_col specification is based on that subset, not the original data.
data = "a,b,c\n4,apple,bat,\n8,orange,cow,"
pd.read_csv(StringIO(data), usecols=["b", "c"], index_col=False)


Specifying categorical dtype
----------------------------
To better facilitate working with datetime data, read_csv() uses the keyword arguments parse_dates and date_parser to allow users to specify a variety of columns and date/time formats to turn the input text data into datetime objects.
The simplest case is to just pass in parse_dates=True:
# Use a column as an index, and parse it as dates.
df = pd.DataFrame({
    'date':pd.date_range('2021.02.01', periods=3),
    'A':['a','b','c'],
    'B':[1, 3, 4],
    'C':[2, 4, 5],
})
df.to_csv('foo.csv', index=False)
pd.read_csv('foo.csv', index_col=0, parse_dates=True)
# These are Python datetime objects
df.index # DatetimeIndex(['2021-02-01', '2021-02-02', '2021-02-03'], dtype='datetime64[ns]', name='date', freq=None)

It is often the case that we may want to store date and time data separately, or store various date fields separately. the parse_dates keyword can be used to specify a combination of columns to parse the dates and/or times from.
You can specify a list of column lists to parse_dates, the resulting date columns will be prepended to the output (so as to not affect the existing column order) and the new column names will be the concatenation of the component column names:
print(open('temp.csv').read())
pd.read_csv('temp.csv', header=None, parse_dates=[[1, 2], [1, 3]])

By default the parser removes the component date columns, but you can choose to retain them via the keep_date_col keyword:
pd.read_csv("temp.csv", header=None, keep_date_col=True, parse_dates=[[1, 2], [1, 3]])

Note that if you wish to combine multiple columns into a single date column, a nested list must be used. In other words,parse_dates=[1, 2]indicates that the second and third columns should each be parsed as separate date columns while parse_dates=[[1, 2]]means the two columns should be parsed into a single column.
You can also use a dict to specify custom name columns:
date_spec = {"nominal": [1, 2], "actual": [1, 3]}
pd.read_csv("temp.csv", header=None, parse_dates=date_spec)

It is important to remember that if multiple text columns are to be parsed into a single date column, then a new column is prepended to the data. The index_col specification is based off of this new set of columns rather than the original data columns:
pd.read_csv("temp.csv", header=None, parse_dates=date_spec, index_col=0)
pd.read_csv("temp.csv", header=None, parse_dates=date_spec, index_col=1)

Note: If a column or index contains an unparsable date, the entire column or index will be returned unaltered as an object data type. For non-standard datetime parsing, use to_datetime() after pd.read_csv.
Note: read_csv has a fast_path for parsing datetime strings in iso8601 format,e.g“2000-01-01T00:01:02+00:00” and similar variations. If you can arrange for your data to store datetimes in this format, load times will be significantly faster, ~20x has been observed.

Date parsing functions
-----------------------
Finally, the parser allows you to specify a custom date_parser function to take full advantage of the flexibility of the date parsing API:
date_spec = {"nominal": [1, 2], "actual": [1, 3]}
print(pd.read_csv('temp.csv', parse_dates=date_spec).dtypes)
pd.read_csv('temp.csv', parse_dates=date_spec, date_parser=pd.to_datetime).dtypes

pandas will try to call the date_parser function in three different ways. If an exception is raised, the next one is tried:
1. date_parser is first called with one or more arrays as arguments, as defined using parse_dates (e.g., date_parser(['2013', '2013'], ['1', '2'])).
2. If #1 fails, date_parser is called with all the columns concatenated row-wise into a single array (e.g., date_parser(['2013 1', '2013 2'])).
Note that performance-wise, you should try these methods of parsing dates in order:
1. Try to infer the format using infer_datetime_format=True (see section below).
2. If you know the format, use pd.to_datetime(): date_parser=lambda x: pd. to_datetime(x, format=...).
3. If you have a really non-standard format, use a custom date_parser function. For optimal performance, this should be vectorized, i.e., it should accept arrays as arguments.

Parsing a CSV with mixed timezones
------------------------------------
pandas cannot natively represent a column or index with mixed timezones. If your CSV file contains columns with a mixture of timezones, the default result will be an object-dtype column with strings, even with parse_dates.
content = """\
a
2000-01-01T00:00:00+05:00
2000-01-01T00:00:00+06:00"""
print(pd.read_csv(StringIO(content), parse_dates=['a']))
(pd.read_csv(StringIO(content), parse_dates=True))
To parse the mixed-timezone values as a datetime column, pass a partially-applied to_datetime() with utc=True as the date_parser.
pd.read_csv(StringIO(content), parse_dates=['a'],
            date_parser=lambda col:pd.to_datetime(col, utc=True))

Inferring datetime format
---------------------------
If you have parse_dates enabled for some or all of your columns, and your datetime strings are all formatted the same way, you may get a large speed up by setting infer_datetime_format=True. If set, pandas will attempt to guess the format of your datetime strings, and then use a faster means of parsing the strings. 5-10x parsing speeds have been observed. pandas will fallback to the usual parsing if either the format cannot be guessed or the format that was guessed cannot properly parse the entire column of strings. So in general, infer_datetime_format should not have any negative consequences if enabled.
Here are some examples of datetime strings that can be guessed (All representing December 30th, 2011 at 00:00:00):
Note that infer_datetime_format is sensitive to dayfirst. With dayfirst=True, it will guess “01/12/2011” to be December 1st. With dayfirst=False (default) it will guess “01/12/2011” to be January 12th.
# Try to infer the format for the index column
pd.read_csv('foo.csv', index_col=0, parse_dates=True, infer_datetime_format=True)

International date formats
While US date formats tend to be MM/DD/YYYY, many international formats use DD/MM/YYYY instead. For convenience, a dayfirst keyword is provided:
date,value,cat
1/6/2000,5,a
2/6/2000,10,b
3/6/2000,15,c
pd.read_csv('temp.csv', parse_dates=[0], dayfirst=True)

Writing CSVs to binary file objects
------------------------------------
df.to_csv(..., mode="wb") allows writing a CSV to a file object opened binary mode. In most cases, it is
not necessary to specify mode as Pandas will auto-detect whether the file object is opened in text or binary mode
import io
data = pd.DataFrame([0, 1, 2])
data.to_csv('test.csv', encoding="utf-8", index=False, compression='gzip')
pd.read_csv('test.csv', encoding="utf-8", compression='gzip')

Specifying method for floating-point conversion
------------------------------------------------
The parameter float_precision can be specified in order to use a specific floating-point converter during parsing with the C engine. The options are the ordinary converter, the high-precision converter, and the round-trip converter (which is guaranteed to round-trip values after writing to a file). For example:
val = "0.3066101993807095471566981359501369297504425048828125"
data = f"a,b,c\n1,2,{val}"
abs(pd.read_csv(StringIO(data), engine='c', float_precision=None)['c'][0] - float(val)) # 5.551115123125783e-17
abs(pd.read_csv(StringIO(data), engine='c', float_precision='high')['c'][0] - float(val)) # 5.551115123125783e-17
abs(pd.read_csv(StringIO(data), engine='c', float_precision='round_trip')['c'][0] - float(val)) # 0.0

Thousand separators
--------------------
For large numbers that have been written with a thousands separator, you can set the thousands keyword to a string of length 1 so that integers will be parsed correctly:
By default, numbers with a thousands separator will be parsed as strings:
ID|level|category
Patient1|123,000|x
Patient2|23,000|y
Patient3|1,234,018|z
df = pd.read_csv('temp.csv', sep='|')
df.level.dtype # dtype('O')
The thousands keyword allows integers to be parsed correctly:
df = pd.read_csv('temp.csv', sep='|', thousands=',')
df.level.dtype # dtype('int64')

NA Values:
----------
To control which values are parsed as missing values (which are signified by NaN), specify a string in na_values. If you specify a list of strings, then all values in it are considered to be missing values. If you specify a number (a float, like 5.0 or an integer like 5), the corresponding equivalent values will also imply a missing value (in this case effectively[5.0, 5]arerecognizedasNaN).
To completely override the default values that are recognized as missing, specify keep_default_na=False.
The default NaN recognized values are ['-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN', '#N/ A N/A', '#N/A', 'N/A', 'n/a', 'NA', '<NA>', '#NA', 'NULL', 'null', 'NaN', '-NaN', 'nan', '-nan', ''].

pd.read_csv("path_to_file.csv", na_values=[5])
In the example above 5 and 5.0 will be recognized as NaN, in addition to the defaults. A string will first be interpreted as a numerical 5, then as a NaN.

pd.read_csv("path_to_file.csv", keep_default_na=False, na_values=[""])
Above, only an empty field will be recognized as NaN.

pd.read_csv("path_to_file.csv", keep_default_na=False, na_values=["NA", "0"])
Above, both NA and 0 as strings are NaN.

pd.read_csv("path_to_file.csv", na_values=["Nope"])
The default values, in addition to the string "Nope" are recognized as NaN.

Test:
s,val
'a',10
'b',5
'c',5.0
'd',20
'e',
'f',30
'g',0
'h',"NA"
'i',"Nope"
pd.read_csv('temp.csv', na_values=[5])
pd.read_csv('temp.csv', keep_default_na=False, na_values=[""])
pd.read_csv('temp.csv', keep_default_na=False, na_values=["NA", "0"])
pd.read_csv('temp.csv', na_values=["Nope"])

Infinity
--------
inf like values will be parsed as np.inf (positive infinity), and -inf as -np.inf (negative infinity). These will ignore the case of the value, meaning Inf, will also be parsed as np.inf.

Returning Series
-----------------
Using the squeeze keyword, the parser will return output with a single column as a Series:
level
Patient1,123000
Patient2,23000
Patient3,1234018

output = pd.read_csv('temp.csv', squeeze=True)
type(output) # pandas.core.series.Series

Boolean values
--------------
The common values True, False, TRUE, and FALSE are all recognized as boolean. Occasionally you might want to recognize other values as being boolean. To do this, use the true_values and false_values options as follows:
data = "a,b,c\n1,Yes,2\n3,No,4"
pd.read_csv(StringIO(data), true_values=['Yes'], false_values=['No', '0'])

Handling “bad” lines
---------------------
Some files may have malformed lines with too few fields or too many. Lines with too few fields will have NA values filled in the trailing fields. Lines with too many fields will raise an error by default:
data = "a,b,c\n1,2,3\n4,5,6,7\n8,9,10"
pd.read_csv(StringIO(data))
ParserError: Error tokenizing data. C error: Expected 3 fields in line 3, saw 4
You can elect to skip bad lines:
pd.read_csv(StringIO(data), error_bad_lines=False)
You can also use the usecols parameter to eliminate extraneous column data that appear in some lines but not others:
pd.read_csv(StringIO(data), usecols=[0,1,2])

Dialect
-------
The dialect keyword gives greater flexibility in specifying the file format. By default it uses the Excel dialect but you can specify either the dialect name or a csv.Dialect instance.
Suppose you had data with unenclosed quotes:
By default, read_csv uses the Excel dialect and treats the double quote as the quote character, which causes it to fail when it finds a newline before it finds the closing double quote.
We can get around this using dialect:
label1,label2,label3
index1,"a,c,e
index2,b,d,f

import csv
dia = csv.excel()
dia.quoting = csv.QUOTE_NONE
pd.read_csv('temp.csv', dialect=dia)

All of the dialect options can be specified separately by keyword arguments:
data = "a,b,c~1,2,3~4,5,6"
pd.read_csv(StringIO(data), lineterminator='~')

Another common dialect option is skipinitialspace, to skip any whitespace after a delimiter:
data = "a, b , c\n1, 2, 3\n4, 5, 6"
pd.read_csv(StringIO(data), skipinitialspace=True)

The parsers make every attempt to “do the right thing” and not be fragile. Type inference is a pretty big deal. If a column can be coerced to integer dtype without altering the contents, the parser will do so. Any non-numeric columns will come through as object dtype as with the rest of pandas objects.

Quoting and Escape Characters
------------------------------
Quotes (and other escape characters) in embedded fields can be handled in any number of ways. One way is to use backslashes; to properly parse this data, you should pass the escapechar option:
data = 'a,b\n"hello, \\"Bob\\", nice to see you",5'
pd.read_csv(StringIO(data), escapechar='\\')

Files with fixed width columns
-------------------------------
While read_csv() reads delimited data, the read_fwf() function works with data files that have known and fixed column widths. The function parameters to read_fwf are largely the same as read_csv with two extra parameters, and a different usage of the delimiter parameter:
• colspecs: A list of pairs (tuples) giving the extents of the fixed-width fields of each line as half-open intervals (i.e., [from, to[ ). String value ‘infer’ can be used to instruct the parser to try detecting the column specifications from the first 100 rows of the data. Default behavior, if not specified, is to infer.
• widths: A list of field widths which can be used instead of ‘colspecs’ if the intervals are contiguous.
• delimiter: Characters to consider as filler characters in the fixed-width file. Can be used to specify the filler
character of the fields if it is not spaces (e.g., ‘~’).

Consider a typical fixed-width data file:
id8141 360.242940 149.910199 11950.7
id1594 444.953632 166.985655 11788.4
id1849 364.136849 183.628767 11806.2
id1230 413.836124 184.375703 11916.8
id1948 502.953953 173.237159 12468.3

In order to parse this file into a DataFrame, we simply need to supply the column specifications to the read_fwf function along with the file name:

# Column specifications are a list of half-intervals
colspecs = [(0, 6), (8, 20), (21, 33), (35, 43)]
#pd.read_fwf('temp.csv', colspecs=colspecs, header=None, index_col=0)
pd.read_fwf('temp.csv', header=None, index_col=0)

Note how the parser automatically picks column names X.<column number> when header=None argument is spec- ified. Alternatively, you can supply just the column widths for contiguous columns:
widths = [6, 14, 13, 10]
df = pd.read_fwf("temp.csv", widths=widths, header=None)

The parser will take care of extra white spaces around the columns so it’s ok to have extra separation between the columns in the file.
By default, read_fwf will try to infer the file’s colspecs by using the first 100 rows of the file. It can do it only in cases when the columns are aligned and correctly separated by the provided delimiter (default delimiter is whitespace).
pd.read_fwf('temp.csv', header=None, index_col=0)

read_fwf supports the dtype parameter for specifying the types of parsed columns to be different from the inferred type.
pd.read_fwf("temp.csv", header=None, index_col=0).dtypes
pd.read_fwf("temp.csv", header=None, index_col=0, dtype={2: "object"}).dtypes

Indexes
Files with an “implicit” index column
======================================
Consider a file with one less entry in the header than the number of data column:
A,B,C
20090101,a,1,2
20090102,b,3,4
20090103,c,4,5

df = pd.read_csv('temp.csv', parse_dates=True)
df.index

Reading an index with a MultiIndex
Suppose you have data indexed by two columns:
year,indiv,zit,xit
1977,"A",1.2,.6
1977,"B",1.5,.5
1977,"C",1.7,.8
1978,"A",.2,.06
1978,"B",.7,.2
1978,"C",.8,.3
1978,"D",.9,.5
1978,"E",1.4,.9
1979,"C",.2,.15
1979,"D",.14,.05
1979,"E",.5,.15
1979,"F",1.2,.5
1979,"G",3.4,1.9
1979,"H",5.4,2.7
1979,"I",6.4,1.2
The index_col argument to read_csv can take a list of column numbers to turn multiple columns into a MultiIndex for the index of the returned object:
df = pd.read_csv('temp.csv', index_col=[0,1])
df
df.loc[1978]

Reading columns with a MultiIndex
----------------------------------
By specifying list of row locations for the header argument, you can read in a MultiIndex for the columns.
Specifying non-consecutive rows will skip the intervening rows.
from pandas._testing import makeCustomDataframe as mkdf
df = mkdf(5, 3, r_idx_nlevels=2, c_idx_nlevels=4)
df

C0,,C_l0_g0,C_l0_g1,C_l0_g2
C1,,C_l1_g0,C_l1_g1,C_l1_g2
C2,,C_l2_g0,C_l2_g1,C_l2_g2
C3,,C_l3_g0,C_l3_g1,C_l3_g2
R0,R1,,,
R_l0_g0,R_l1_g0,R0C0,R0C1,R0C2
R_l0_g1,R_l1_g1,R1C0,R1C1,R1C2
R_l0_g2,R_l1_g2,R2C0,R2C1,R2C2
R_l0_g3,R_l1_g3,R3C0,R3C1,R3C2
R_l0_g4,R_l1_g4,R4C0,R4C1,R4C2

read_csv is also able to interpret a more common format of multi-columns indices.
,a,a,a,b,c,c
,q,r,s,t,u,v
one,1,2,3,4,5,6
two,7,8,9,10,11,12

pd.read_csv('temp.csv', index_col=0, header=[0, 1])
Note: If an index_col is not specified (e.g. you don’t have an index, or wrote it with df.to_csv(..., index=False), then any names on the columns index will be lost.

Automatically “sniffing” the delimiter
---------------------------------------
read_csv is capable of inferring delimited (not necessarily comma-separated) files, as pandas uses the csv. Sniffer class of the csv module. For this, you have to specify sep=None.
:0:1:2:3
0:0.4691122999071863:-0.2828633443286633:-1.5090585031735124:-1.1356323710171934
1:1.2121120250208506:-0.17321464905330858:0.11920871129693428:-1.0442359662799567
2:-0.8618489633477999:-2.1045692188948086:-0.4949292740687813:1.071803807037338
3:0.7215551622443669:-0.7067711336300845:-1.0395749851146963:0.27185988554282986
4:-0.42497232978883753:0.567020349793672:0.27623201927771873:-1.0874006912859915
5:-0.6736897080883706:0.1136484096888855:-1.4784265524372235:0.5249876671147047
6:0.4047052186802365:0.5770459859204836:-1.7150020161146375:-1.0392684835147725
7:-0.3706468582364464:-1.1578922506419993:-1.344311812731667:0.8448851414248841
8:1.0757697837155533:-0.10904997528022223:1.6435630703622064:-1.4693879595399115
9:0.35702056413309086:-0.6746001037299882:-1.776903716971867:-0.9689138124473498

pd.read_csv('temp.csv', sep=None, engine='python', index_col=0)

Reading multiple files to create a single DataFrame:
----------------------------------------------------
It’s best to use concat() to combine multiple files.

Iterating through files chunk by chunk
---------------------------------------
Suppose you wish to iterate through a (potentially very large) file lazily rather than reading the entire file into memory, such as the following:
|0|1|2|3
0|0.4691122999071863|-0.2828633443286633|-1.5090585031735124|-1.1356323710171934
1|1.2121120250208506|-0.17321464905330858|0.11920871129693428|-1.0442359662799567
2|-0.8618489633477999|-2.1045692188948086|-0.4949292740687813|1.071803807037338
3|0.7215551622443669|-0.7067711336300845|-1.0395749851146963|0.27185988554282986
4|-0.42497232978883753|0.567020349793672|0.27623201927771873|-1.0874006912859915
5|-0.6736897080883706|0.1136484096888855|-1.4784265524372235|0.5249876671147047
6|0.4047052186802365|0.5770459859204836|-1.7150020161146375|-1.0392684835147725
7|-0.3706468582364464|-1.1578922506419993|-1.344311812731667|0.8448851414248841
8|1.0757697837155533|-0.10904997528022223|1.6435630703622064|-1.4693879595399115
9|0.35702056413309086|-0.6746001037299882|-1.776903716971867|-0.9689138124473498

To read the file in one go:
pd.read_csv('temp.csv', sep='|', index_col=0)

By specifying a chunksize to read_csv, the return value will be an iterable object of type TextFileReader:
with pd.read_csv('temp.csv', sep='|', iterator=True, index_col=0) as reader:
    p = reader.get_chunk(5)
print(p)

Specifying the parser engine
----------------------------
Under the hood pandas uses a fast and efficient parser implemented in C as well as a Python implementation which is currently more feature-complete. Where possible pandas uses the C parser (specified as engine='c'), but may fall back to Python if C-unsupported options are specified. Currently, C-unsupported options include:
• sep other than a single character (e.g. regex separators) • skipfooter
• sep=None with delim_whitespace=False
Specifying any of the above options will produce a ParserWarning unless the python engine is selected explicitly using engine='python'.

Reading/writing remote files
-----------------------------
You can pass in a URL to read or write remote files to many of pandas’ IO functions - the following example shows reading a CSV file:
df = pd.read_csv("https://download.bls.gov/pub/time.series/cu/cu.item", sep="\t")
df.head()
df.info()
df.shape
df.sample(5)

All URLs which are not local files or HTTP(s) are handled by fsspec, if installed, and its various filesystem implementations (including Amazon S3, Google Cloud, SSH, FTP, webHDFS. . . ). Some of these implementations will require additional packages to be installed, for example S3 URLs require the s3fs library:
df = pd.read_json("s3://pandas-test/adatafile.json")
When dealing with remote storage systems, you might need extra configuration with environment variables or config files in special locations. For example, to access data in your S3 bucket, you will need to define credentials in one of the several ways listed in the S3Fs documentation. The same is true for several of the storage backends, and you
should follow the links at fsimpl1 for implementations built into fsspec and fsimpl2 for those not included in the main fsspec distribution.
You can also pass parameters directly to the backend driver. For example, if you do not have S3 credentials, you can still access public data by specifying an anonymous connection, such as
New in version 1.2.0.
pd.read_csv( "s3://ncei-wcsd-archive/data/processed/SH1305/18kHz/SaKe2013" "-D20130523-T080854_to_SaKe2013-D20130523-T085643.csv", storage_options={"anon": True},
)
fsspec also allows complex URLs, for accessing data in compressed archives, local caching of files, and more. To locally cache the above example, you would modify the call to
where we specify that the “anon” parameter is meant for the “s3” part of the implementation, not to the caching implementation. Note that this caches to a temporary directory for the duration of the session only, but you can also specify a permanent store.

------------------------
Writing out data
Writing to CSV format
------------------------
df = pd.DataFrame({
    'a':np.arange(400000),
    'b':np.arange(400000, 800000)
})
df.to_csv('temp.csv', sep='|', na_rep='Ramesh', index=False, chunksize=2)

The Series and DataFrame objects have an instance method to_csv which allows storing the contents of the object as a comma-separated-values file. The function takes a number of arguments. Only the first is required.
• path_or_buf: A string path to the file to write or a file object. If a file object it must be opened with newline=''
• sep : Field delimiter for the output file (default “,”)
• na_rep: A string representation of a missing value (default ‘’)
• float_format: Format string for floating point numbers
• columns: Columns to write (default None)
• header: Whether to write out the column names (default True)
• index: whether to write row (index) names (default True)
• index_label: Column label(s) for index column(s) if desired. If None (default), and header and index are True, then the index names are used. (A sequence should be given if the DataFrame uses MultiIndex).
• mode : Python write mode, default ‘w’
• encoding: a string representing the encoding to use if the contents are non-ASCII, for Python versions prior
to 3
• line_terminator: Character sequence denoting line end (default os.linesep)
• quoting: Set quoting rules as in csv module (default csv.QUOTE_MINIMAL). Note that if you have set a float_format then floats are converted to strings and csv.QUOTE_NONNUMERIC will treat them as non-numeric
• quotechar: Character used to quote fields (default ‘”’)
• doublequote: Control quoting of quotechar in fields (default True)
• escapechar: Character used to escape sep and quotechar when appropriate (default None)
• chunksize: Number of rows to write at a time
• date_format: Format string for datetime objects

Writing a formatted string
--------------------------
The DataFrame object has an instance method to_string which allows control over the string representation of the object. All arguments are optional:
• buf default None, for example a StringIO object
• columns default None, which columns to write
• col_space default None, minimum width of each column.
• na_rep default NaN, representation of NA value
• formatters default None, a dictionary (by column) of functions each of which takes a single argument and returns a formatted string
• float_format default None, a function which takes a single (float) argument and returns a formatted string; to be applied to floats in the DataFrame.
• sparsify default True, set to False for a DataFrame with a hierarchical index to print every MultiIndex key at each row.
• index_names default True, will print the names of the indices
• index default True, will print the index (ie, row labels)
• header default True, will print the column labels
• justify default left, will print column headers left- or right-justified
The Series object also has a to_string method, but with only the buf, na_rep, float_format arguments. There is also a length argument which, if set to True, will additionally output the length of the Series.

=============================================
2.4.2 JSON
Read and write JSON format files and strings.
=============================================
Writing JSON
A Series or DataFrame can be converted to a valid JSON string. Use to_json with optional parameters:
• path_or_buf : the pathname or buffer to write the output This can be None in which case a JSON string is
returned
• orient : Series:
– default is index
– allowed values are {split, records, index} DataFrame:
– default is columns
– allowed values are {split, records, index, columns, values, table}

The format of the JSON string
split       dict like {index -> [index], columns -> [columns], data -> [values]}
records     list like [{column -> value}, . . . , {column -> value}]
index       dict like {index -> {column -> value}}
columns     dict like {column -> {index -> value}}
values      just the values array

• date_format : string, type of date conversion, ‘epoch’ for timestamp, ‘iso’ for ISO8601.
• double_precision : The number of decimal places to use when encoding floating point values, default 10.
• force_ascii : force encoded string to be ASCII, default True.
• date_unit : The time unit to encode to, governs timestamp and ISO8601 precision. One of ‘s’, ‘ms’, ‘us’ or ‘ns’ for seconds, milliseconds, microseconds and nanoseconds respectively. Default ‘ms’.
• default_handler : The handler to call if an object cannot otherwise be converted to a suitable format for JSON. Takes a single argument, which is the object to convert, and returns a serializable object.
• lines : If records orient, then will write each record per line as json.
Note NaN’s, NaT’s and None will be converted to null and datetime objects will be converted based on the
date_format and date_unit parameters.

dfj = pd.DataFrame(np.random.choice(10, 6).reshape(3,2), columns=list("AB"))
print(dfj)
dfj.to_json()

Orient options
---------------
There are a number of different options for the format of the resulting JSON file / string. Consider the following DataFrame and Series:
dfjo = pd.DataFrame(
    dict(A=range(1,4), B=range(4, 7), C=range(7, 10)),
    columns=list("ABC"),
    index=list("xyz")
)
dfjo

sjo = pd.Series(dict(x=15, y=16, z=17), name='D')
sjo

Column oriented (the default for DataFrame) serializes the data as nested JSON objects with column labels acting as the primary index:
# Not available for Series
dfjo.to_json(orient='columns') # default

Index oriented (the default for Series) similar to column oriented but the index labels are now primary:
dfjo.to_json(orient="index")
sjo.to_json(orient="index")

Record oriented serializes the data to a JSON array of column -> value records, index labels are not included. This is useful for passing DataFrame data to plotting libraries, for example the JavaScript library d3.js:
dfjo.to_json(orient="records")
sjo.to_json(orient="records")

Value oriented is a bare-bones option which serializes to nested JSON arrays of values only, column and index labels are not included:
dfjo.to_json(orient="values")
# Not available for Series

Split oriented serializes to a JSON object containing separate entries for values, index and columns. Name is also included for Series:
dfjo.to_json(orient="split")
sjo.to_json(orient="split")

Table oriented serializes to the JSON Table Schema, allowing for the preservation of metadata including but not limited to dtypes and index names.
dfjo.to_json(orient="table")

Note: Any orient option that encodes to a JSON object will not preserve the ordering of index and column la- bels during round-trip serialization. If you wish to preserve label ordering use the split option as it uses ordered containers.

===============================
2.5 Indexing and selecting data
===============================
The axis labeling information in pandas objects serves many purposes:
• Identifies data (i.e. provides metadata) using known indicators, important for analysis, visualization, and inter- active console display.
• Enables automatic and explicit data alignment.
• Allows intuitive getting and setting of subsets of the data set.
In this section, we will focus on the final point: namely, how to slice, dice, and generally get and set subsets of pandas objects. The primary focus will be on Series and DataFrame as they have received more development attention in this area.

Note: The Python and NumPy indexing operators [] and attribute operator . provide quick and easy access to pandas data structures across a wide range of use cases. This makes interactive work intuitive, as there’s little new to learn if you already know how to deal with Python dictionaries and NumPy arrays. However, since the type of the data to be accessed isn’t known in advance, directly using standard operators has some optimization limits. For production code, we recommended that you take advantage of the optimized pandas data access methods exposed in this chapter.

 Warning: Whether a copy or a reference is returned for a setting operation, may depend on the context. This is sometimes called chained assignment and should be avoided.

2.5.1 Different choices for indexing
--------------------------------------
Object selection has had a number of user-requested additions in order to support more explicit location based index- ing. pandas now supports three types of multi-axis indexing.
• .loc is primarily label based, but may also be used with a boolean array. .loc will raise KeyError when the items are not found. Allowed inputs are:
– A single label, e.g. 5 or 'a' (Note that 5 is interpreted as a label of the index. This use is not an integer position along the index.).
– A list or array of labels ['a', 'b', 'c'].
– A slice object with labels 'a':'f' (Note that contrary to usual Python slices, both the start and the stop
are included, when present in the index! See Slicing with labels and Endpoints are inclusive.)
– A boolean array (any NA values will be treated as False).
– A callable function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above).
• .iloc is primarily integer position based (from 0 to length-1 of the axis), but may also be used with a boolean array. .iloc will raise IndexError if a requested indexer is out-of-bounds, except slice indexers which allow out-of-bounds indexing. (this conforms with Python/NumPy slice semantics). Allowed inputs are:
– An integer e.g. 5.
– A list or array of integers [4, 3, 0].
– A slice object with ints 1:7.
– A boolean array (any NA values will be treated as False).
– A callable function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above).
• .loc, .iloc, and also [] indexing can accept a callable as indexer. See more at Selection By Callable.
Getting values from an object with multi-axes selection uses the following notation (using .loc as an example, but the following applies to .iloc as well). Any of the axes accessors may be the null slice :. Axes left out of the specification are assumed to be:,e.g.p.loc['a']is equivalent to p.loc['a', :, :].
Object Type         Indexers
Series              s.loc[indexer]
DataFrame           df.loc[row_indexer,column_indexer]

2.5.2 Basics
------------
As mentioned when introducing the data structures in the last section, the primary function of indexing with [] (a.k.a. __getitem__ for those familiar with implementing class behavior in Python) is selecting out lower-dimensional slices. The following table shows return type values when indexing pandas objects with []:
ObjectType             Selection                ReturnValueType
Series                series[label]             scalar value
DataFrame             frame[colname]            Series corresponding to colname

Here we construct a simple time series data set to use for illustrating the indexing functionality:
dates=pd.date_range('1/2/2021', periods=8)
df = pd.DataFrame(np.random.randn(8, 4), columns=list("ABCD"),
                 index=dates)
df

Note: None of the indexing functionality is timeseries specific unless specifically stated.
Thus, as per above, we have the most basic indexing using []:
s = df['A']
s[dates[5]]

You can pass a list of columns to [] to select columns in that order. If a column is not contained in the DataFrame, an exception will be raised. Multiple columns can also be set in this manner:
df[['B', 'A']] = df[['A', 'B']]
df
You may find this useful for applying a transform (in-place) to a subset of the columns.
Warning: pandas aligns all AXES when setting Series and DataFrame from .loc, and .iloc. This will not modify df because the column alignment is before value assignment.
df.loc[:, ['B', 'A']] = df[['A', 'B']] ## This is not the correct way, no affect on the output
df[['A', 'B']]

The correct way to swap column values is by using raw values:
df.loc[:, ['B', 'A']]= df[['A', 'B']].to_numpy()

2.5.3 Attribute access
-----------------------
You may access an index on a Series or column on a DataFrame directly as an attribute:
sa = pd.Series([1, 2, 3], index=list('abc'))
sa.b
s['2021-01-07']
dfa = df.copy()
dfa.A

print(sa)
sa.a = 5
sa

dfa.A = list(range(len(dfa.index))) # # ok if A already exists
dfa

dfa.E = list(range(len(dfa.index))) ## Error - UserWarning: Pandas doesn't allow columns to be created via a new attribute name
dfa

# use below form to create a new ˓→column
dfa['E'] = list(range(len(dfa.index)))
dfa

Warning:
• You can use this access only if the index element is a valid Python identifier, e.g. s.1 is not allowed.
• The attribute will not be available if it conflicts with an existing method name, e.g. s.min is not allowed, but s['min'] is possible.
• Similarly, the attribute will not be available if it conflicts with any of the following list: index, major_axis, minor_axis, items.
• In any of these cases, standard indexing will still work, e.g. s['1'], s['min'], and s['index'] will access the corresponding element or column.

If you are using the IPython environment, you may also use tab-completion to see these accessible attributes.
You can also assign a dict to a row of a DataFrame:
x = pd.DataFrame({'x': [1, 2, 3], 'y': [3, 4, 5]})
x.iloc[1] = {'x':9, 'y':99}
x

You can use attribute access to modify an existing element of a Series or column of a DataFrame, but be careful; if you try to use attribute access to create a new column, it creates a new attribute rather than a new column.
df = pd.DataFrame({'one': [1., 2., 3.]})
df.two = [4, 5, 6] ## Error - UserWarning: Pandas doesn't allow columns to be created via a new attribute name
df

2.5.4 Slicing ranges
----------------------
The most robust and consistent way of slicing ranges along arbitrary axes is described in the Selection by Position section detailing the .iloc method. For now, we explain the semantics of slicing using the [] operator.
With Series, the syntax works exactly as with an ndarray, returning a slice of the values and the corresponding labels:
s[:5]
s[::2]
s[::-1]

#Note that setting works as well:
s2 = s.copy()
s2[:5] = 0
s2

With DataFrame, slicing inside of [] slices the rows. This is provided largely as a convenience since it is such a common operation.
df[:3]
df[::-1]

2.5.5 Selection by label
------------------------
 Warning: Whether a copy or a reference is returned for a setting operation, may depend on the context. This is sometimes called chained assignment and should be avoided.
 Warning: .loc is strict when you present slicers that are not compatible (or convertible) with the index type. For example using integers in a DatetimeIndex. These will raise a TypeError.
df1 = pd.DataFrame(np.random.randn(5, 4), columns=list('ABCD'), index=pd.date_range('20130101', periods=5))
#df1.loc[2:4] #TypeError: cannot do slice indexing on DatetimeIndex with these indexers [2] of type int
df1.iloc[2:4]
String likes in slicing can be convertible to the type of the index and lead to natural slicing.
df1.loc['2013-01-03': '2013-01-04']
df1.loc[['2013-01-03', '2013-01-04']]

pandas provides a suite of methods in order to have purely label based indexing. This is a strict inclusion based protocol. Every label asked for must be in the index, or a KeyError will be raised. When slicing, both the start bound AND the stop bound are included, if present in the index. Integers are valid labels, but they refer to the label and not the position.
The .loc attribute is the primary access method. The following are valid inputs:
• A single label, e.g. 5 or 'a' (Note that 5 is interpreted as a label of the index. This use is not an integer
position along the index.).
• A list or array of labels ['a', 'b', 'c'].
• A slice object with labels 'a':'f' (Note that contrary to usual Python slices, both the start and the stop are included, when present in the index! See Slicing with labels.
• A boolean array.
• A callable,
s1 = pd.Series(np.random.randn(6), index=list('abcdef'))
s1.loc['c':]
s1.loc['b']
s1.loc['c':] = 0
s1

With a DataFrame:
df1 = pd.DataFrame(np.random.randn(6, 4), index=list('abcdef'), columns=list('ABCD'))
df1.loc[['a', 'b', 'd'], :]
Accessing via label slices:
df1.loc['d':, 'A':'C']

For getting a cross section using a label (equivalent to df.xs('a')):
df1.loc['a']

For getting values with a boolean array:
df1.loc['a'] > 0

df1.loc[:, df1.loc['a'] > 0]

NA values in a boolean array propagate as False:
mask = pd.array([True, False, True, False, pd.NA, False], dtype="boolean")
mask
df1[mask]

For getting a value explicitly:
# this is also equivalent to ``df1.at['a','A']`
df1.loc['a', 'A']

Slicing with labels
--------------------
When using .loc with slices, if both the start and the stop labels are present in the index, then elements located between the two (including them) are returned:
s = pd.Series(list('abcde'), index=[0, 3, 2, 5, 4])
s.loc[3:5]

If at least one of the two is absent, but the index is sorted, and can be compared against start and stop labels, then slicing will still work as expected, by selecting labels which rank between the two:
s.sort_index()
s.sort_index().loc[1:6]

However, if at least one of the two is absent and the index is not sorted, an error will be raised (since doing otherwise would be computationally expensive, as well as potentially ambiguous for mixed type indexes). For instance, in the above example, s.loc[1:6] would raise KeyError.
s = pd.Series(list('abcdef'), index=[0, 3, 2, 5, 4, 2])
s.loc[3:5]
Also, if the index has duplicate labels and either the start or the stop label is dupulicated, an error will be raised. For instance, in the above example, s.loc[2:5] would raise a KeyError.

2.5.6 Selection by position
----------------------------
 Warning: Whether a copy or a reference is returned for a setting operation, may depend on the context. This is sometimes called chained assignment and should be avoided.
pandas provides a suite of methods in order to get purely integer based indexing. The semantics follow closely Python and NumPy slicing. These are 0-based indexing. When slicing, the start bound is included, while the upper bound is excluded. Trying to use a non-integer, even a valid label will raise an IndexError.
The .iloc attribute is the primary access method. The following are valid inputs: • An integer e.g. 5.
• A list or array of integers [4, 3, 0].
• A slice object with ints 1:7.
• A boolean array.
• A callable,

# this is also equivalent to ``df1.iat[1,1]``
df1.iloc[1, 1]
For getting a cross section using an integer position (equiv to df.xs(1)):
df1.iloc[1]

Out of range slice indexes are handled gracefully just as in Python/NumPy.
s.iloc[4:10]
Note that using slices that go out of bounds can result in an empty axis (e.g. an empty DataFrame being returned).

A single indexer that is out of bounds will raise an IndexError. A list of indexers where any element is out of bounds will raise an IndexError.
dfl.iloc[[4, 5, 6]] # IndexError: positional indexers are out-of-bounds
dfl.iloc[:, 4] # IndexError: positional indexers are out-of-bounds

2.5.7 Selection by callable
----------------------------
.loc, .iloc, and also [] indexing can accept a callable as indexer. The callable must be a function with one argument (the calling Series or DataFrame) that returns valid output for indexing.
df1.loc[lambda df: df['A'] > 0, :]
df1.loc[:, lambda df: ['A', 'B']]
df1.iloc[:, lambda df: [0, 1]]
df1[lambda df: df.columns[0]]

You can use callable indexing in Series.
df1['A'].loc[lambda s: s > 0]

Using these methods / indexers, you can chain data selection operations without using a temporary variable.
bb = pd.read_csv('data/baseball.csv', index_col='id')
bb.groupby(['year', 'team']).sum().loc[lambda df: df['r'] > 100]

2.5.8 Combining positional and label-based indexing
----------------------------------------------------
If you wish to get the 0th and the 2nd elements from the index in the ‘A’ column, you can do:
dfd = pd.DataFrame({'A': [1, 2, 3],
                    'B': [4, 5, 6]},
                   index=list('abc'))
dfd.A[['a', 'c']]
dfd.loc[['a','c'],'A']
dfd.loc[dfd.index[[0,2]], 'A']

This can also be expressed using .iloc, by explicitly getting locations on the indexers, and using positional indexing to select things.
dfd.iloc[[0, 2], dfd.columns.get_loc('A')]

For getting multiple indexers, using .get_indexer:
dfd.iloc[[0, 2], dfd.columns.get_indexer(['A', 'B'])]

Reindexing
----------
The idiomatic way to achieve selecting potentially not-found elements is via .reindex().
s = pd.Series([1, 2, 3])
s.reindex([1,2,3])

Alternatively, if you want to select only valid keys, the following is idiomatic and efficient; it is guaranteed to preserve the dtype of the selection.
s=pd.Series([ 1, 2, 3, ])
labels = [ 1, 2, 3, ]
s.loc[s.index.intersection(labels)]

Having a duplicated index will raise for a .reindex():
s = pd.Series(np.arange(4), index=['a', 'a', 'b', 'c'])
labels = ['c', 'd']
s.reindex(labels)
ValueError: cannot reindex from a duplicate axis

Generally, you can intersect the desired labels with the current axis, and then reindex.
s.loc[s.index.intersection(labels)].reindex(labels)

However, this would still raise if your resulting index is duplicated.
labels = ['a', 'd']
s.loc[s.index.intersection(labels)].reindex(labels)
ValueError: cannot reindex from a duplicate axis

2.5.10 Selecting random samples
--------------------------------
A random selection of rows or columns from a Series or DataFrame with the sample() method. The method will sample rows by default, and accepts a specific number of rows/columns to return, or a fraction of rows.
s = pd.Series([0, 1, 2, 3, 4, 5])

# When no arguments are passed, returns 1 row.
s.sample()

# One may specify either a number of rows:
s.sample(n=3)
s.sample(2)

# Or a fraction of the rows: below means return half of the rows from the series
s.sample(frac=0.5)

By default, sample will return each row at most once, but one can also sample with replacement using the replace option:
s = pd.Series([0, 1, 2, 3, 4, 5])
# Without replacement (default):
s.sample(6, replace=False)
# With replacement: i.e get duplicate rows
s.sample(n=6, replace=True)

By default, each row has an equal probability of being selected, but if you want rows to have different probabilities, you can pass the sample function sampling weights as weights. These weights can be a list, a NumPy array, or a Series, but they must be of the same length as the object you are sampling. Missing values will be treated as a weight of zero, and inf values are not allowed. If weights do not sum to 1, they will be re-normalized by dividing all weights by the sum of the weights. For example:
s = pd.Series([0, 1, 2, 3, 4, 5])
example_weights = [0, 0, 0.2, 0.2, 0.2, 0.4]
s.sample(n=3, weights=example_weights)

# Weights will be re-normalized automatically
example_weights2 = [0.5, 0, 0, 0, 0, 0]
s.sample(n=1, weights=example_weights2)

When applied to a DataFrame, you can use a column of the DataFrame as sampling weights (provided you are sampling rows and not columns) by simply passing the name of the column as a string.
df2 = pd.DataFrame({'col1': [9, 8, 7, 6], 'weight_column': [0.4, 0.3, 0.2, 0.1]})
df2.sample(3, weights='weight_column')

sample also allows users to sample columns instead of rows using the axis argument.
df3 = pd.DataFrame({'col1': [1, 2, 3], 'col2': [2, 3, 4]})
df3.sample(1, axis=1)

Finally, one can also set a seed for sample’s random number generator using the random_state argument, which will accept either an integer (as a seed) or a NumPy RandomState object.
df4 = pd.DataFrame({'col1': [1, 2, 3], 'col2': [2, 3, 4]})
# With a given seed, the sample will always draw the same rows.
df4.sample(n=2, random_state=2)

2.5.11 Setting with enlargement
--------------------------------
The .loc/[] operations can perform enlargement(changing the dtype to bigger dtype) when setting a non-existent key for that axis. In the Series case this is effectively an appending operation.
se = pd.Series([1, 2, 3])
se[5] = 5. # converted the series from int to float

A DataFrame can be enlarged on either axis via .loc.
dfi = pd.DataFrame(np.arange(6).reshape(3, 2), columns=['A', 'B'])
dfi.loc[:, 'C'] = dfi.loc[:, 'A'] ## This is not performant way
dfi['C'] = dfi['A'] ## use this instead
dfi.loc[3] = 5
dfi

2.5.12 Fast scalar value getting and setting
---------------------------------------------
Since indexing with [] must handle a lot of cases (single-label access, slicing, boolean indexing, etc.), it has a bit of overhead in order to figure out what you’re asking for. If you only want to access a scalar value, the fastest way is to use the at and iat methods, which are implemented on all of the data structures.
Similarly to loc, at provides label based scalar lookups, while, iat provides integer based lookups analogously to iloc
s.iat[5]
df.at[dates[5], 'A']
df.iat[3, 0]
You can also set using these same indexers.
df.at[dates[5], 'E'] = 7
df.iat[3, 0] = 7
at may enlarge the object in-place as above if the indexer is missing.
df.at[dates[-1] + pd.Timedelta('1 day'), 0] = 7

2.5.13 Boolean indexing
-----------------------
Another common operation is the use of boolean vectors to filter the data. The operators are: | for or, & for and, and ~ for not. These must be grouped by using parentheses, since by default Python will evaluate an expression such as df['A'] > 2 & df['B'] < 3 as df['A'] > (2 & df['B']) < 3, while the desired evaluation order is (df['A'] > 2) & (df['B'] < 3).
Using a boolean vector to index a Series works exactly as in a NumPy ndarray:
s = pd.Series(range(-3, 4))
s[s > 0]
s[(s < -1) | (s > 0.5)]
s[~(s < 0)]

You may select rows from a DataFrame using a boolean vector the same length as the DataFrame’s index (for example, something derived from one of the columns of the DataFrame):
df[df['A'] > 0]

List comprehensions and the map method of Series can also be used to produce more complex criteria:
df2 = pd.DataFrame({'a': ['one', 'one', 'two', 'three', 'two', 'one', 'six'],
                    'b': ['x', 'y', 'y', 'x', 'y', 'x', 'x'],
                    'c': np.random.randn(7)})
df2
# only want 'two' or 'three'
criterion = df2['a'].map(lambda x: x.startswith('t'))
df2[criterion]
# equivalent but slower
criterion = [x.startswith('t') for x in df2.a]
df2[criterion]
# Multiple criteria
df2[criterion & (df2['b'] == 'x')]

With the choice methods Selection by Label, Selection by Position, and Advanced Indexing you may select along more than one axis using boolean vectors combined with other indexing expressions.
df2.loc[criterion & (df2['b'] == 'x'), 'b':'c']

Warning: iloc supports two kinds of boolean indexing. If the indexer is a boolean Series, an error will be raised. For instance, in the following example, df.iloc[s.values, 1] is ok. The boolean indexer is an array.But df.iloc[s, 1] would raise ValueError.

df = pd.DataFrame([[1, 2], [3, 4], [5, 6]],
                  index=list('abc'),
                  columns=['A', 'B'])
df
s = (df['A'] > 2)
df.loc[s, 'B']
#df.iloc[s,1] # ValueError: Location based indexing can only have
df.iloc[s.values, 1] # works perfectly
# s returns a series but s.values returns a numpyarray
# Therefore iloc fails with s and works with s.values

2.5.14 Indexing with isin
--------------------------
Consider the isin() method of Series, which returns a boolean vector that is true wherever the Series elements exist in the passed list. This allows you to select rows where one or more columns have values you want:
s = pd.Series(np.arange(10,51,10), index=np.arange(5), dtype='int64')
s.isin([10, 30])
s[s.isin([10, 30])]

The same method is available for Index objects and is useful for the cases when you don’t know which of the sought labels are in fact present:
s.index.isin([2, 4])
s[s.index.isin([2, 4, 6])]

# compare it to the following
s.reindex([2, 4, 6]) ## reindex returns nan if the requested index is not present

In addition to that, MultiIndex allows selecting a separate level to use in the membership check:
s_mi = pd.Series(np.arange(6),
        index=pd.MultiIndex.from_product([[0, 1], ['a', 'b', 'c']]))
s_mi.index.isin([(1, 'a'), (2, 'b'), (0, 'c')])
s_mi[s_mi.index.isin([(1, 'a'), (2, 'b'), (0, 'c')])]

s_mi.iloc[s_mi.index.isin(['a', 'c', 'e'], level=1)]

DataFrame also has an isin() method. When calling isin, pass a set of values as either an array or dict. If values is an array, isin returns a DataFrame of booleans that is the same shape as the original DataFrame, with True wherever the element is in the sequence of values.
df = pd.DataFrame({'vals': [1, 2, 3, 4], 'ids': ['a', 'b', 'f', 'n'],
                   'ids2': ['a', 'n', 'c', 'n']})
values = [ 'a', 'b', 1, 2, ]
df[df.isin(values)]

Often times you’ll want to match certain values with certain columns. Just make values a dict where the key is the column, and the value is a list of items you want to check for.
df = pd.DataFrame({'vals': [1, 2, 3, 4], 'ids': ['a', 'b', 'f', 'n'],
                   'ids2': ['a', 'n', 'c', 'n']})
values = {'ids': ['a', 'b'], 'vals': [1, 3]}
df[df.isin(values)]

Combine DataFrame’s isin with the any() and all() methods to quickly select subsets of your data that meet a given criteria. To select a row where each column meets its own criterion:
df = pd.DataFrame({'vals': [1, 2, 3, 4], 'ids': ['a', 'b', 'f', 'n'],
                   'ids2': ['a', 'n', 'c', 'n']})
values = {'ids': ['a', 'b'], 'ids2': ['a', 'c'], 'vals': [1, 3]}
mask = df.isin(values).all(axis=1)
df[mask]

2.5.15 The where() Method and Masking
--------------------------------------
Selecting values from a Series with a boolean vector generally returns a subset of the data. To guarantee that selection
output has the same shape as the original data, you can use the where method in Series and DataFrame. To return only the selected rows:
s = pd.Series(range(-3, 4))
s[s>0]
To return a Series of the same shape as the original:
s.where(s>0)

Selecting values from a DataFrame with a boolean criterion now also preserves input data shape. where is used under the hood as the implementation.The code below is equivalent to df.where(df < 0).
dates=pd.date_range('1/2/2021', periods=8)
df = pd.DataFrame(np.random.randn(8, 4), columns=list("ABCD"),
                 index=dates)
df[df<0]

In addition, where takes an optional other argument for replacement of values where the condition is False, in the returned copy.
df.where(df<0, -df)

You may wish to set values based on some boolean criteria. This can be done intuitively like so:
s2 = s.copy()
s2[s2 < 0] = 0
s2

df2= df.copy()
df2[df2<0] = 0
df2

By default, where returns a modified copy of the data. There is an optional parameter inplace so that the original data can be modified without creating a copy:
df_orig = df.copy()
df_orig.where(df_orig>0, 0., inplace=True)
df_orig

Note: The signature for DataFrame.where() differs from numpy.where(). Roughly df1.where(m, df2)is equivalent tonp.where(m, df1, df2)
df.where(df < 0, -df) == np.where(df < 0, df, -df)

which is faster?
np.where is much faster then df.where.
%timeit np.where(df < 0, df, -df) # 276 µs ± 27.7 µs per loop
%timeit df.where(df < 0, -df) # 1.22 ms ± 113 µs per loop

Alignment
Furthermore, where aligns the input boolean condition (ndarray or DataFrame), such that partial selection with setting is possible. This is analogous to partial setting via .loc (but on the contents rather than the axis labels).
df2 = df.copy()
print(df2)
df2[df2[1:4] > 0] = 3
df2

Where can also accept axis and level parameters to align the input when performing the where.
df2 = DataFrame({
    'a':np.arange(-3,3),
    'b':np.arange(-2,4),
    'c':np.arange(-1,5)
})
print(df2)
df2.where(df2>0, df2['a'], axis='index')

another example:
df2 = DataFrame({
    'a':np.arange(-3,3),
    'b':np.arange(-2,4),
    'c':np.arange(-1,5)
})
print(df2)
print(df2.where(df2>0, df2['a'], axis=0))

This is equivalent to (but faster than) the following.
df2.apply(lambda x,y: x.where(x>0,y), y=df2['a'])

how fast is df.where then apply?
%timeit df2.where(df2>0, df2['a'], axis=0) # 939 µs ± 31.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
%timeit df2.apply(lambda x,y: x.where(x>0,y), y=df2['a']) # 2.99 ms ± 147 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

np.where is faster than both:
%timeit np.where(df2>0, df2, df2[list(len(df2.columns)*'a')]) # 809 µs ± 29.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

where can accept a callable as condition and other arguments. The function must be with one argument (the calling Series or DataFrame) and that returns valid output as condition and other argument.
df3 = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [7, 8, 9]})
df3.where(lambda x:x>4, lambda x:x+10)
df3.where(df3>4, lambda x:x+10)

Mask
-----
mask() is the inverse boolean operation of where.
print(s.where(s>0))
s.mask(s>0)
df3.mask(df3>4, lambda x:x+10)

2.5.16 Setting with enlargement conditionally using numpy()
------------------------------------------------------------
An alternative to where() is to use numpy.where(). Combined with setting a new column, you can use it to
enlarge a dataframe where the values are determined conditionally.
Consider you have two choices to choose from in the following dataframe. And you want to set a new column color to ‘green’ when the second column has ‘Z’. You can do the following:
df = pd.DataFrame({'col1': list('ABBC'), 'col2': list('ZZXY')})
df['color'] = np.where(df['col2']=='Z', 'green', 'red')
df

If you have multiple conditions, you can use numpy.select() to achieve that. Say corresponding to three conditions there are three choice of colors, with a fourth color as a fallback, you can do the following.
conditions = [
    (df['col1']=='A') & (df['col2']=='Z'),
    (df['col1']=='B') & (df['col2']=='Z'),
    (df['col1']=='B')
]
choices = ['yellow', 'blue', 'blue']
df['color'] = np.select(conditions, choices, default='black')
df

2.5.17 The query() Method
--------------------------
DataFrame objects have a query() method that allows selection using an expression.
You can get the value of the frame where column b has values between the values of columns a and c. For example:
df = DataFrame({
    'a': np.random.choice(10, 5),
    'b': np.random.choice(10, 5),
    'c': np.random.choice(10, 5),
})

# pure python
df[(df['a'] < df['b']) & (df['b'] < df['c'])]

# query
df.query('(a<b) & (b<c)')

which one is faster?
pure python is much faster than query
%timeit df[(df['a'] < df['b']) & (df['b'] < df['c'])] # 679 µs ± 22.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
%timeit df.query('(a<b) & (b<c)') # 1.89 ms ± 63.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

Do the same thing but fall back on a named index if there is no column with the name a.
df.set_index('a', inplace=True)
df[(df['a'] < df['b']) & (df['b'] < df['c'])]
df.query('a<b & b<c')

If instead you don’t want to or cannot name your index, you can use the name index in your query expression:
df.query('index<b<c')

Note: If the name of your index overlaps with a column name,the column name is given precedence. For example,
df = df.reset_index()
df.index.name='a'
df.query('a<b<c')

You can still use the index in a query expression by using the special identifier ‘index’:
df.query('index > 2')
If for some reason you have a column named index, then you can refer to the index as ilevel_0 as well, but at this point you should consider renaming your columns to something less ambiguous.

MultiIndex query() Syntax
You can also use the levels of a DataFrame with a MultiIndex as if they were columns in the frame:
n=10
colors = np.random.choice(['green', 'red'], size=n)
foods = np.random.choice(['eggs', 'ham'], size=n)
index = pd.MultiIndex.from_arrays([colors, foods], names=['color', 'food'])
df = pd.DataFrame(np.random.choice(n, 20).reshape(10,2), index=index)
df.query('color=="red"')

If the levels of the MultiIndex are unnamed, you can refer to them using special names:
df.index.names = [None, None]
df.query('ilevel_0 == "red"')
The convention is ilevel_0, which means “index level 0” for the 0th level of the index.

query() Use Cases
-----------------
A use case for query() is when you have a collection of DataFrame objects that have a subset of column names (or index levels/names) in common. You can pass the same query to both frames without having to specify which frame you’re interested in querying
df1 = DataFrame({
    'a':np.random.choice(10, 5),
    'b':np.random.choice(10, 5),
    'c':np.random.choice(10, 5),
})

df2 = DataFrame({
    'a':np.random.choice(np.arange(5,15,3), 8),
    'b':np.random.choice(np.arange(5,13,2), 8),
    'c':np.random.choice(np.arange(3,11,3), 8),
})
expr = '4<a<c<9'
map(lambda frame:frame.query(expr), [df1, df2])

query() Python versus pandas Syntax Comparison
-----------------------------------------------
Full numpy-like syntax:
df = pd.DataFrame(np.random.randint(n, size=(n, 3)), columns=list('abc'))
df.query('(a < b) & (b < c)')
df[(df['a'] < df['b']) & (df['b'] < df['c'])]

Slightly nicer by removing the parentheses (by binding making comparison operators bind tighter than & and |).
df.query('a < b & b < c')
Use English instead of symbols:
df.query('a < b and b < c')
Pretty close to how you might write it on paper:
df.query('a < b < c')

The in and not in operators
---------------------------
query() also supports special use of Python’s in and not in comparison operators, providing a succinct syntax
for calling the isin method of a Series or DataFrame.
# get all rows where columns "a" and "b" have overlapping values
df = pd.DataFrame({'y': list('aabbccddeeff'), 'z': list('aaaabbbbcccc'),
                   'c': np.random.randint(5, size=12),
                   'd': np.random.randint(9, size=12)})
df.query('y in z')
# How you'd do it in pure Python
df[df['y'].isin(df['z'])]
df.query('y not in z')
# pure Python
df[~df['y'].isin(df['z'])]

You can combine this with other expressions for very succinct queries:
# rows where cols a and b have overlapping values # and col c's values are less than col d's
df.query('y in z and c > d')
#pure python
df[(df.y.isin(df.z)) & (df.c > df.d)]

Note: Note that in and not in are evaluated in Python, since numexpr has no equivalent of this operation. However,only the in/not in expression itself is evaluated in vanilla Python.For example, in the expression
df.query('a in b + c + d')
(b + c + d) is evaluated by numexpr and then the in operation is evaluated in plain Python. In general, any
operations that can be evaluated using numexpr will be.

Special use of the == operator with list objects
-------------------------------------------------
Comparing a list of values to a column using ==/!= works similarly to in/not in.
df.query('z == ["y", "z", "c"]')
it is equivalent to  - df.query('z in ["y", "z", "c"]')

which one is faster in or ==?
both are almost same

df.query('z == ["y", "z", "c"]')
#pure python
df[df['z'].isin(['y','z','c'])]

df.query('c == [1, 2]')
df.query('c != [1, 2]')

df.query('[1, 2] in c')
df.query('[1, 2] not in c')
# pure Python
df[df['c'].isin([1, 2])]

Boolean operators
------------------
You can negate boolean expressions with the word not or the ~ operator.
df = pd.DataFrame(np.random.rand(n, 3), columns=list('abc'))
df['bools'] = np.random.rand(len(df)) > 0.5
df.query('~bools')
df.query('not bools')
df.query('not bools') == df[~df['bools']]

Of course, expressions can be arbitrarily complex too:
# short query syntax
shorter = df.query('a < b < c and (not bools) or bools > 2')
# equivalent in pure Python
longer = df[(df['a'] < df['b']) & (df['b'] < df['c']) & (~df['bools']) | (df['bools'] > 2)]

Performance of query()
DataFrame.query() using numexpr is slightly faster than Python for large frames.
Note: You will only see the performance benefits of using the numexpr engine with DataFrame.query() if your frame has more than approximately 200,000 rows.

2.5.18 Duplicate data
---------------------
If you want to identify and remove duplicate rows in a DataFrame, there are two methods that will help: duplicated and drop_duplicates. Each takes as an argument the columns to use to identify duplicated rows.
• duplicated returns a boolean vector whose length is the number of rows, and which indicates whether a row is duplicated.
• drop_duplicates removes duplicate rows.
By default, the first observed row of a duplicate set is considered unique, but each method has a keep parameter to
specify targets to be kept.
• keep='first' (default): mark / drop duplicates except for the first occurrence. • keep='last': mark / drop duplicates except for the last occurrence.
• keep=False: mark / drop all duplicates.
df2 = pd.DataFrame({'a': ['one', 'one', 'two', 'two', 'two', 'three', 'four'],
                   'b': ['x', 'y', 'x', 'y', 'x', 'x', 'x'],
                   'c': np.random.randn(7)})
print(df2)
df2.duplicated('a')
df2.duplicated('a', keep='last')
df2.duplicated('a', keep=False)

df2.drop_duplicates('a')
df2.drop_duplicates('a', keep='last')
df2.drop_duplicates('a', keep=False)

Also, you can pass a list of columns to identify duplications.
df2.duplicated(['a', 'b'])
df2.drop_duplicates(['a', 'b'])

To drop duplicates by index value, use Index.duplicated then perform slicing. The same set of options are available for the keep parameter.
df3 = pd.DataFrame({'a': np.arange(6), 'b': np.random.randn(6)},
                   index=['a', 'a', 'b', 'c', 'b', 'a'])
print(df3)
df3.index.duplicated()
df3[~df3.index.duplicated()]
df3[~df3.index.duplicated(keep='last')]
df3[~df3.index.duplicated(keep=False)]

df3.index.drop_duplicates()
df3.index.drop_duplicates(keep='last')
df3.index.drop_duplicates(keep=False)

2.5.19 Dictionary-like get() method
------------------------------------
Each of Series or DataFrame have a get method which can return a default value.
s = pd.Series([1, 2, 3], index=['a', 'b', 'c'])
s = pd.Series([1, 2, 3], index=['a', 'b', 'c'])
s.get('a')
s.get('x', -1)
s.get('x', default=-2)

If you want to get a scaler at is the fastest:
%timeit df2.at[3,'a'] # 4.76 µs ± 234 ns per loop
%timeit df2.get('a').get(3) # 5.87 µs ± 56.7 ns per loop
%timeit df2.loc[3,'a'] # 8.49 µs ± 264 ns per loop

2.5.20 Looking up values by index/column labels
------------------------------------------------
Sometimes you want to extract a set of values given a sequence of row labels and column labels, this can be achieved by DataFrame.melt combined by filtering the corresponding rows with DataFrame.loc. For instance:
df = pd.DataFrame({'col': ["A", "A", "B", "B"],
                   'A': [80, 23, np.nan, 22],
                   'B': [80, 55, 76, 67]})
melt = df.melt('col')
melt.loc[melt.col == melt.variable, 'value']
melt = melt.get('value').get(melt.col == melt.variable) # get is faster, use it
melt.reset_index(drop=True)

2.5.21 Index objects
---------------------
The pandas Index class and its subclasses can be viewed as implementing an ordered multiset. Duplicates are allowed. However, if you try to convert an Index object with duplicate entries into a set, an exception will be raised.
Index also provides the infrastructure necessary for lookups, data alignment, and reindexing. The easiest way to create an Index directly is to pass a list or other sequence to Index:
index = pd.Index(['e', 'd', 'a', 'b'])
'd' in index
You can also pass a name to be stored in the index:
index = pd.Index(['e', 'd', 'a', 'b'], name='something')
index.name

The name, if set, will be shown in the console display:
index = pd.Index(np.arange(5), name='rows')
columns = pd.Index(['A', 'B', 'C'], name='cols')
df = pd.DataFrame(np.random.randn(5,3), index=index, columns=columns)
df['A']
df.iloc[0]

Setting metadata
-----------------
Indexes are “mostly immutable”, but it is possible to set and change their name attribute. You can use the rename, set_names to set these attributes directly, and they default to returning a copy.
ind = pd.Index([1, 2, 3])
ind.name = 'ramesh'
ind
ind = ind.rename('suresh')
ind.set_names(['chandu'], inplace=True)
ind.name = 'Rupesh'
ind

set_names, set_levels, and set_codes also take an optional level argument
index = pd.MultiIndex.from_product([range(3), ['one', 'two']], names=['first', 'second'])
index.levels[0]
index.set_levels(['a','b'], level=1)

Set operations on Index objects
--------------------------------
The two main operations are union and intersection. Difference is provided via the .difference() method.
a = pd.Index(['c', 'b', 'a'])
b = pd.Index(['c', 'e', 'd'])
a.difference(b)

Also available is the symmetric_difference operation, which returns elements that appear in either idx1 or idx2, but not in both. This is equivalent to the Index created by idx1.difference(idx2).union(idx2. difference(idx1)), with duplicates dropped.
x = pd.Index(['a','b','c'])
y = pd.Index(['c','d','e'])
x.symmetric_difference(y)

Note: The resulting index from a set operation will be sorted in ascending order.

When performing Index.union() between indexes with different dtypes, the indexes must be cast to a common dtype. Typically, though not always, this is object dtype. The exception is when performing a union between integer and float data. In this case, the integer values are converted to float
idx1 = pd.Index([0, 1, 2])
idx2 = pd.Index([0.5, 1.5])
idx1.union(idx2)

idx1 = pd.Index([0, 1, 2, 0.5])
idx2 = pd.Index([0.5, 1.5])
idx1.union(idx2)

Missing values
---------------
Important: Even though Index can hold missing values (NaN), it should be avoided if you do not want any unexpected results. For example, some operations exclude missing values implicitly.
Index.fillna fills missing values with specified scalar value.
idx1 = pd.Index([1, np.nan, 3, 4])
idx1 = idx1.fillna(2)
idx1

idx2 = pd.DatetimeIndex([pd.Timestamp('2011-01-01'),
                         pd.NaT, pd.Timestamp('2011-01-03')])
idx2 = idx2.fillna(pd.Timestamp('2011-01-02'))
idx2

2.5.22 Set / reset index
-------------------------
Occasionally you will load or create a data set into a DataFrame and want to add an index after you’ve already done so. There are a couple of different ways.
Set an index
-------------
DataFrame has a set_index() method which takes a column name (for a regular Index) or a list of column names (for a MultiIndex). To create a new, re-indexed DataFrame:
data = pd.DataFrame({
    'a':['bar', 'bar', 'foo', 'foo'],
    'b':['one', 'two', 'one', 'two'],
    'c':['z', 'y', 'x', 'w'],
    'd':np.arange(1,5.)
})
index1 = data.set_index('c')
index2 = data.set_index(['a', 'b'])

The append keyword option allow you to keep the existing index and append the given columns to a MultiIndex:
frame = data.set_index('c', drop=False)
frame = frame.set_index(['a', 'b'], append=True)
frame

Other options in set_index allow you not drop the index columns or to add the index in-place (without creating a new object):
data.set_index('c', drop=False)
data.set_index(['a', 'b'], inplace=True)
data

Reset the index
----------------
As a convenience, there is a new function on DataFrame called reset_index() which transfers the index values into the DataFrame’s columns and sets a simple integer index. This is the inverse operation of set_index().
data.reset_index()

The output is more similar to a SQL table or a record array. The names for the columns derived from the index are the ones stored in the names attribute.
You can use the level keyword to remove only a portion of the index:
frame.reset_index(level=[1,2])

reset_index takes an optional parameter drop which if true simply discards the index, instead of putting index values in the DataFrame’s columns.
frame.reset_index(level=[1,2], drop=True)
frame.reset_index(level=[1,2], drop=True, inplace=True)

Adding an ad hoc index
If you create an index yourself, you can just assign it to the index field:
data.index = index[:4]
data

2.5.23 Returning a view versus a copy
--------------------------------------
dfmi = pd.DataFrame([list('abcd'),
                     list('efgh'),
                     list('ijkl'),
                     list('mnop')],
                    columns=pd.MultiIndex.from_product([['one', 'two'],
                                                        ['first', 'second']]
                                                        ))
dfmi['one']['second']
dfmi.loc[:, ('one', 'second')]

Both command above returns the same result?
Which one is faster?
.loc is faster.
%timeit dfmi.loc[:, ('one', 'second')] # 139 µs ± 643 ns per loop (mean ± std. dev. of 7 runs, 10000
%timeit dfmi['one']['second'] # 578 µs ± 56.6 µs per loop (mean ± std. dev. of 7 runs, 1000

These both yield the same results, so which should you use? It is instructive to understand the order of operations on these and why method 2 (.loc) is much preferred over method 1 (chained []).
dfmi['one'] selects the first level of the columns and returns a DataFrame that is singly-indexed. Then an- other Python operation dfmi_with_one['second'] selects the series indexed by 'second'. This is indicated by the variable dfmi_with_one because pandas sees these operations as separate events. e.g. separate calls to __getitem__, so it has to treat them as linear operations, they happen one after another.
Contrast this to df.loc[:,('one','second')] which passes a nested tuple of (slice(None),('one', 'second')) to a single call to __getitem__. This allows pandas to deal with this as a single entity. Furthermore this order of operations can be significantly faster, and allows one to index both axes if so desired.

Why does assignment fail when using chained indexing?
The problem in the previous section is just a performance issue. What’s up with the SettingWithCopy warning? We don’t usually throw warnings around when you do something that might cost a few extra milliseconds!
But it turns out that assigning to the product of chained indexing has inherently unpredictable results. To see this, think about how the Python interpreter executes this code:
dfmi.loc[:, ('one', 'second')] = value
# becomes
dfmi.loc.__setitem__((slice(None), ('one', 'second')), value)

But this code is handled differently:
dfmi['one']['second'] = value
# becomes dfmi.__getitem__('one').__setitem__('second', value)
See that __getitem__ in there? Outside of simple cases, it’s very hard to predict whether it will return a view or a copy (it depends on the memory layout of the array, about which pandas makes no guarantees), and therefore whether the __setitem__ will modify dfmi or a temporary object that gets thrown out immediately afterward. That’s what SettingWithCopy is warning you about!

Note: You may be wondering whether we should be concerned about the loc property in the first example. But dfmi.loc is guaranteed to be dfmi itself with modified indexing behavior, so dfmi.loc.__getitem__ / dfmi.loc.__setitem__ operate on dfmi directly. Of course, dfmi.loc.__getitem__(idx) may be a view or a copy of dfmi.

Sometimes a SettingWithCopy warning will arise at times when there’s no obvious chained indexing going on. These are the bugs that SettingWithCopy is designed to catch! pandas is probably trying to warn you that you’ve done this:
def do_something(df):
    foo = df[['bar', 'baz']] # Is foo a view? A copy? Nobody knows!
    # ... many lines here ...
    # We don't know whether this will modify df or not!
    foo['quux'] = value
    return foo

Evaluation order matters
When you use chained indexing, the order and type of the indexing operation partially determine whether the result is a slice into the original object, or a copy of the slice.
pandas has the SettingWithCopyWarning because assigning to a copy of a slice is frequently not intentional, but a mistake caused by chained indexing returning a copy where a slice was expected.
If you would like pandas to be more or less trusting about assignment to a chained indexing expression, you can set the option mode.chained_assignment to one of these values:
• 'warn', the default, means a SettingWithCopyWarning is printed.
• 'raise' means pandas will raise a SettingWithCopyException you have to deal with. • None will suppress the warnings entirely.
dfb = pd.DataFrame({'a': ['one', 'one', 'two','three', 'two', 'one', 'six'],
                    'c': np.arange(7)})
dfb
# This will show the SettingWithCopyWarning
# but the frame values will be set
dfb['c'][dfb['a'].str.startswith('o')]=42
dfb

This however is operating on a copy and will not work.
pd.set_option('mode.chained_assignment','warn')
dfb[dfb['a'].str.startswith('o')]['c'] = 43
dfb

A chained assignment can also crop up in setting in a mixed dtype frame.
Note: These setting rules apply to all of .loc/.iloc.

The following is the recommended access method using .loc for multiple items (using mask) and a single item
 using a fixed index:
dfc = pd.DataFrame({'a': ['one', 'one', 'two','three', 'two', 'one', 'six'],
                    'c': np.arange(7)})
dfd = dfc.copy()
# Setting multiple items using a mask
mask = dfd['a'].str.startswith('o')
dfd.loc[mask, 'c'] = 42
dfd

# Setting a single item
dfd = dfc.copy()
dfd.loc[2, 'a'] = 11
dfd

The following can work at times, but it is not guaranteed to, and therefore should be avoided:
dfd = dfc.copy()
dfd['a'][2] = 111
dfd

Last, the subsequent example will not work at all, and so should be avoided:
pd.set_option('mode.chained_assignment','raise')
dfd.loc[0]['a'] = 1111

 Warning: The chained assignment warnings / exceptions are aiming to inform the user of a possibly invalid assignment. There may be false positives; situations where a chained assignment is inadvertently reported.

RULE of Thumb - Whenever assigning a value to a slice of df, better make its copy and assign the value using loc.

-------------Setting with copy warning,real python-------------------------
https://realpython.com/pandas-settingwithcopywarning/

Now that you have a DataFrame to work with, let’s try to get a SettingWithCopyWarning. You’ll take all values from column z that are less than fifty and replace them with zeros. You can start by creating a mask, or a filter with Pandas Boolean operators:

data = {
    "x": 2**np.arange(5),
    "y": 3**np.arange(5),
    "z": np.array([ 45, 98, 24, 11, 64, ])
}
index = [ "a", "b", "c", "d", "e"]
df = pd.DataFrame(data=data, index=index)
print(df)
mask = df["z"]<50
df[mask]

mask is an instance of a Pandas Series with Boolean data and the indices from df
If you try to change df by extracting rows a, c, and d using mask, you’ll get a SettingWithCopyWarning, and df will remain the same:
df[mask]["z"] = 0
df
<ipython-input-208-83909dadc682>:10: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

As you can see, the assignment of zeros to the column z fails.
Here’s what happens in the code sample above:
df[mask] returns a completely new DataFrame (outlined in purple). This DataFrame holds a copy of the data from df that correspond to True values from mask (highlighted in green).
df[mask]["z"] = 0 modifies the column z of the new DataFrame to zeros, leaving df untouched.

Usually, you don’t want this! You want to modify df and not some intermediate data structure that isn’t referenced by any variable. That’s why Pandas issues a SettingWithCopyWarning and warns you about this possible mistake.

In this case, the proper way to modify df is to apply one of the accessors .loc[], .iloc[], .at[], or .iat[]:
df = pd.DataFrame(data=data, index=index)
df.loc[mask, "z"] = 0
df

This approach enables you to provide two arguments, mask and "z", to the single method that assigns the values to the DataFrame.
An alternative way to fix this issue is to change the evaluation order:
df = pd.DataFrame(data=data, index=index)
df["z"]
df["z"][mask] = 0

This works! You’ve modified df. Here’s what this process looks like:
Here’s a breakdown of the image::

df["z"] returns a Series object (outlined in purple) that points to the same data as the column z in df, not its copy.
df["z"][mask] = 0 modifies this Series object by using chained assignment to set the masked values (highlighted in green) to zero.
df is modified as well since the Series object df["z"] holds the same data as df.

You’ve seen that df[mask] contains a copy of the data, whereas df["z"] points to the same data as df. The rules used by Pandas to determine whether or not you make a copy are very complex. Fortunately, there are some straightforward ways to assign values to DataFrames and avoid a SettingWithCopyWarning.
Invoking accessors is usually considered better practice than chained assignment for these reasons:

The intention to modify df is clearer to Pandas when you use a single method.
The code is cleaner for readers.
The accessors tend to have better performance, even though you won’t notice this in most cases.
However, using accessors sometimes isn’t enough. They might also return copies, in which case you can get a SettingWithCopyWarning:

df = pd.DataFrame(data=data, index=index)
df.loc[mask]["z"] = 0
In this example, as in the previous one, you use the accessor .loc[]. The assignment fails because df.loc[mask] returns a new DataFrame with a copy of the data from df. Then df.loc[mask]["z"] = 0 modifies the new DataFrame, not df.
Generally, to avoid a SettingWithCopyWarning in Pandas, you should do the following:

AVOID CHAINED ASSIGNMENTS that combine two or more indexing operations like df["z"][mask] = 0 and df.loc[mask]["z"] = 0.
APPLY SINGLE ASSIGNMENTS with just one indexing operation like df.loc[mask, "z"] = 0. This might (or might not) involve the use of accessors, but they are certainly very useful and are often preferable.
With this knowledge, you can successfully avoid the SettingWithCopyWarning and any unwanted behavior in most cases.

Views and Copies in NumPy and Pandas
------------------------------------
Understanding views and copies is an important part of getting to know how NumPy and Pandas manipulate data. It can also help you avoid errors and performance bottlenecks. Sometimes data is copied from one part of memory to another, but in other cases two or more objects can share the same data, saving both time and memory.
Let’s start by creating a NumPy array:
arr = 2**np.arange(7)
arr

Now that you have arr, you can use it to create other arrays. Let’s first extract the second and fourth elements of arr (2 and 8) as a new array. There are several ways to do this:
arr[1:4:2] # array([2, 8])
arr[[1, 3]] # array([2, 8])

it’s important to notice that both statements return array([2, 8]). However, they have different behavior under the surface:
arr[1:4:2].base # array([ 1,  2,  4,  8, 16, 32, 64])
arr[1:4:2].flags.owndata # False

arr[[1, 3]].base
arr[[1, 3]].flags.owndata # True

This might seem odd at the first sight. The difference is in the fact that arr[1:4:2] returns a shallow copy, while arr[[1, 3]] returns a deep copy. Understanding this difference is essential not only for dealing with the SettingWithCopyWarning but also for manipulating big data with NumPy and Pandas

Shallow copy -
tarr = 2**np.arange(7)
tta = tarr[1:4:2]
print(tarr, id(tarr)) # [ 1  2  4  8 16 32 64] 4983419728
print(tta, id(tta)) # [2 8] 4983404384
tta[0] = 999
print(tta, tarr) # [999   8] [  1 999   4   8  16  32  64]

Deep copy -
tarr = 2**np.arange(7)
tdc = tarr[[1,3]]
print(tarr, id(tarr))
print(tdc, id(tdc))
tdc[0] = 999
print(tta, tarr)

Views in NumPy
--------------
A shallow copy or view is a NumPy array that doesn’t have its own data. It looks at, or “views,” the data contained in the original array. You can create a view of an array with .view():
view_of_arr = arr.view()
view_of_arr.base # array([ 1,  2,  4,  8, 16, 32, 64])
view_of_arr.base is arr # True
print(id(arr), id(view_of_arr.base), id(view_of_arr)) # 4503673712 4503673712 4983405984

You’ve obtained the array view_of_arr, which is a view, or shallow copy, of the original array arr. The attribute .base of view_of_arr is arr itself. In other words, view_of_arr doesn’t own any data—it uses the data that belongs to arr. You can also verify this with the attribute .flags:
view_of_arr.flags.owndata # False

Copies in NumPy
---------------
A deep copy of a NumPy array, sometimes called just a copy, is a separate NumPy array that has its own data. The data of a deep copy is obtained by copying the elements of the original array into the new array. The original and the copy are two separate instances. You can create a copy of an array with .copy():

copy_of_arr = arr.copy()
copy_of_arr
copy_of_arr.base is None # True
copy_of_arr.flags.owndata # True

As you can see, copy_of_arr doesn’t have .base. To be more precise, the value of copy_of_arr.base is None. The attribute .flags.owndata is True. This means that copy_of_arr owns data:

Differences Between Views and Copies
There are two very important differences between views and copies:

Views don’t need additional storage for data, but copies do.
Modifying the original array affects its views, and vice versa. However, modifying the original array will not affect its copy.
To illustrate the first difference between views and copies, let’s compare the sizes of arr, view_of_arr, and copy_of_arr. The attribute .nbytes returns the memory consumed by the elements of the array:
arr.nbytes # 48
>>> view_of_arr.nbytes # 48
>>> copy_of_arr.nbytes # 48

The amount of memory is the same for all arrays: 48 bytes. Each array looks at six integer elements of 8 bytes (64 bits) each. That’s 48 bytes in total.
However, if you use sys.getsizeof() to get the memory amount directly attributed to each array, then you’ll see the difference:
from sys import getsizeof
>>> getsizeof(arr) # 144
>>> getsizeof(view_of_arr) # 96
>>> getsizeof(copy_of_arr) # 144

arr and copy_of_arr hold 144 bytes each. As you’ve seen previously, 48 bytes out of the 144 total are for the data elements. The remaining 96 bytes are for other attributes. view_of_arr holds only those 96 bytes because it doesn’t have its own data elements.
To illustrate the second difference between views and copies, you can modify any element of the original array:

arr[1] = 64
>>> arr # array([ 1,  64,   4,   8,  16,  32])
>>> view_of_arr # array([ 1,  64,   4,   8,  16,  32])
>>> copy_of_arr # array([ 1,  2,  4,  8, 16, 32])

The view is modified because it looks at the elements of arr, and its .base is the original array. The copy is unchanged because it doesn’t share data with the original, so a change to the original doesn’t affect it at all

Understanding Views and Copies in Pandas
-----------------------------------------
Pandas also makes a distinction between views and copies. You can create a view or copy of a DataFrame with .copy(). The parameter deep determines if you want a view (deep=False) or copy (deep=True). deep is True by default, so you can omit it to get a copy:
df = pd.DataFrame(data=data, index=index)
view_of_df = df.copy(deep=False)
copy_of_df = df.copy()

At first, the view and copy of df look the same. If you compare their NumPy representations, though, then you may notice this subtle difference:
view_of_df.to_numpy().base is df.to_numpy().base # True
copy_of_df.to_numpy().base is df.to_numpy().base # False

Here, .to_numpy() returns the NumPy array that holds the data of the DataFrames. You can see that df and view_of_df have the same .base and share the same data. On the other hand, copy_of_df contains different data.
You can verify this by modifying df:

df["z"] = 0
view_of_df
copy_of_df
You’ve assigned zeros to all elements of the column z in df. That causes a change in view_of_df, but copy_of_df remains unmodified.

Rows and column labels also exhibit the same behavior:
view_of_df.index is df.index # True
view_of_df.columns is df.columns # True
copy_of_df.index is df.index # True
copy_of_df.columns is df.columns # True

df and view_of_df share the same row and column labels, while copy_of_df has separate index instances. Keep in mind that you can’t modify particular elements of .index and .columns. They are immutable objects.

Indices and Slices in NumPy and Pandas
---------------------------------------
Basic indexing and slicing in NumPy is similar to the indexing and slicing of lists and tuples. However, both NumPy and Pandas provide additional options to reference and assign values to the objects and their parts.

NumPy arrays and Pandas objects (DataFrame and Series) implement special methods that enable referencing, assigning, and deleting values in a style similar to that of containers:

.__getitem__() references values.
.__setitem__() assigns values.
.__delitem__() deletes values.

When you’re referencing, assigning, or deleting data in Python container-like objects, you often call these methods:

var = obj[key] is equivalent to var = obj.__getitem__(key).
obj[key] = value is equivalent to obj.__setitem__(key, value).
del obj[key] is equivalent to obj.__delitem__(key).
The argument key represents the index, which can be an integer, slice, tuple, list, NumPy array, and so on.

Indexing in NumPy: Copies and Views
------------------------------------
NumPy has a strict set of rules related to copies and views when indexing arrays. Whether you get views or copies of the original data depends on the approach you use to index your arrays: slicing, integer indexing, or Boolean indexing.

One-Dimensional Arrays
Slicing is a well-known operation in Python for getting particular data from arrays, lists, or tuples. When you slice a NumPy array, you get a view of the array:

arr = 2**np.arange(7)
a = arr[1:3]
a.base # array([ 1,  2,  4,  8, 16, 32, 64])
a.base is arr # True
a.flags.owndata # False

b = arr[1:4:2]
b.base # array([ 1,  2,  4,  8, 16, 32, 64])
b.base is arr # True
b.flags.owndata # False

You’ve created the original array arr and sliced it to get two smaller arrays, a and b. Both a and b use arr as their bases and neither has its own data. Instead, they look at the data of arr:

Note: When you have a large original array and need only a small part of it, you can call .copy() after slicing and delete the variable that points to the original with a del statement. This way, you keep the copy and remove the original array from memory.

Though slicing returns a view, there are other cases where creating one array from another actually makes a copy.
Indexing an array with a list of integers returns a copy of the original array. The copy contains the elements from the original array whose indices are present in the list:
c = arr[[1, 3]]
c.base is arr # False
c.base is None # True
c.flags.owndata # True

The resulting array c contains the elements from arr with the indices 1 and 3. These elements have the values 2 and 8. In this case, c is a copy of arr, its .base is None, and it has its own data:
The elements of arr with the chosen indices 1 and 3 are copied into the new array c. After the copying is done, arr and c are independent.

You can also index NumPy arrays with mask arrays or lists. Masks are Boolean arrays or lists of the same shape as the original. You’ll get a copy of the original array that contains only the elements that correspond to the True values of the mask:

mask = [False, True, False, True, False, False, False]
d = arr[mask]
d.base is None # True
d.flags.owndata # True

Note: Instead of a list, you can use another NumPy array of integers, but not a tuple.

To recap, here are the variables you’ve created so far that reference arr:
# `arr` is the original array:
arr = np.array([1, 2, 4, 8, 16, 32])

# `a` and `b` are views created through slicing:
a = arr[1:3]
b = arr[1:4:2]

# `c` and `d` are copies created through integer and Boolean indexing:
c = arr[[1, 3]]
d = arr[[False, True, False, True, False, False]]

Keep in mind that these examples show how you can reference data in an array. Referencing data returns views when slicing arrays and copies when using index and mask arrays. Assignments, on the other hand, always modify the original data of the array.

Now that you have all these arrays, let’s see what happens when you alter the original:
>>> arr[1] = 64
>>> arr
array([  1, 64,   4,   8,  16,  32])
>>> a
array([64,   4])
>>> b
array([64,   8])
>>> c
array([2, 8])
>>> d
array([2, 8])

You’ve changed the second value of arr from 2 to 64. The value 2 was also present in the derived arrays a, b, c, and d. However, only the views a and b are modified:
The views a and b look at the data of arr, including its second element. That’s why you see the change. The copies c and d remain unchanged because they don’t have common data with arr. They are independent of arr.

Chained Indexing in NumPy
-------------------------
Does this behavior with a and b look at all similar to the earlier Pandas examples? It might, because the concept of chained indexing applies in NumPy, too:
>>> arr = np.array([1, 2, 4, 8, 16, 32])
>>> arr[1:4:2][0] = 64
>>> arr
array([ 1, 64,  4,  8, 16, 32])

>>> arr = np.array([1, 2, 4, 8, 16, 32])
>>> arr[[1, 3]][0] = 64
>>> arr
array([ 1,  2,  4,  8, 16, 32])

This example illustrates the difference between copies and views when using chained indexing in NumPy.
In the first case, arr[1:4:2] returns a view that references the data of arr and contains the elements 2 and 8. The statement arr[1:4:2][0] = 64 modifies the first of these elements to 64. The change is visible in both arr and the view returned by arr[1:4:2].
In the second case, arr[[1, 3]] returns a copy that also contains the elements 2 and 8. But these aren’t the same elements as in arr. They’re new ones. arr[[1, 3]][0] = 64 modifies the copy returned by arr[[1, 3]] and leaves arr unchanged.
This is essentially the same behavior that produces a SettingWithCopyWarning in Pandas, but that warning doesn’t exist in NumPy.


Multidimensional Arrays
------------------------
Referencing multidimensional arrays follows the same principles:
Slicing arrays returns views.
Using index and mask arrays returns copies.
Combining index and mask arrays with slicing is also possible. In such cases, you get copies.
Here are a few examples:
arr = 2**np.arange(12).reshape(3,-1)
a = arr[:, 1:3]
a
a.base
a.base is arr # True

b = arr[:, 1:4:2]  # Take columns 1 and 3
b.base
b.base is arr # True

c = arr[:, [1, 3]]  # Take columns 1 and 3
c.base
c.base is arr # False

d = arr[:, [False, True, False, True]]  # Take columns 1 and 3
d.base
d.base is arr # False

In this example, you start from the two-dimensional array arr. You apply slices for rows. Using the colon syntax (:), which is equivalent to slice(None), means that you want to take all rows.
When you work with the slices 1:3 and 1:4:2 for columns, the views a and b are returned. However, when you apply the list [1, 3] and mask [False, True, False, True], you get the copies c and d.
The .base of both a and b is arr itself. Both c and d have their own bases unrelated to arr.
As with one-dimensional arrays, when you modify the original, the views change because they see the same data, but the copies remain the same:

arr[0, 1] = 100
a
b
c
d

You changed the value 2 in arr to 100 and altered the corresponding elements from the views a and b. The copies c and d can’t be modified this way.

Indexing in Pandas: Copies and Views
-------------------------------------
You’ve learned how you can use different indexing options in NumPy to refer to either actual data (a view, or shallow copy) or newly copied data (deep copy, or just copy). NumPy has a set of strict rules about this.

Pandas heavily relies on NumPy arrays but offers additional functionality and flexibility. Because of that, the rules for returning views and copies are more complex and less straightforward. They depend on the layout of data, data types, and other details. In fact, Pandas often doesn’t guarantee whether a view or copy will be referenced.

Note: Indexing in Pandas is a very wide topic. It’s essential for using Pandas data structures properly. You can use a variety of techniques:

Dictionary-like notation
Attribute-like (dot) notation
The accessors .loc[], .iloc[], .at[], and .iat

In this section, you’ll see two examples of how Pandas behaves similarly to NumPy. First, you can see that accessing the first three rows of df with a slice returns a view:
data = {
    "x": 2**np.arange(5),
    "y": 3**np.arange(5),
    "z": np.array([ 45, 98, 24, 11, 64, ])
}
index = [ "a", "b", "c", "d", "e"]
df = pd.DataFrame(data=data, index=index)
df["a":"c"].to_numpy().base
df["a":"c"].to_numpy().base is df.to_numpy().base # True

This view looks at the same data as df.
On the other hand, accessing the first two columns of df with a list of labels returns a copy:
df = pd.DataFrame(data=data, index=index)
df[["x", "y"]].to_numpy().base
df[["x", "y"]].to_numpy().base is df.to_numpy().base # False

The copy has a different .base than df.
In the next section, you’ll find more details related to indexing DataFrames and returning views and copies. You’ll see some cases where the behavior of Pandas becomes more complex and differs from NumPy.

Use of Views and Copies in Pandas
---------------------------------
As you’ve already learned, Pandas can issue a SettingWithCopyWarning when you try to modify the copy of data instead of the original. This often follows chained indexing.
In this section, you’ll see some specific cases that produce a SettingWithCopyWarning. You’ll identify the causes and learn how to avoid them by properly using views, copies, and accessors.

Chained Indexing and SettingWithCopyWarning
-------------------------------------------
You’ve already seen how the SettingWithCopyWarning works with chained indexing in the first example. Let’s elaborate on that a bit.
You’ve created the DataFrame and the mask Series object that corresponds to df["z"] < 50:
df = pd.DataFrame(data=data, index=index)
mask = df['z'] < 50

You already know that the assignment df[mask]["z"] = 0 fails. In this case, you get a SettingWithCopyWarning:
df[mask]["z"] = 0
SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

The assignment fails because df[mask] returns a copy. To be more precise, the assignment is made on the copy, and df isn’t affected.
You’ve also seen that in Pandas, evaluation order matters. In some cases, you can switch the order of operations to make the code work:
df["z"][mask] = 0

df["z"][mask] = 0 succeeds and you get the modified df without a SettingWithCopyWarning.
Using the accessors is recommended, but you can run into trouble with them as well:
>>> df = pd.DataFrame(data=data, index=index)
>>> df.loc[mask]["z"] = 0

In this case, df.loc[mask] returns a copy, the assignment fails, and Pandas correctly issues the warning.
In some cases, Pandas fails to detect the problem and the assignment on the copy passes without a SettingWithCopyWarning:
df = pd.DataFrame(data=data, index=index)
df.loc[["a", "c", "e"]]["z"] = 0  # Assignment fails, no warning
df

Here, you don’t receive a SettingWithCopyWarning and df isn’t changed because df.loc[["a", "c", "e"]] uses a list of indices and returns a copy, not a view.
There are some cases in which the code works, but Pandas issues the warning anyway:
df = pd.DataFrame(data=data, index=index)
df[:3]["z"] = 0  # Assignment succeeds, with warning
df = pd.DataFrame(data=data, index=index)
df.loc["a":"c"]["z"] = 0  # Assignment succeeds, with warning

In these two cases, you select the first three rows with slices and get views. The assignments succeed both on the views and on df. But you still receive a SettingWithCopyWarning.
The recommended way of performing such operations is to avoid chained indexing. Accessors can be of great help with that:
>>> df = pd.DataFrame(data=data, index=index)
>>> df.loc[mask, "z"] = 0
>>> df

This approach uses one method call, without chained indexing, and both the code and your intentions are clearer. As a bonus, this is a slightly more efficient way to assign data.

Impact of Data Types on Views, Copies, and the SettingWithCopyWarning
----------------------------------------------------------------------
In Pandas, the difference between creating views and creating copies also depends on the data types used. When deciding if it’s going to return a view or copy, Pandas handles DataFrames that have a single data type differently from ones with multiple types.
Let’s focus on the data types in this example:
df = pd.DataFrame(data=data, index=index)
df.dtypes

You’ve created the DataFrame with all integer columns. The fact that all three columns have the same data types is important here! In this case, you can select rows with a slice and get a view:
df["b":"d"]["z"] = 0

This mirrors the behavior that you’ve seen in the article so far. df["b":"d"] returns a view and allows you to modify the original data. That’s why the assignment df["b":"d"]["z"] = 0 succeeds. Notice that in this case you get a SettingWithCopyWarning regardless of the successful change to df.

If your DataFrame contains columns of different types, then you might get a copy instead of a view, in which case the same assignment will fail:
df = pd.DataFrame(data=data, index=index).astype(dtype={"z": float})
df.dtypes

df["b":"d"]["z"] = 0
df

In this case, you used .astype() to create a DataFrame that has two integer columns and one floating-point column. Contrary to the previous example, df["b":"d"] now returns a copy, so the assignment df["b":"d"]["z"] = 0 fails and df remains unchanged.
When in doubt, avoid the confusion and use the .loc[], .iloc[], .at[], and .iat[] access methods throughout your code!

Hierarchical Indexing and SettingWithCopyWarning
-------------------------------------------------
Hierarchical indexing, or MultiIndex, is a Pandas feature that enables you to organize your row or column indices on multiple levels according to a hierarchy. It’s a powerful feature that increases the flexibility of Pandas and enables working with data in more than two dimensions.

Hierarchical indices are created using tuples as row or column labels:

df = pd.DataFrame(
    data = {("powers", "x"): 2**np.arange(5),
          ("powers", "y"): 3**np.arange(5),
          ("random", "z"): np.array([45, 98, 24, 11, 64])},
    index = [ "a", "b", "c", "d", "e", ]
    )
df
Now you have the DataFrame df with two-level column indices:

The first level contains the labels powers and random.
The second level has the labels x and y, which belong to powers, and z, which belongs to random.
The expression df["powers"] will return a DataFrame containing all columns below powers, which are the columns x and y. If you wanted to get just the column x, then you could pass both powers and x. The proper way to do this is with the expression df["powers", "x"]:

df["powers"]
df["powers", "x"]
df["powers", "x"] = 0
df

That’s one way to get and set columns in the case of multilevel column indices. You can also use accessors with multi-indexed DataFrames to get or modify the data:
df.loc[["a", "b"], "powers"]

The example above uses .loc[] to return a DataFrame with the rows a and b and the columns x and y, which are below powers. You can get a particular column (or row) similarly:
df.loc[["a", "b"], ("powers", "x")]

In this example, you specify that you want the intersection of the rows a and b with the column x, which is below powers. To get a single column, you pass the tuple of indices ("powers", "x") and get a Series object as the result.
You can use this approach to modify the elements of DataFrames with hierarchical indices:
df.loc[["a", "b"], ("powers", "x")] = 0

In the examples above, you avoid chained indexing both with accessors (df.loc[["a", "b"], ("powers", "x")]) and without them (df["powers", "x"]).

As you saw earlier, chained indexing can lead to a SettingWithCopyWarning:
df["powers"]["x"] = 0 # SettingWithCopyWarning:

Here, df["powers"] returns a DataFrame with the columns x and y. This is just a view that points to the data from df, so the assignment is successful and df is modified. But Pandas still issues a SettingWithCopyWarning.
If you repeat the same code, but with different data types in the columns of df, then you’ll get a different behavior:

df = pd.DataFrame(
    data = {("powers", "x"): 2**np.arange(5),
          ("powers", "y"): 3**np.arange(5),
          ("random", "z"): np.array([45, 98, 24, 11, 64], dtype=float)},
    index = [ "a", "b", "c", "d", "e", ]
    )
df["powers"]["x"] = 0
df

This time, df has more than one data type, so df["powers"] returns a copy, df["powers"]["x"] = 0 makes a change on this copy, and df remains unchanged, giving you a SettingWithCopyWarning.

The recommended way to modify df is to avoid chained assignment. You’ve learned that accessors can be very convenient, but they aren’t always needed:

df["powers", "x"] = 0
df.loc[:, ("powers", "x")] = 0

In both cases, you get the modified DataFrame df without a SettingWithCopyWarning.

Change the Default SettingWithCopyWarning Behavior
The SettingWithCopyWarning is a warning, not an error. Your code will still execute when it’s issued, even though it may not work as intended.

To change this behavior, you can modify the Pandas mode.chained_assignment option with pandas.set_option(). You can use the following settings:

pd.set_option("mode.chained_assignment", "raise") raises a SettingWithCopyException.
pd.set_option("mode.chained_assignment", "warn") issues a SettingWithCopyWarning. This is the default behavior.
pd.set_option("mode.chained_assignment", None) suppresses both the warning and the error.
For example, this code will raise a SettingWithCopyException instead of issuing a SettingWithCopyWarning:

df = pd.DataFrame(
    data = {("powers", "x"): 2**np.arange(5),
          ("powers", "y"): 3**np.arange(5),
          ("random", "z"): np.array([45, 98, 24, 11, 64], dtype=float)},
    index = [ "a", "b", "c", "d", "e", ]
    )
pd.set_option("mode.chained_assignment", "raise")
df["powers"]["x"] = 0 # SettingWithCopyError

In addition to modifying the default behavior, you can use get_option() to retrieve the current setting related to mode.chained_assignment:
pd.get_option("mode.chained_assignment") # "raise"

You get "raise" in this case because you changed the behavior with set_option(). Normally, pd.get_option("mode.chained_assignment") returns "warn".

Although you can suppress it, keep in mind that the SettingWithCopyWarning can be very useful in notifying you about improper code.

Conclusion
In this article, you learned what views and copies are in NumPy and Pandas and what the differences are in their behavior. You also saw what a SettingWithCopyWarning is and how to avoid the subtle errors it points to.

In particular, you’ve learned the following:

*Indexing-based assignments in NumPy and Pandas can return either views or copies.
*Both views and copies can be useful, but they have different behaviors.
*Special care must be taken to avoid setting unwanted values on copies.
*Accessors in Pandas are very useful objects for properly assigning and referencing data.

Understanding views and copies is an important requirement for using NumPy and Pandas properly, especially when you’re working with big data. Now that you have a solid grasp on these concepts, you’re ready to dive deeper into the exciting world of data science!

### Learnings:
* Slice returns views and list returns copy
* It is good to use accessor .loc to assign a value to partial df
* SettingWithCopy warning may raise but then also assignment will be done if a view is returned.
* Sometimes without using chained assignment and loc accessor you can get desired result - df["power","x"] = 0
* you can use .base and .flags.owndata to understand the underlying data structure.
* you can set the option to raise, warn, None using set_option method.
* you can get the option of "mode.chained_assignment" using get_option method.
* Internally it all boils down to __getitem__ and __setitem__ method.
--------------------------------------------------------------------------------------------------------

2.6.1 Hierarchical indexing (MultiIndex)
========================================
Hierarchical / Multi-level indexing is very exciting as it opens the door to some quite sophisticated data analysis and manipulation, especially for working with higher dimensional data. In essence, it enables you to store and manipulate data with an arbitrary number of dimensions in lower dimensional data structures like Series (1d) and DataFrame (2d).
In this section, we will show what exactly we mean by “hierarchical” indexing and how it integrates with all of the pandas indexing functionality described above and in prior sections. Later, when discussing group by and pivoting and reshaping data, we’ll show non-trivial applications to illustrate how it aids in structuring data for analysis.

Changed in version 0.24.0: MultiIndex.labels has been renamed to MultiIndex.codes and
MultiIndex.set_labels to MultiIndex.set_codes.

Creating a MultiIndex (hierarchical index) object:
--------------------------------------------------
The MultiIndex object is the hierarchical analogue of the standard Index object which typically stores the axis labels in pandas objects. You can think of MultiIndex as an array of tuples where each tuple is unique. A MultiIndex can be created from a list of arrays (using MultiIndex.from_arrays()), an array of tuples (using MultiIndex.from_tuples()), a crossed set of iterables (using MultiIndex.from_product()), or a DataFrame (using MultiIndex.from_frame()). The Index constructor will attempt to return a MultiIndex when it is passed a list of tuples. The following examples demonstrate different ways to initialize MultiIndexes.

arrays = [["bar", "bar", "baz", "baz", "foo", "foo", "qux", "qux",],
          ["one", "two", "one", "two", "one", "two", "one", "two",],]
tuples = list(zip(*arrays))
index = pd.MultiIndex.from_tuples(tuples, names=["first", "second"])
s = pd.Series(np.random.randn(8), index=index)
s

When you want every pairing of the elements in two iterables, it can be easier to use the MultiIndex. from_product() method:
iterables = [[ "bar", "baz", "foo", "qux", ], ['one', 'two,']]
index = pd.MultiIndex.from_product(iterables)
index

You can also construct a MultiIndex from a DataFrame directly, using the method MultiIndex. from_frame(). This is a complementary method to MultiIndex.to_frame().
df = pd.DataFrame(
    [["bar", "one"], ["bar", "two"], ["foo", "one"], ["foo", "two"]],
    columns=["first", "second"],
)
df
pd.MultiIndex.from_frame(df)

As a convenience, you can pass a list of arrays directly into Series or DataFrame to construct a MultiIndex automatically:
arrays = [ np.array(["bar", "bar", "baz", "baz", "foo", "foo", "qux", "qux"]),
          np.array(["one", "two", "one", "two", "one", "two", "one", "two"]), ]
s = pd.Series(np.random.randn(8), index=arrays)
s

df = pd.DataFrame(np.random.randn(8, 4), index=arrays)
df

All of the MultiIndex constructors accept a names argument which stores string names for the levels themselves. If no names are provided, None will be assigned:
df.index.names # FrozenList([None, None])

This index can back any axis of a pandas object, and the number of levels of the index is up to you:
df = pd.DataFrame(np.random.randn(3, 8), index=["A", "B", "C"],
                  columns=index)
df

pd.DataFrame(np.random.randn(6, 6), index=index[:6], columns=index[:6])
We’ve “sparsified” the higher levels of the indexes to make the console output a bit easier on the eyes. Note that how the index is displayed can be controlled using the multi_sparse option in pandas.set_options():
with pd.option_context("display.multi_sparse", False):
    df

It’s worth keeping in mind that there’s nothing preventing you from using tuples as atomic labels on an axis:
pd.Series(np.random.randn(8), index=tuples)
The reason that the MultiIndex matters is that it can allow you to do grouping, selection, and reshaping operations as we will describe below and in subsequent areas of the documentation. As you will see in later sections, you can find yourself working with hierarchically-indexed data without creating a MultiIndex explicitly yourself. However, when loading data from a file, you may wish to generate your own MultiIndex when preparing the data set.


Basic indexing on axis with MultiIndex
---------------------------------------
One of the important features of hierarchical indexing is that you can select data by a “partial” label identifying a subgroup in the data. Partial selection “drops” levels of the hierarchical index in the result in a completely analogous way to selecting a column in a regular DataFrame:
df["bar","one"]

Same could be achieved though chained indexing but never use it? df["bar"]["one"]
Chained Indexing is slow and prone to SettingWithCopy warning and could result in ambiguous results.
%timeit df["bar","one"] # 101 µs ± 6.02 µs per loop (mean ± std. dev. of 7 runs, 10000
%timeit df["bar"]["one"] # 312 µs ± 8.42 µs per loop (mean ± std. dev. of 7 runs, 1000

Defined levels
---------------
The MultiIndex keeps all the defined levels of an index, even if they are not actually used. When slicing an index, you may notice this. For example:
df.columns.levels # original MultiIndex
# FrozenList([['bar', 'baz', 'foo', 'qux'], ['one', 'two']])
df[["foo","qux"]].columns.levels # sliced
# FrozenList([['bar', 'baz', 'foo', 'qux'], ['one', 'two']])

This is done to avoid a recomputation of the levels in order to make slicing highly performant. If you want to see only the used levels, you can use the get_level_values() method.
df[["foo", "qux"]].columns.to_numpy()
# for a specific level
df[["foo", "qux"]].columns.get_level_values(0)

To reconstruct the MultiIndex with only the used levels, the remove_unused_levels() method may be used.
new_mi = df[["foo", "qux"]].columns.remove_unused_levels()
new_mi

Data alignment and using reindex
--------------------------------
Operations between differently-indexed objects having MultiIndex on the axes will work as you expect; data
alignment will work the same as an Index of tuples:
s + s[:-2]
s + s[::2]

The reindex() method of Series/DataFrames can be called with another MultiIndex, or even a list or array of tuples:
s.reindex(index[:3])
s.reindex([("foo", "two"), ("bar", "one"), ("qux", "one"), ("baz", "one")])

2.6.2 Advanced indexing with hierarchical index
------------------------------------------------
Syntactically integrating MultiIndex in advanced indexing with .loc is a bit challenging, but we’ve made every effort to do so. In general, MultiIndex keys take the form of tuples. For example, the following works as you would expect:
df = df.T
df.loc[('bar','two')]

Note that df.loc['bar', 'two'] would also work in this example, but this shorthand notation can lead to ambiguity in general.
If you also want to index a specific column with .loc, you must use a tuple like this:
df.loc[('bar','two'), "A"]

You don’t have to specify all levels of the MultiIndex by passing only the first elements of the tuple. For example, you can use “partial” indexing to get all elements with bar in the first level as follows:
df.loc['bar']

This is a shortcut for the slightly more verbose notation df.loc[('bar',),] (equivalent to df.loc['bar',] in this example).
“Partial” slicing also works quite nicely.
df.loc["baz":"foo"]

You can slice with a ‘range’ of values, by providing a slice of tuples.
df.loc[("baz", "two"):("qux", "one")]
df.loc[("baz", "two"):"foo"]

Passing a list of labels or tuples works similar to reindexing:
df.loc[[("bar", "two"), ("qux", "one")]]

Note: It is important to note that tuples and lists are not treated identically in pandas when it comes to indexing. Whereas a tuple is interpreted as one multi-level key, a list is used to specify several keys. Or in other words, tuples go horizontally (traversing levels), lists go vertically (scanning levels).
Importantly, a list of tuples indexes several complete MultiIndex keys, whereas a tuple of lists refer to several values within a level:
s = pd.Series([1, 2, 3, 4, 5, 6],
              index=pd.MultiIndex.from_product([["A", "B"], ["c", "d", "e"]]),)
s.loc[[('A','c'), ('B','d')]] # list of tuples
s.loc[(["A", "B"], ["c","d"])] # tuple of lists

You can slice a MultiIndex by providing multiple indexers.
You can provide any of the selectors as if you are indexing by label, see Selection by Label, including slices, lists of
labels, labels, and boolean indexers.
You can use slice(None) to select all the contents of that level. You do not need to specify all the deeper levels,
they will be implied as slice(None).
As usual, both sides of the slicers are included as this is label indexing.
Warning: You should specify all axes in the .loc specifier, meaning the indexer for the index and for the columns. There are some ambiguous cases where the passed indexer could be mis-interpreted as indexing both axes, rather than into say the MultiIndex for the rows.
You should do this:
df.loc[(slice("A1", "A3"), ...), :] # noqa: E999

You should not do this:
df.loc[(slice("A1", "A3"), ...)] # noqa: E999

Swapping levels with swaplevel
-------------------------------
The swaplevel() method can switch the order of two levels:
df.swaplevel(0,1,axis=0)

Reordering levels with reorder_levels
df.reorder_levels([1,0], axis=0)

Renaming names of an Index or MultiIndex
The rename() method is used to rename the labels of a MultiIndex, and is typically used to rename the columns of a DataFrame. The columns argument of rename allows a dictionary to be specified that includes only the columns you wish to rename.
df.rename(columns={'A':'Ramesh', 'B':'Suresh'})

This method can also be used to rename specific labels of the main index of the DataFrame.
df.rename(index={'bar':'far', 'one':'seven'})
The rename_axis() method is used to rename the name of a Index or MultiIndex. In particular, the names of the levels of a MultiIndex can be specified, which is useful if reset_index() is later used to move the values from the MultiIndex to a column.
df.rename_axis(index=["abc", "def"])

Note that the columns of a DataFrame are an index, so that using rename_axis with the columns argument will change the name of that index.
df.rename_axis(columns="Cols").columns

Both rename and rename_axis support specifying a dictionary, Series or a mapping function to map la- bels/names to new values.
When working with an Index object directly, rather than via a DataFrame, Index.set_names() can be used to change the names.
mi = pd.MultiIndex.from_product([[1, 2], ["a", "b"]], names=["x", "y"])
mi.names
mi2 = mi.rename("new name", level=0)
mi2

You cannot set the names of the MultiIndex via a level.
mi.levels[0].name = "name via level"
Use Index.set_names() instead.

2.6.3 Sorting a MultiIndex
---------------------------
For MultiIndex-ed objects to be indexed and sliced effectively, they need to be sorted. As with any index, you can use sort_index().
df.sort_index(level=1)
You may also pass a level name to sort_index if the MultiIndex levels are named.
s.sort_index(level="L1")

On higher dimensional objects, you can sort any of the other axes by level if they have a MultiIndex:
df.T.sort_index(level=1, axis=1)

Indexing will work even if the data are not sorted, but will be rather inefficient (and show a PerformanceWarning). It will also return a copy of the data rather than a view:
dfm = pd.DataFrame(
    {"jim": [0, 0, 1, 1], "joe": ["x", "x", "z", "y"], "jolie": np.random.rand(4)}
)
dfm = dfm.set_index(["jim", "joe"])
dfm.loc[(1, 'z')]

Furthermore, if you try to index something that is not fully lexsorted, this can raise:
The is_lexsorted() method on a MultiIndex shows if the index is sorted, and the lexsort_depth prop- erty returns the sort depth:
dfm.index.is_lexsorted()
dfm.index.lexsort_depth

dfm = dfm.sort_index()
dfm.index.is_lexsorted()
dfm.index.lexsort_depth

And now selection works as expected.
dfm.loc[(0, "y"):(1, "z")]

2.6.4 Take methods
-------------------
Similar to NumPy ndarrays, pandas Index, Series, and DataFrame also provides the take() method that retrieves elements along a given axis at the given indices. The given indices must be either a list or an ndarray of integer index positions. take will also accept negative integers as relative positions to the end of the object.

index = pd.Index(np.random.randint(0, 1000, 10))
print(index)
positions = [0, 9, 3]
print(index[positions])
index.take(positions)

For DataFrames, the given indices should be a 1d list or ndarray that specifies row or column positions.
frm = pd.DataFrame(np.random.randn(5, 3))
frm.take([1,4,3])
frm.take([0,2], axis=1)

which one is faster, take() or loc?
take is faster -
%timeit frm.loc[[1,4,3]] # 251 µs ± 22.3 µs per loop (mean ± std. dev. of 7 runs, 1000
%timeit frm.take([1,4,3]) # 116 µs ± 1.32 µs per loop (mean ± std. dev. of 7 runs, 10000

It is important to note that the take method on pandas objects are not intended to work on boolean indices and may return unexpected results.
arr = np.random.randn(10)
arr
arr.take([False, False, True, True])
arr[[0, 1]]
ser = pd.Series(np.random.randn(10))
ser
ser.take([False, False, True, True])
ser.iloc[[0, 1]]

Finally, as a small note on performance, because the take method handles a narrower range of inputs, it can offer performance that is a good deal faster than fancy indexing.
arr = np.random.randn(10000, 5)
indexer = np.arange(10000)
random.shuffle(indexer)

%timeit arr[indexer]
%timeit arr.take(indexer, axis=0)

2.6.5 Index types
-----------------
CategoricalIndex
CategoricalIndex is a type of index that is useful for supporting indexing with duplicates. This is a container around a Categorical and allows efficient indexing and storage of an index with a large number of duplicated elements.
from pandas.api.types import CategoricalDtype

df = pd.DataFrame({"A": np.arange(6), "B": list("aabbca")})
df["B"] = df["B"].astype(CategoricalDtype(list("cab")))
df.dtypes
df.B.cat.categories

Setting the index will create a CategoricalIndex.
df2 = df.set_index("B")
df2.index

Indexing with __getitem__/.iloc/.loc works similarly to an Index with duplicates. The indexers must be in the category or the operation will raise a KeyError.
df2.loc["a"]
#df2.loc["x"] # KeyError

The CategoricalIndex is preserved after indexing:
df2.loc["a"].index

Sorting the index will sort by the order of the categories (recall that we created the index with CategoricalDtype(list('cab')), so the sorted order is cab).
df2.sort_index()

Groupby operations on the index will preserve the index nature as well.
df2.groupby('B').sum()
df2.groupby(level='B').sum()
df2.groupby(level=0).sum()
df2.groupby(level=0).sum().index

Reindexing operations will return a resulting index based on the type of the passed indexer. Passing a list will return a plain-old Index; indexing with a Categorical will return a CategoricalIndex, indexed according to the categories of the passed Categorical dtype. This allows one to arbitrarily index these even with values not in the categories, similarly to how you can reindex any pandas index.
df3 = pd.DataFrame(
    {"A": np.arange(3), "B": pd.Series(list("abc")).astype("category")}
)
df3.set_index('B', inplace=True)
df3.reindex(["b", "e"])
df3.reindex(["b", "e"]).index
df3.reindex(pd.Categorical(["b", "e"], categories=list("abe")))
df3.reindex(pd.Categorical(["a", "e"], categories=list("abe"))).index

Warning: Reshaping and Comparison operations on a Categorical Index must have the same categories or a TypeError will be raised.
In [165]: df4 = pd.DataFrame({"A": np.arange(2), "B": list("ba")}) In [166]: df4["B"] = df4["B"].astype(CategoricalDtype(list("ab"))) In [167]: df4 = df4.set_index("B")
In [168]: df4.index
Out[168]: CategoricalIndex(['b', 'a'], categories=['a', 'b'], ordered=False, name='B
˓→', dtype='category')
In [169]: df5 = pd.DataFrame({"A": np.arange(2), "B": list("bc")}) In [170]: df5["B"] = df5["B"].astype(CategoricalDtype(list("bc"))) In [171]: df5 = df5.set_index("B")
In [172]: df5.index
Out[172]: CategoricalIndex(['b', 'c'], categories=['b', 'c'], ordered=False, name='B
˓→', dtype='category')
In [1]: pd.concat([df4, df5])
TypeError: categories must match existing categories when appending

Int64Index and RangeIndex
-------------------------
Int64Index is a fundamental basic index in pandas. This is an immutable array implementing an ordered, sliceable set.
RangeIndex is a sub-class of Int64Index that provides the default index for all NDFrame objects. RangeIndex is an optimized version of Int64Index that can represent a monotonic ordered set. These are analogous to Python range types.

Float64Index
-------------
By default a Float64Index will be automatically created when passing floating, or mixed-integer-floating values in index creation. This enables a pure label-based slicing paradigm that makes [],ix,loc for scalar indexing and slicing work exactly the same.
indexf = pd.Index([1.5, 2, 3, 4.5, 5])
sf = pd.Series(range(5), index=indexf)
sf

Scalar selection for [],.loc will always be label based. An integer will match an equal float index (e.g. 3 is equivalent to 3.0).
sf[3]
sf[3.0]
sf.loc[3]
sf.loc[3.0]

The only positional indexing is via iloc.
sf.iloc[3]

A scalar index that is not found will raise a KeyError. Slicing is primarily on the values of the index when using [],ix,loc, and always positional when using iloc. The exception is when the slice is boolean, in which case it will always be positional.
sf[2:4]
sf.loc[2:4]
sf.iloc[2:4]

In float indexes, slicing using floats is allowed.
sf[2.1:4.6]
sf.loc[2.1:4.6]

In non-float indexes, slicing using floats will raise a TypeError.
pd.Series(range(5))[3.5]
TypeError: the label [3.5] is not a proper indexer for this index type (Int64Index)

pd.Series(range(5))[3.5:4.5]
TypeError: the slice start [3.5] is not a proper indexer for this index type

Here is a typical use-case for using this type of indexing. Imagine that you have a somewhat irregular timedelta-like indexing scheme, but the data is recorded as floats. This could, for example, be millisecond offsets.
df1 = pd.DataFrame(np.random.randn(5, 2), index=np.arange(5) * 250.0,
                   columns=list("AB"))
df2 = pd.DataFrame(np.random.randn(6, 2), index=np.arange(4, 10) * 250.1,
                   columns=list("AB"),)
pd.concat([df1, df2])

Selection operations then will always work on a value basis, for all selection operators.
dfir[1.0:1000.5]
dfir.loc[1.0:1000.5, "B"]

You could retrieve the first 1 second (1000 ms) of data as such:
dfir[0:1000]

If you need integer based selection, you should use iloc:
dfir.iloc[0:5]

IntervalIndex
-------------
IntervalIndex together with its own dtype, IntervalDtype as well as the Interval scalar type, allow first-class support in pandas for interval notation.
The IntervalIndex allows some unique indexing and is also used as a return type for the categories in cut() and qcut().
Indexing with an IntervalIndex
An IntervalIndex can be used in Series and in DataFrame as the index.
df = pd.DataFrame({"A": [1, 2, 3, 4]}, index=pd.IntervalIndex.from_breaks([0, 1, 2, 3, 4]))
Label based indexing via .loc along the edges of an interval works as you would expect, selecting that particular interval.
df.loc[2]
df.loc[[2, 3]]
If you select a label contained within an interval, this will also select the interval.
df.loc[2.5]
df.loc[2.0000000001]
df.loc[[2.5, 3.5]]
Selecting using an Interval will only return exact matches (starting from pandas 0.25.0).
df.loc[pd.Interval(1, 2)]
Trying to select an Interval that is not exactly contained in the IntervalIndex will raise a KeyError.
df.loc[pd.Interval(0.5, 2.5)]

Selecting all Intervals that overlap a given Interval can be performed using the overlaps() method to create a boolean indexer.
idxr = df.index.overlaps(pd.Interval(0.5, 2.5))
df[idxr]

Binning data with cut and qcut
cut() and qcut() both return a Categorical object, and the bins they create are stored as an IntervalIndex
in its .categories attribute.
c = pd.cut(range(4), bins=2)
c.categories

cut() also accepts an IntervalIndex for its bins argument, which enables a useful pandas idiom. First, We call cut() with some data and bins set to a fixed number, to generate the bins. Then, we pass the values of . categories as the bins argument in subsequent calls to cut(), supplying new data which will be binned into the same bins.
pd.cut([0, 3, 5, 1], bins=c.categories)

Any value which falls outside all bins will be assigned a NaN value.

Generating ranges of intervals
If we need intervals on a regular frequency, we can use the interval_range() function to create an IntervalIndex using various combinations of start, end, and periods. The default frequency for interval_range is a 1 for numeric intervals, and calendar day for datetime-like intervals:
pd.interval_range(start=0, end=5)
pd.interval_range(start=pd.Timestamp("2017-01-01"), periods=4)
pd.interval_range(end=pd.Timedelta("3 days"), periods=3)

The freq parameter can used to specify non-default frequencies, and can utilize a variety of frequency aliases with datetime-like intervals:
pd.interval_range(start=0, periods=5, freq=1.5)
pd.interval_range(start=pd.Timestamp("2017-01-01"), periods=4, freq="W")
pd.interval_range(start=pd.Timedelta("0 days"), periods=3, freq="9H")
Additionally, the closed parameter can be used to specify which side(s) the intervals are closed on. Intervals are closed on the right side by default.
pd.interval_range(start=0, end=4, closed="both")
pd.interval_range(start=0, end=4, closed="neither")
Specifying start, end, and periods will generate a range of evenly spaced intervals from start to end inclu- sively, with periods number of elements in the resulting IntervalIndex:
pd.interval_range(start=0, end=6, periods=4)
pd.interval_range(pd.Timestamp("2018-01-01"), pd.Timestamp("2018-02-28"), periods=3)

2.6.6 Miscellaneous indexing FAQ
---------------------------------
Integer indexing
Label-based indexing with integer axis labels is a thorny topic. It has been discussed heavily on mailing lists and among various members of the scientific Python community. In pandas, our general viewpoint is that labels matter more than integer locations. Therefore, with an integer axis index only label-based indexing is possible with the standard tools like .loc. The following code will generate exceptions:
s = pd.Series(range(5))
s[-1]

df = pd.DataFrame(np.random.randn(5, 4))
df.loc[-2:] # No change then original df

This deliberate decision was made to prevent ambiguities and subtle bugs (many users reported finding bugs when the API change was made to stop “falling back” on position-based indexing).

If the index of a Series or DataFrame is monotonically increasing or decreasing, then the bounds of a label-based slice can be outside the range of the index, much like slice indexing a normal Python list. Monotonicity of an index can be tested with the is_monotonic_increasing() and is_monotonic_decreasing() attributes.
df = pd.DataFrame(index=[2, 3, 3, 4, 5], columns=["data"],data=list(range(5)))
df.index.is_monotonic_increasing
# no rows 0 or 1, but still returns rows 2, 3 (both of them), and 4:
df.loc[0:4, :]
# slice is are outside the index, so empty DataFrame is returned
df.loc[13:15, :]

On the other hand, if the index is not monotonic, then both slice bounds must be unique members of the index.
df = pd.DataFrame(index=[2, 3, 1, 4, 3, 5], columns=["data"],data=list(range(6)))
df.index.is_monotonic_increasing
# OK because 2 and 4 are in the index
df.loc[2:4, :]
# 0 is not in the index
df.loc[0:4, :]

Endpoints are inclusive -- [Only with labels like df.index.loc["b":"d"] --> b and d are inclusive]
------------------------
Compared with standard Python sequence slicing in which the slice endpoint is not inclusive, label-based slicing in pandas is inclusive. The primary reason for this is that it is often not possible to easily determine the “successor” or next element after a particular label in an index. For example, consider the following Series:
s = pd.Series(np.random.randn(6), index=list("abcdef"))

Suppose we wished to slice from c to e, using integers this would be accomplished as such:
s[2:5]

However, if you only had c and e, determining the next element in the index can be somewhat complicated. For example, the following does not work:
s.loc['c':'e' + 1] # Error
A very common use case is to limit a time series to start and end at two specific dates. To enable this, we made the
design choice to make label-based slicing include both endpoints:
s.loc["c":"e"]

This is most definitely a “practicality beats purity” sort of thing, but it is something to watch out for if you expect label-based slicing to behave exactly in the way that standard Python integer slicing works.

Indexing potentially changes underlying Series dtype
----------------------------------------------------
The different indexing operation can potentially change the dtype of a Series.
series1 = pd.Series([1, 2, 3])
print(series1)
series1.reindex([0,4])

series2 = pd.Series([True])
res = series2.reindex_like(series1)
This is because the (re)indexing operations above silently inserts NaNs and the dtype changes accordingly. This can cause some issues when using numpy ufuncs such as numpy.logical_and.

==========================================
2.7 Merge, join, concatenate and compare
concat(), append()
merge(), join(), combine_first(), combine(), update()
merge_ordered(), merge_asof()
compare()
==========================================
pandas provides various facilities for easily combining together Series or DataFrame with various kinds of set logic for the indexes and relational algebra functionality in the case of join / merge-type operations.
In addition, pandas also provides utilities to compare two Series or DataFrame and summarize their differences.

2.7.1 Concatenating objects
----------------------------
The concat() function (in the main pandas namespace) does all of the heavy lifting of performing concatenation operations along an axis while performing optional set logic (union or intersection) of the indexes (if any) on the other axes. Note that I say “if any” because there is only a single possible axis of concatenation for Series.
Before diving into all of the details of concat and what it can do, here is a simple example:
df1 = pd.DataFrame({
    "A": ["A0", "A1", "A2", "A3"],
    "B": ["B0", "B1", "B2", "B3"],
    "C": ["C0", "C1", "C2", "C3"],
    "D": ["D0", "D1", "D2", "D3"],
}, index=[0, 1, 2, 3],)

df2 = pd.DataFrame({
    "A": ["A4", "A5", "A6", "A7"],
    "B": ["B4", "B5", "B6", "B7"],
    "C": ["C4", "C5", "C6", "C7"],
    "D": ["D4", "D5", "D6", "D7"],
}, index=[4, 5, 6, 7],)

df3 = pd.DataFrame({
    "A": ["A8", "A9", "A10", "A11"],
    "B": ["B8", "B9", "B10", "B11"],
    "C": ["C8", "C9", "C10", "C11"],
    "D": ["D8", "D9", "D10", "D11"],
}, index=[8,9,10,11,])

frames = [ df1, df2, df3, ]
result = pd.concat(frames)

Like its sibling function on ndarrays, numpy.concatenate, pandas.concat takes a list or dict of homogeneously-typed objects and concatenates them with some configurable handling of “what to do with the other axes”:

pd.concat(
    objs,
    axis=0,
    join="outer", ignore_index=False, keys=None,
    levels=None, names=None, verify_integrity=False, copy=True,
)

• objs : a sequence or mapping of Series or DataFrame objects. If a dict is passed, the sorted keys will be used as the keys argument, unless it is passed, in which case the values will be selected (see below). Any None objects will be dropped silently unless they are all None in which case a ValueError will be raised.
• axis : {0, 1, . . . }, default 0. The axis to concatenate along.
• join : {‘inner’, ‘outer’}, default ‘outer’. How to handle indexes on other axis(es). Outer for union and inner
for intersection.
• ignore_index : boolean, default False. If True, do not use the index values on the concatenation axis. The resulting axis will be labeled 0, . . . , n - 1. This is useful if you are concatenating objects where the concatenation axis does not have meaningful indexing information. Note the index values on the other axes are still respected in the join.
• keys : sequence, default None. Construct hierarchical index using the passed keys as the outermost level. If multiple levels passed, should contain tuples.
• levels : list of sequences, default None. Specific levels (unique values) to use for constructing a MultiIndex. Otherwise they will be inferred from the keys.
• names : list, default None. Names for the levels in the resulting hierarchical index.
• verify_integrity : boolean, default False. Check whether the new concatenated axis contains duplicates.
This can be very expensive relative to the actual data concatenation.
• copy : boolean, default True. If False, do not copy data unnecessarily.
Without a little bit of context many of these arguments don’t make much sense. Let’s revisit the above example. Suppose we wanted to associate specific keys with each of the pieces of the chopped up DataFrame. We can do this using the keys argument:

result = pd.concat(frames, keys=['x', 'y', 'z',])

As you can see, the resulting object’s index has a hierarchical index. This means that we can now select out each chunk by key:

result.loc["y"]
Note: It is worth noting that concat() (and therefore append()) makes a full copy of the data,and that constantly reusing this function can create a significant performance hit. If you need to use the operation over several datasets, use a list comprehension.

 frames = [ process_your_file(f) for f in files ]
 result = pd.concat(frames)

Note: When concatenating DataFrames with named axes,pandas will attempt to preserve these index/column names whenever possible. In the case where all inputs share a common name, this name will be assigned to the result. When the input names do not all agree, the result will be unnamed. The same is true for MultiIndex, but the logic is applied separately on a level-by-level basis.

Set logic on the other axes
---------------------------
When gluing together multiple DataFrames, you have a choice of how to handle the other axes (other than the one being concatenated). This can be done in the following two ways:
• Take the union of them all, join='outer'. This is the default option as it results in zero information loss.
• Take the intersection, join='inner'.
Here is an example of each of these methods. First, the default join='outer' behavior:
df4 = pd.DataFrame({
    "B": ["B2", "B3", "B6", "B7"],
    "D": ["D2", "D3", "D6", "D7"],
    "F": ["F2", "F3", "F6", "F7"],
}, index=[2, 3, 6, 7],)

pd.concat([df1, df4])
pd.concat([df1, df4], axis=1)
pd.concat([df1, df4], axis=1, join='inner')

Lastly, suppose we just wanted to reuse the exact index from the original DataFrame:
pd.concat([df1, df4], axis=1).reindex(df1.index)

Similarly, we could index before the concatenation:
pd.concat([df1, df4.reindex(df1.index)], axis=1)

Concatenating using append
---------------------------
A useful shortcut to concat() are the append() instance methods on Series and DataFrame. These methods
actually predated concat. They concatenate along axis=0, namely the index:
df1.append(df2)

In the case of DataFrame, the indexes must be disjoint but the columns do not need to be:
result = df1.append(df4, sort=False)

append may take multiple objects to concatenate:
result = df1.append([df2, df3])

NOTE: Unlike the append() method, which appends to the original list and returns None, append() here does not modify df1 and returns its copy with df2 appended.

Which one is faster?
append is slightly faster then concat.
%timeit df1.append(df4) # 1.77 ms ± 125 µs per loop
%timeit pd.concat([df1, df4]) # 1.88 ms ± 180 µs per loop

Ignoring indexes on the concatenation axis
-------------------------------------------
For DataFrame objects which don’t have a meaningful index, you may wish to append them and ignore the fact that they may have overlapping indexes. To do this, use the ignore_index argument:
pd.concat([df1, df4], ignore_index=True, sort=False)

This is also a valid argument to DataFrame.append():
result = df1.append(df4, ignore_index=True, sort=False)

Concatenating with mixed ndims
-------------------------------
You can concatenate a mix of Series and DataFrame objects. The Series will be transformed to DataFrame with the column name as the name of the Series.
s1 = pd.Series(["X0", "X1", "X2", "X3"], name="X")
pd.concat([df1, s1], axis=1)

Note: Since we’re concatenating a Series to a DataFrame, we could have achieved the same result with DataFrame.assign(). To concatenate an arbitrary number of pandas objects (DataFrame or Series), use concat.

which one is faster?
assign is much faster than concat.
%timeit df1.assign(X=s1) # 455 µs ± 33.7 µs per loop
%timeit pd.concat([df1, s1], axis=1) # 808 µs ± 50.7 µs per loop

If unnamed Series are passed they will be numbered consecutively.
s2 = pd.Series(["_0", "_1", "_2", "_3"])
result = pd.concat([df1, s2, s2, s2], axis=1)

Passing ignore_index=True will drop all name references
result = pd.concat([df1, s1], axis=1, ignore_index=True)

More concatenating with group keys
-----------------------------------
A fairly common use of the keys argument is to override the column names when creating a new DataFrame based on existing Series. Notice how the default behaviour consists on letting the resulting DataFrame inherit the parent Series’ name, when these existed.
s3 = pd.Series([0, 1, 2, 3], name="foo")
s4 = pd.Series([0, 1, 2, 3])
s5 = pd.Series([0, 1, 4, 5])
pd.concat([s3, s4, s5], axis=1)

Through the keys argument we can override the existing column names.
pd.concat([s3, s4, s5], axis=1, keys=["red", "blue", "yellow"])

Let’s consider a variation of the very first example presented:
result = pd.concat(frames, keys=["x", "y", "z"])

You can also pass a dict to concat in which case the dict keys will be used for the keys argument (unless other keys are specified):
pieces = {"x": df1, "y": df2, "z": df3}
pd.concat(pieces)

result = pd.concat(pieces, keys=["z", "y"])
The MultiIndex created has levels that are constructed from the passed keys and the index of the DataFrame pieces:
result.index.levels

If you wish to specify other levels (as will occasionally be the case), you can do so using the levels argument:
result = pd.concat(pieces, keys=["x", "y", "z"],
                   levels=[["z", "y", "x", "w"]], names=["group_key"])
result

This is fairly esoteric, but it is actually necessary for implementing things like GroupBy where the order of a categorical variable is meaningful.

Appending rows to a DataFrame
-----------------------------
While not especially efficient (since a new object must be created), you can append a single row to a DataFrame by passing a Series or dict to append, which returns a new DataFrame as above.
s2 = pd.Series(["X0", "X1", "X2", "X3"], index=["A", "B", "C", "D"])
df1.append(s2, ignore_index=True)

You should use ignore_index with this method to instruct DataFrame to discard its index. If you wish to preserve the index, you should construct an appropriately-indexed DataFrame and append or concatenate those objects.

You can also pass a list of dicts or Series:
dicts = [{"A": 1, "B": 2, "C": 3, "X": 4}, {"A": 5, "B": 6, "C": 7, "Y": 8}]
df1.append(dicts, ignore_index=True, sort=False)

Add rows to existing columns using dict
dicts = [{"A": 1, "B": 2, "C": 3, "D": 4}, {"A": 5, "B": 6, "C": 7, "D": 8}]
df1.append(dicts, ignore_index=True)

2.7.2 Database-style DataFrame or named Series joining/merging
---------------------------------------------------------------
pandas has full-featured, high performance in-memory join operations idiomatically very similar to relational databases like SQL. These methods perform significantly better (in some cases well over an order of magnitude better) than other open source implementations (like base::merge.data.frame in R). The reason for this is careful algorithmic design and the internal layout of the data in DataFrame.

pandas provides a single function, merge(), as the entry point for all standard database join operations between DataFrame or named Series objects:

pd.merge(
    left, right,    how="inner",    on=None,
    left_on=None, right_on=None, left_index=False, right_index=False,
    sort=True, suffixes=("_x", "_y"), copy=True, indicator=False, validate=None,
)

• left: A DataFrame or named Series object.
• right: Another DataFrame or named Series object.
• on: Column or index level names to join on. Must be found in both the left and right DataFrame and/or Series objects. If not passed and left_index and right_index are False, the intersection of the columns in the DataFrames and/or Series will be inferred to be the join keys.
• left_on: Columns or index levels from the left DataFrame or Series to use as keys. Can either be column names, index level names, or arrays with length equal to the length of the DataFrame or Series.
• right_on: Columns or index levels from the right DataFrame or Series to use as keys. Can either be column names, index level names, or arrays with length equal to the length of the DataFrame or Series.
• left_index: If True, use the index (row labels) from the left DataFrame or Series as its join key(s). In the case of a DataFrame or Series with a MultiIndex (hierarchical), the number of levels must match the number of join keys from the right DataFrame or Series.
• right_index: Same usage as left_index for the right DataFrame or Series
• how: One of 'left', 'right', 'outer', 'inner'. Defaults to inner. See below for more detailed
description of each method.
• sort: Sort the result DataFrame by the join keys in lexicographical order. Defaults to True, setting to False will improve performance substantially in many cases.
• suffixes: A tuple of string suffixes to apply to overlapping columns. Defaults to ('_x', '_y').
• copy: Always copy data (default True) from the passed DataFrame or named Series objects, even when reindexing is not necessary. Cannot be avoided in many cases but may improve performance / memory usage. The cases where copying can be avoided are somewhat pathological but this option is provided nonetheless.
• indicator: Add a column to the output DataFrame called _merge with information on the source of each row. _merge is Categorical-type and takes on a value of left_only for observations whose merge key only appears in 'left' DataFrame or Series, right_only for observations whose merge key only appears in 'right' DataFrame or Series, and both if the observation’s merge key is found in both.
• validate : string, default None. If specified, checks if merge is of specified type.
– “one_to_one” or “1:1”: checks if merge keys are unique in both left and right datasets. – “one_to_many” or “1:m”: checks if merge keys are unique in left dataset.
– “many_to_one” or “m:1”: checks if merge keys are unique in right dataset.
– “many_to_many” or “m:m”: allowed, but does not result in checks.

Note: Support for specifying index levels as the on, left_on, and right_on parameters was added in version
0.23.0. Support for merging named Series objects was added in version 0.24.0.

The return type will be the same as left. If left is a DataFrame or named Series and right is a subclass of
DataFrame, the return type will still be DataFrame.
merge is a function in the pandas namespace, and it is also available as a DataFrame instance method merge(),
with the calling DataFrame being implicitly considered the left object in the join.
The related join() method, uses merge internally for the index-on-index (by default) and column(s)-on-index join.
If you are joining on index only, you may wish to use DataFrame.join to save yourself some typing.

MERGE ON INDEX IS JOIN AND CAN BE USED IN JOINING DF/SERIES ON INDEX.

Brief primer on merge methods (relational algebra)
Experienced users of relational databases like SQL will be familiar with the terminology used to describe join oper- ations between two SQL-table like structures (DataFrame objects). There are several cases to consider which are very important to understand:
• one-to-one joins: for example when joining two DataFrame objects on their indexes (which must contain unique values).
• many-to-one joins: for example when joining an index (unique) to one or more columns in a different DataFrame.
• many-to-many joins: joining columns on columns.

Note: When joining columns on columns (potentially a many-to-many join),any indexes on the passed DataFrame objects will be discarded.

left = pd.DataFrame({
    "key": ["K0", "K1", "K2", "K3"],
    "A": ["A0", "A1", "A2", "A3"],
    "B": ["B0", "B1", "B2", "B3"],
})

right = pd.DataFrame({
    "key": ["K0", "K1", "K2", "K3"],
    "C": ["C0", "C1", "C2", "C3"],
    "D": ["D0", "D1", "D2", "D3"],
})

pd.merge(left, right, on="key")

Here is a more complicated example with multiple join keys. Only the keys appearing in left and right are present (the intersection), since how='inner' by default.

left = pd.DataFrame({
    "key1": ["K0", "K0", "K1", "K2"],
    "key2": ["K0", "K1", "K0", "K1"],
    "A": ["A0", "A1", "A2", "A3"],
    "B": ["B0", "B1", "B2", "B3"],
})

right = pd.DataFrame({
    "key1": ["K0", "K1", "K1", "K2"],
    "key2": ["K0", "K0", "K0", "K0"],
    "C": ["C0", "C1", "C2", "C3"],
    "D": ["D0", "D1", "D2", "D3"],
})

pd.merge(left, right, on=['key1', 'key2'])

The how argument to merge specifies how to determine which keys are to be included in the resulting table. If a key combination does not appear in either the left or right tables, the values in the joined table will be NA. Here is a summary of the how options and their SQL equivalent names:
Merge method                SQL Join Name               Description
left                        LEFT OUTER JOIN         Use keys from left frame only
right                       RIGHT OUTER JOIN        Use keys from right frame only
outer                       FULL OUTER  JOIN        Use union of keys from both frames
inner                       INNER JOIN              Use intersection of keys from both frames

result = pd.merge(left, right, how="left", on=["key1", "key2"])
result = pd.merge(left, right, how="right", on=["key1", "key2"])
result = pd.merge(left, right, how="outer", on=["key1", "key2"])
result = pd.merge(left, right, how="inner", on=["key1", "key2"])

You can merge a mult-indexed Series and a DataFrame, if the names of the MultiIndex correspond to the columns from the DataFrame. Transform the Series to a DataFrame using Series.reset_index() before merging, as shown in the following example.
df = pd.DataFrame({"Let": ["A", "B", "C"], "Num": [1, 2, 3]})
ser = pd.Series(["a", "b", "c", "d", "e", "f"],
        index=pd.MultiIndex.from_arrays(
        [["A", "B", "C"] * 2, [1, 2, 3, 4, 5, 6]], names=["Let", "Num"]))

pd.merge(df, ser.reset_index(), on=["Let","Num"])

Here is another example with duplicate join keys in DataFrames:
left = pd.DataFrame({"A": [1, 2], "B": [2, 2]})
right = pd.DataFrame({"A": [4, 5, 6], "B": [2, 2, 2]})
pd.merge(left, right, on='B')

Warning: Joining / merging on duplicate keys can cause a returned frame that is the multiplication of the row dimensions, which may result in memory overflow. It is the user’ s responsibility to manage duplicate values in keys before joining large DataFrames.

Checking for duplicate keys
---------------------------
Users can use the validate argument to automatically check whether there are unexpected duplicates in their merge keys. Key uniqueness is checked before merge operations and so should protect against memory overflows. Checking key uniqueness is also a good way to ensure user data structures are as expected.
In the following example, there are duplicate values of B in the right DataFrame. As this is not a one-to-one merge – as specified in the validate argument – an exception will be raised.

left = pd.DataFrame({"A": [1, 2], "B": [1, 2]})
right = pd.DataFrame({"A": [4, 5, 6], "B": [2, 2, 2]})
pd.merge(left, right, on='B', how='outer', validate='one_to_one')
#MergeError: Merge keys are not unique in right dataset; not a one-to-one merge

If the user is aware of the duplicates in the right DataFrame but wants to ensure there are no duplicates in the left DataFrame, one can use the validate='one_to_many' argument instead, which will not raise an exception.
pd.merge(left, right, on="B", how="outer", validate="one_to_many")

The merge indicator
merge() accepts the argument indicator. If True, a Categorical-type column called _merge will be added to the output object that takes on values:

Observation Origin                      _merge value
Merge key only in 'left' frame          left_only
Merge key only in 'right' frame         right_only
Merge key in both frames0               both

df1 = pd.DataFrame({"col1": [0, 1], "col_left": ["a", "b"]})
df2 = pd.DataFrame({"col1": [1, 2, 2], "col_right": [2, 2, 2]})
pd.merge(df1, df2, on="col1", how="outer", indicator=True)

The indicator argument will also accept string arguments, in which case the indicator function will use the value of the passed string as the name for the indicator column.
pd.merge(df1, df2, on="col1", how="outer", indicator="indicator_column")

Merge dtypes
------------
Merging will preserve the dtype of the join keys.
left = pd.DataFrame({"key": [1], "v1": [10]})
right = pd.DataFrame({"key": [1, 2], "v1": [20, 30]})
We are able to preserve the join keys:
pd.merge(left, right, how="outer")
pd.merge(left, right, how="outer").dtypes

Of course if you have missing values that are introduced, then the resulting dtype will be upcast.
pd.merge(left, right, how="outer", on="key")
pd.merge(left, right, how="outer", on="key").dtypes

Merging will preserve category dtypes of the mergands.
from pandas.api.types import CategoricalDtype
X = pd.Series(np.random.choice(["foo", "bar"], size=(10,)))
X = X.astype(CategoricalDtype(categories=["foo", "bar"]))
left = pd.DataFrame({"X": X, "Y": np.random.choice(["one", "two", "three"], size=(10,))})
right = pd.DataFrame({"X": pd.Series(["foo", "bar"], dtype=CategoricalDtype(["foo", "bar]), "Z": [1, 2],)})

result = pd.merge(left, right, how="outer")
Note: The category dtypes must be exactly the same, meaning the same categories and the ordered attribute. Other- wise the result will coerce to the categories’ dtype.
NOTE: MERGING ON CATEGORY DTYPES THAT ARE THE SAME CAN BE QUITE PERFORMANT COMPARED TO OBJECT DTYPE MERGING.

Joining on index
==================
DataFrame.join() is a convenient method for combining the columns of two potentially differently-indexed DataFrames into a single result DataFrame. Here is a very basic example:
left = pd.DataFrame(
    {"A": ["A0", "A1", "A2"], "B": ["B0", "B1", "B2"]},
    index=["K0", "K1","K2",]
)

right = pd.DataFrame(
    {"C": ["C0", "C2", "C3"], "D": ["D0", "D2", "D3"]},
    index=["K0", "K2","K3",]
)

left.join(right)
left.join(right, how='inner')
left.join(right, how='outer')
left.join(right, how='left')
left.join(right, how='right')

The data alignment here is on the indexes (row labels). This same behavior can be achieved using merge plus additional arguments instructing it to use the indexes:
result = pd.merge(left, right, left_index=True, right_index=True, how="outer")
result = pd.merge(left, right, left_index=True, right_index=True, how="inner")

=========================================
default join in left join
default merge is inner merge
==========================================

join() takes an optional on argument which may be a column or multiple column names, which specifies that the passed DataFrame is to be aligned on that column in the DataFrame. These two function calls are completely equivalent:
left.join(right, on=key_or_keys)
pd.merge(left, right, left_on=key_or_keys, right_index=True, how="left", sort=False)

Obviously you can choose whichever form you find more convenient. For many-to-one joins (where one of the DataFrame’s is already indexed by the join key), using join may be more convenient. Here is a simple example:
left = pd.DataFrame({
    "A": ["A0", "A1", "A2", "A3"],
    "B": ["B0", "B1", "B2", "B3"],
    "key": ["K0", "K1", "K0", "K1"],
})

right = pd.DataFrame({"C": ["C0", "C1"], "D": ["D0", "D1"]},
                     index=["K0", "K1"])

result = left.join(right, on="key")
print(result)

left.merge(right, right_index=True, left_on='key', how='left')

To join on multiple keys,the passed DataFrame must have a MultiIndex:
left = pd.DataFrame({
    "A": ["A0", "A1", "A2", "A3"],
    "B": ["B0", "B1", "B2", "B3"],
    "key1": ["K0", "K0", "K1", "K2"],
    "key2": ["K0", "K1", "K0", "K1"],
})

index = pd.MultiIndex.from_tuples(
[("K0", "K0"), ("K1", "K0"), ("K2", "K0"), ("K2", "K1")]
)

right = pd.DataFrame({
    "C": ["C0", "C1", "C2", "C3"], "D": ["D0", "D1", "D2", "D3"]},
    index=index
)

left.join(right, on=["key1", "key2"])

The default for DataFrame.join is to perform a left join (essentially a “VLOOKUP” operation, for Excel users), which uses only the keys found in the calling DataFrame. Other join types, for example inner join, can be just as easily
performed:

left.join(right, on=["key1", "key2"], how='inner')

As you can see, this drops any rows where there was no match.

Joining a single Index to a MultiIndex
---------------------------------------
You can join a singly-indexed DataFrame with a level of a MultiIndexed DataFrame. The level will match on the name of the index of the singly-indexed frame against a level name of the MultiIndexed frame.
left = pd.DataFrame({
    "A": ["A0", "A1", "A2"], "B": ["B0", "B1", "B2"]},
    index=pd.Index(["K0", "K1", "K2"], name="key"),
)

index = pd.MultiIndex.from_tuples(
    [("K0", "Y0"), ("K1", "Y1"), ("K2", "Y2"), ("K2", "Y3")],
    names=["key", "Y"]
)

right = pd.DataFrame({
    "C": ["C0", "C1", "C2", "C3"], "D": ["D0", "D1", "D2", "D3"]},
    index=index)

left.join(right, how='inner')

This is equivalent but less verbose and more memory efficient but slower than join.
pd.merge(left.reset_index(), right.reset_index(), on=["key"]).set_index(["key","Y"])

%timeit pd.merge(left.reset_index(), right.reset_index(), on=["key"]).set_index(["key","Y"])
# 6.07 ms ± 233 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

%timeit left.join(right, how='inner') #740 µs ± 96.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

Joining with two MultiIndexes
-----------------------------
This is supported in a limited way, provided that the index for the right argument is completely used in the join, and is a subset of the indices in the left argument, as in this example:

leftindex = pd.MultiIndex.from_product([list("abc"), list("xy"), [1, 2]],
                    names=["abc", "xy", "num"])
left = pd.DataFrame({"v1": range(12)}, index=leftindex)

rightindex = pd.MultiIndex.from_product(
    [list("abc"), list("xy")], names=["abc", "xy"]
)
right = pd.DataFrame({"v2": [100 * i for i in range(1, 7)]},
                     index=rightindex)

left.join(right, on=["abc", "xy"], how="inner")

Equivalent Merge Code:
pd.merge(left.reset_index(), right.reset_index(), on=['abc', 'xy']).set_index(['abc', 'xy', 'num'])

%timeit pd.merge(left.reset_index(), right.reset_index(), on=['abc', 'xy']).set_index(['abc', 'xy', 'num'])
# 9.04 ms ± 1.04 ms per loop

%timeit left.join(right, on=["abc", "xy"], how="inner") # 4.8 ms ± 259 µs per loop

If that condition is not satisfied, a join with two multi-indexes can be done using the following code.
leftindex = pd.MultiIndex.from_tuples([("K0", "X0"), ("K0", "X1"), ("K1", "X2")], names=["key", "X"])
left = pd.DataFrame({"A": ["A0", "A1", "A2"], "B": ["B0", "B1", "B2"]}, index=leftindex)

rightindex = pd.MultiIndex.from_tuples([("K0", "Y0"), ("K1", "Y1"), ("K2", "Y2"), ("K2", "Y3")], names=["key",Y"])
right = pd.DataFrame({"C": ["C0", "C1", "C2", "C3"], "D": ["D0", "D1", "D2", "D3"]},
            index=rightindex)

result = pd.merge(left.reset_index(), right.reset_index(), on=["key"], ).set_index(["key", "X", "Y"])

Merging on a combination of columns and index levels
-----------------------------------------------------
Strings passed as the on, left_on, and right_on parameters may refer to either column names or index level names. This enables merging DataFrame instances on a combination of index levels and columns without resetting indexes.
left_index = pd.Index(["K0", "K0", "K1", "K2"], name="key1")

left = pd.DataFrame({
    "A": ["A0", "A1", "A2", "A3"],
    "B": ["B0", "B1", "B2", "B3"],
    "key2": ["K0", "K1", "K0", "K1"],},
    index=left_index)

right_index = pd.Index(["K0", "K1", "K2", "K2"], name="key1")
right = pd.DataFrame({
    "C": ["C0", "C1", "C2", "C3"],
    "D": ["D0", "D1", "D2", "D3"],
    "key2": ["K0", "K0", "K0", "K1"],},
    index=right_index)

left.merge(right, on=['key1', 'key2'])

Note: When DataFrames are merged on a string that matches an index level in both frames, the index level is preserved as an index level in the resulting DataFrame.

Note: When DataFrames are merged using only some of the levels of a MultiIndex, the extra levels will be dropped from the resulting merge. In order to preserve those levels, use reset_index on those level names to move those levels to columns prior to doing the merge.

Note: If a string matches both a column name and an index level name,then a warning is issued and the column takes precedence. This will result in an ambiguity error in a future version.

Overlapping value columns
---------------------------
The merge suffixes argument takes a tuple of list of strings to append to overlapping column names in the input DataFrames to disambiguate the result columns:
left = pd.DataFrame({"k": ["K0", "K1", "K2"], "v": [1, 2, 3]})
right = pd.DataFrame({"k": ["K0", "K0", "K3"], "v": [4, 5, 6]})
pd.merge(left, right, on="k", suffixes=('_l', '_r'))

DataFrame.join() has lsuffix and rsuffix arguments which behave similarly.
left.set_index('k').join(right.set_index('k'), lsuffix="_l", rsuffix="_r", how='inner')

Joining multiple DataFrames
----------------------------
A list or tuple of DataFrames can also be passed to join() to join them together on their indexes.
left = left.set_index('k')
right = right.set_index('k')
right2 = pd.DataFrame({"v": [7, 8, 9]}, index=["K1", "K1", "K2"])
left.join([right, right2])

Merging together values within Series or DataFrame columns
-----------------------------------------------------------
Another fairly common situation is to have two like-indexed (or similarly indexed) Series or DataFrame objects and wanting to “patch” values in one object from values for matching indices in the other. Here is an example:
df1 = pd.DataFrame(
    [[np.nan, 3.0, 5.0], [-4.6, np.nan, np.nan], [np.nan, 7.0, np.nan]]
)
df2 = pd.DataFrame([[-42.6, np.nan, -8.2], [-5.0, 1.6, 4]], index=[1, 2])
df1.combine_first(df2)

Note that this method only takes values from the right DataFrame if they are missing in the left DataFrame. A related method, update(), alters non-NA values in place:
df1.update(df2)

2.7.3 Timeseries friendly merging
----------------------------------
Merging ordered data
--------------------
A merge_ordered() function allows combining time series and other ordered data. In particular it has an optional fill_method keyword to fill/interpolate missing data:
left = pd.DataFrame({
    "k": ["K0", "K1", "K1", "K2"], "lv": [1, 2, 3, 4],
     "s": ["a", "b", "c", "d"]
})
right = pd.DataFrame({"k": ["K1", "K2", "K4"], "rv": [1, 2, 3]})

pd.merge_ordered(left, right, fill_method='ffill', left_by='s')

Merging asof
-------------
A merge_asof() is similar to an ordered left-join except that we match on nearest key rather than equal keys. For each row in the left DataFrame, we select the last row in the right DataFrame whose on key is less than the left’s key. Both DataFrames must be sorted by the key.
Optionally an asof merge can perform a group-wise merge. This matches the by key equally, in addition to the nearest match on the on key.
For example; we might have trades and quotes and we want to asof merge them.
trades = pd.DataFrame({
    "time": pd.to_datetime([
        "20160525 13:30:00.023",
        "20160525 13:30:00.038",
        "20160525 13:30:00.048",
        "20160525 13:30:00.048",
        "20160525 13:30:00.048",
    ]),
    "ticker": ["MSFT", "MSFT", "GOOG", "GOOG", "AAPL"],
    "price": [51.95, 51.95, 720.77, 720.92, 98.00],
    "quantity": [75, 155, 100, 100, 100],
}, columns=["time", "ticker", "price", "quantity"],)

quotes = pd.DataFrame({
    "time": pd.to_datetime([
        "20160525 13:30:00.023",
        "20160525 13:30:00.023",
        "20160525 13:30:00.030",
        "20160525 13:30:00.041",
        "20160525 13:30:00.048",
        "20160525 13:30:00.049",
        "20160525 13:30:00.072",
        "20160525 13:30:00.075",
    ]),
    "ticker": ["GOOG", "MSFT", "MSFT", "MSFT", "GOOG", "AAPL", "GOOG","MSFT"],
    "bid": [720.50, 51.95, 51.97, 51.99, 720.50, 97.99, 720.50, 52.01],
    "ask": [720.93, 51.96, 51.98, 52.00, 720.93, 98.01, 720.88, 52.03],
}, columns=["time", "ticker", "bid", "ask"],)

pd.merge_asof(trades, quotes, on="time", by="ticker")

We only asof within 2ms between the quote time and the trade time.
pd.merge_asof(trades, quotes, on="time",
              by="ticker", tolerance=pd.Timedelta("2ms"))

We only asof within 10ms between the quote time and the trade time and we exclude exact matches on time. Note that though we exclude the exact matches (of the quotes), prior quotes do propagate to that point in time.
pd.merge_asof(trades, quotes, on='time', by="ticker",
             tolerance=pd.Timedelta('10ms'),
             allow_exact_matches=False)

2.7.4 Comparing objects
=======================
The compare()(DataFrame) and compare()(Series) methods allow you to compare two DataFrame or Series, respectively, and summarize their differences.
For example, you might want to compare two DataFrame and stack their differences side by side.
df = pd.DataFrame({
    "col1": ["a", "a", "b", "b", "a"],
    "col2": [1.0, 2.0, 3.0, np.nan, 5.0],
    "col3": [1.0, 2.0, 3.0, 4.0, 5.0],
})
df2 = df.copy()
df2.loc[0, "col1"] = "c"
df2.loc[2, "col3"] = 4.0
df.compare(df2)

By default, if two corresponding values are equal, they will be shown as NaN. Furthermore, if all values in an entire row / column, the row / column will be omitted from the result. The remaining differences will be aligned on columns.
If you wish, you may choose to stack the differences on rows.
df.compare(df2, align_axis=0)

If you wish to keep all original rows and columns, set keep_shape argument to True.
df.compare(df2, keep_shape=True)

2.8 Reshaping and pivot tables
===============================
2.8.1 Reshaping by pivoting DataFrame objects
----------------------------------------------
Data is often stored in so-called “stacked” or “record” format:
import pandas._testing as tm
def unpivot(frame):
    N, K = frame.shape
    data = {
        "value":frame.to_numpy().ravel("F"),
        "variable":np.asarray(frame.columns).repeat(N),
        "date":np.tile(np.asarray(frame.index), K)
    }
    return pd.DataFrame(data, columns=['date', 'variable', 'value'])

df = unpivot(tm.makeTimeDataFrame(3))

------------------------------------------------
Difference between repeat and tile
a = np.array([0, 1, 2])
print(np.repeat(a, 3)) # [0 0 0 1 1 1 2 2 2]
print(np.tile(a,3)) # [0 1 2 0 1 2 0 1 2]
------------------------------------------------

To select out everything for variable A we could do:
df[df["variable"] == "A"]

But suppose we wish to do time series operations with the variables. A better representation would be where the columns are the unique variables and an index of dates identifies individual observations. To reshape the data into this form, we use the DataFrame.pivot() method (also implemented as a top level function pivot()):

df.pivot(index='date', columns='variable', values='value')

If the values argument is omitted, and the input DataFrame has more than one column of values which are not used as column or index inputs to pivot, then the resulting “pivoted” DataFrame will have hierarchical columns whose topmost level indicates the respective value column:

pivoted = df.pivot(index='date', columns='variable')

You can then select subsets from the pivoted DataFrame:
pivoted = df.pivot(index='date', columns='variable')
pivoted['value2']

Note that this returns a view on the underlying data in the case where the data are homogeneously-typed.
Note: pivot() will error with a ValueError: Index contains duplicate entries, cannot reshape if the index/column pair is not unique. In this case, consider using pivot_table() which is a generalization of pivot that can handle duplicate values for one index/column pair.

2.8.2 Reshaping by stacking and unstacking
-------------------------------------------
width to height
-----------------
Closely related to the pivot() method are the related stack() and unstack() methods available on Series and DataFrame. These methods are designed to work together with MultiIndex objects (see the section on hierarchical indexing). Here are essentially what these methods do:
• stack: “pivot” a level of the (possibly hierarchical) column labels, returning a DataFrame with an index with a new inner-most level of row labels.
• unstack: (inverse operation of stack) “pivot” a level of the (possibly hierarchical) row index to the column axis, producing a reshaped DataFrame with a new inner-most level of column labels.

The clearest way to explain is by example. Let’s take a prior example data set from the hierarchical indexing section:
tuples = list(zip(*[
    [ "bar", "bar", "baz", "baz", "foo", "foo", "qux", "qux", ],
    [ "one", "two", "one", "two", "one", "two", "one", "two", ],
]))

index = pd.MultiIndex.from_tuples(tuples, names=["first", "second"])

df = pd.DataFrame(np.random.randn(8, 2), index=index, columns=["A", "B"])
df2 = df[:4]
df2

The stack function “compresses” a level in the DataFrame’s columns to produce either: • A Series, in the case of a simple column Index.
• A DataFrame, in the case of a MultiIndex in the columns.
If the columns have a MultiIndex, you can choose which level to stack. The stacked level becomes the new lowest level in a MultiIndex on the columns:
stacked = df2.stack()
stacked

With a “stacked” DataFrame or Series (having a MultiIndex as the index), the inverse operation of stack is unstack, which by default unstacks the last level:
stacked.unstack()
same can be achieved through pivot_table()
%timeit stacked.reset_index().rename(columns={"level_2":"col1", 0:"col2"}).pivot_table(index=['first', 'second'], columns='col1', values='col2')
unstack is definitely faster than pivot_table in this case.

stacked.unstack(1)
stacked.unstack(0)

If the indexes have names, you can use the level names instead of specifying the level numbers:
stacked.unstack("second")
Notice that the stack and unstack methods implicitly sort the index levels involved. Hence a call to stack and then unstack, or vice versa, will result in a sorted copy of the original DataFrame or Series:
index = pd.MultiIndex.from_product([[2, 1], ["a", "b"]])
df = pd.DataFrame(np.random.randn(4), index=index, columns=["A"])
print(df)
all(df.unstack().stack() == df.sort_index())

Multiple levels
You may also stack or unstack more than one level at a time by passing a list of levels, in which case the end result is as if each level in the list were processed individually.
columns = pd.MultiIndex.from_tuples([
    ("A", "cat", "long"), ("B", "cat", "long"),
    ("A", "dog", "short"), ("B", "dog", "short"),
], names=["exp", "animal", "hair_length"],)
df = pd.DataFrame(np.random.randn(4, 4), columns=columns)
df.stack(level=["animal", "hair_length"])

The list of levels can contain either level names or level numbers (but not a mixture of the two).
# df.stack(level=['animal', 'hair_length']) # from above is equivalent to:
df.stack(level=[1,2])

Missing data
-------------
These functions are intelligent about handling missing data and do not expect each subgroup within the hierarchical index to have the same set of labels. They also can handle the index being unsorted (but you can make it sorted by calling sort_index, of course). Here is a more complex example:
columns = pd.MultiIndex.from_tuples(
    [("A", "cat"), ("B", "dog"), ("B", "cat"),("A", "dog"),],
    names=['exp', "animals"]
)
index = pd.MultiIndex.from_product([("bar", "baz", "foo", "qux"), ("one", "two")],
                                   names=["first", "second"])
df = pd.DataFrame(np.random.randn(8, 4), index=index, columns=columns)
df2 = df.iloc[[0, 1, 2, 4, 5, 7]]

As mentioned above, stack can be called with a level argument to select which level in the columns to stack:
df2.stack('exp')
df.stack('animals')

Unstacking can result in missing values if subgroups do not have the same set of labels. By default, missing values will be replaced with the default fill value for that data type, NaN for float, NaT for datetimelike, etc. For integer types, by default data will converted to float and missing values will be set to NaN.

df3 = df.iloc[[0, 1, 4, 7], [1, 2]]
df3.unstack()

Alternatively, unstack takes an optional fill_value argument, for specifying the value of missing data.
df3.unstack(fill_value=0.)

With a MultiIndex
------------------
Unstacking when the columns are a MultiIndex is also careful about doing the right thing:
df[:3].unstack(1)
df[:3].unstack(0)

2.8.3 Reshaping by melt
========================
The top-level melt() function and the corresponding DataFrame.melt() are useful to massage a DataFrame into a format where one or more columns are identifier variables, while all other columns, considered measured variables, are “unpivoted” to the row axis, leaving just two non-identifier columns, “variable” and “value”. The names of those columns can be customized by supplying the var_name and value_name parameters.
cheese = pd.DataFrame({
    "first": ["John", "Mary"],
    "last": ["Doe", "Bo"],
    "height": [5.5, 6.0],
    "weight": [130, 150],
})
print(cheese)
cheese.melt(id_vars=['first', 'last'], var_name='quantity')

When transforming a DataFrame using melt(), the index will be ignored. The original index values can be kept around by setting the ignore_index parameter to False (default is True). This will however duplicate them.
index = pd.MultiIndex.from_tuples([("person", "A"), ("person", "B")])
cheese = pd.DataFrame({
    "first": ["John", "Mary"],
    "last": ["Doe", "Bo"],
    "height": [5.5, 6.0],
    "weight": [130, 150],
}, index=index)

cheese.melt(id_vars=["first", "last"], ignore_index=False)

Another way to transform is to use the wide_to_long() panel data convenience function. It is less flexible than melt(), but more user-friendly.
dft = pd.DataFrame({
    "A1970": {0: "a", 1: "b", 2: "c"},
    "A1980": {0: "d", 1: "e", 2: "f"},
    "B1970": {0: 2.5, 1: 1.2, 2: 0.7},
    "B1980": {0: 3.2, 1: 1.3, 2: 0.1},
    "X": dict(zip(range(3), np.random.randn(3))),
})

dft["id"] = dft.index
pd.wide_to_long(dft, ["A", "B"], i="id", j="year")

2.8.4 Combining with stats and GroupBy
--------------------------------------
It should be no shock that combining pivot / stack / unstack with GroupBy and the basic Series and DataFrame statistical functions can produce some very expressive and fast data manipulations.

df.stack().mean(axis=1).unstack()
# same result, another way
df.groupby(level=1, axis=1).mean()

which one is faster?
groupby is faster than stack and unstack.
%timeit df.groupby(level=1, axis=1).mean() # 1.77 ms ± 155 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
%timeit df.stack().mean(axis=1).unstack() # %timeit df.stack().mean(axis=1).unstack()

df.stack().groupby(level=1).mean()
df.mean().unstack(0)

which one is faster?
%timeit df.mean().unstack(0) # 1.7 ms ± 165 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
%timeit df.stack().groupby(level=1).mean() # 4.98 ms ± 369 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

2.8.5 Pivot tables
-------------------
While pivot() provides general purpose pivoting with various data types (strings, numerics, etc.), pandas also provides pivot_table() for pivoting with aggregation of numeric data.
The function pivot_table() can be used to create spreadsheet-style pivot tables.

It takes a number of arguments:
• data: a DataFrame object.
• values: a column or a list of columns to aggregate.
• index: a column, Grouper, array which has the same length as data, or list of them. Keys to group by on the pivot table index. If an array is passed, it is being used as the same manner as column values.
• columns: a column, Grouper, array which has the same length as data, or list of them. Keys to group by on the pivot table column. If an array is passed, it is being used as the same manner as column values.
• aggfunc: function to use for aggregation, defaulting to numpy.mean.

pd.pivot_table(df, values='D', index=['A','B'], columns="C")
pd.pivot_table(df, values='D', index=['B'], columns="C", aggfunc=np.sum)
pd.pivot_table(df, values=['D','E'], index=['B'],
               columns=["A","C"], aggfunc=np.sum)

The result object is a DataFrame having potentially hierarchical indexes on the rows and columns. If the values column name is not given, the pivot table will include all of the data that can be aggregated in an additional level of hierarchy in the columns:
pd.pivot_table(df, index=['A','B'], columns=["C"], aggfunc=np.sum)

Also, you can use Grouper for index and columns keywords.
pd.pivot_table(df, values='D', index=pd.Grouper(freq='3M', key='F'), columns='C')

You can render a nice output of the table omitting the missing values by calling to_string if you wish:
table = pd.pivot_table(df, index=['A','B'], columns='C')
print(table.to_string(na_rep=''))

Note that pivot_table is also available as an instance method on DataFrame, i.e. DataFrame. pivot_table().

Adding margins
If you pass margins=True to pivot_table, special All columns and rows will be added with partial group aggregates across the categories on the rows and columns:
df.pivot_table(index=['A', 'B'], columns='C', margins=True, aggfunc=np.sum)

2.8.6 Cross tabulations
------------------------
Use crosstab() to compute a cross-tabulation of two (or more) factors. By default crosstab computes a fre- quency table of the factors unless an array of values and an aggregation function are passed.
It takes a number of arguments
• index: array-like, values to group by in the rows.
• columns: array-like, values to group by in the columns.
• values: array-like, optional, array of values to aggregate according to the factors.
• aggfunc: function, optional, If no values array is passed, computes a frequency table.
• rownames: sequence, default None, must match number of row arrays passed.
• colnames: sequence, default None, if passed, must match number of column arrays passed.
• margins: boolean, default False, Add row/column margins (subtotals)
• normalize: boolean, {‘all’, ‘index’, ‘columns’}, or {0,1}, default False. Normalize by dividing all values by the sum of values.

Any Series passed will have their name attributes used unless row or column names for the cross-tabulation are specified
foo, bar, dull, shiny, one, two = "foo", "bar", "dull", "shiny", "one", "two"
a = np.array([foo, foo, bar, bar, foo, foo], dtype=object)
b = np.array([one, one, two, one, two, one], dtype=object)
c = np.array([dull, dull, shiny, dull, dull, shiny], dtype=object)
pd.crosstab(a,[b,c], rownames=["a"], colnames=["b", "c"])

If crosstab receives only two Series, it will provide a frequency table
df = pd.DataFrame({
    "A": [1, 2, 2, 2, 2], "B": [3, 3, 4, 4, 4], "C": [1, 1, np.nan, 1, 1]
})
pd.crosstab(df['A'], df['B'], margins=True)

crosstab can also be implemented to Categorical data.
foo = pd.Categorical(["a", "b"], categories=["a", "b", "c"])
bar = pd.Categorical(["d", "e"], categories=["d", "e", "f"])
pd.crosstab(foo, bar)

If you want to include all of data categories even if the actual data does not contain any instances of a particular category, you should set dropna=False.
pd.crosstab(foo, bar, dropna=False)

Normalization
Frequency tables can also be normalized to show percentages rather than counts using the normalize argument:
pd.crosstab(df["A"], df["B"], normalize=True)

normalize can also normalize values within each row or within each column:
pd.crosstab(df["A"], df["B"], normalize="columns")

crosstab can also be passed a third Series and an aggregation function (aggfunc) that will be applied to the values of the third Series within each group defined by the first two Series:
pd.crosstab(df["A"], df["B"], values=df["C"], aggfunc=np.sum)

Adding margins
Finally, one can also add margins or normalize this output.
pd.crosstab(df["A"], df["B"], values=df["C"], aggfunc=np.sum, normalize=True,margins=True)

2.8.7 Tiling
-------------
The cut() function computes groupings for the values of the input array and is often used to transform continuous variables to discrete or categorical variables:
ages = np.array([1, 2, 4, 3, 8, 6])
pd.cut(ages, bins=3)

If the bins keyword is an integer, then equal-width bins are formed. Alternatively we can specify custom bin-edges:
c = pd.cut(ages, bins=[0, 3, 6, 9])

If the bins keyword is an IntervalIndex, then these will be used to bin the passed data.:
pd.cut([25, 20, 50], bins=c.categories)

2.8.8 Computing indicator / dummy variables
-------------------------------------------
To convert a categorical variable into a “dummy” or “indicator” DataFrame, for example a column in a DataFrame (a Series) which has k distinct values, can derive a DataFrame containing k columns of 1s and 0s using get_dummies():
Similar to symbol in kdb, it replaces the categorical values with their index.
df = pd.DataFrame({"key": list("bbac"), "data1": range(4)})
print(df)
pd.get_dummies(df['key'])

Sometimes it’s useful to prefix the column names, for example when merging the result with the original DataFrame:
dummies = pd.get_dummies(df["key"], prefix="key")
dummies

df[["data1"]].join(dummies)

This function is often used along with discretization functions like cut:

values = np.random.randn(10)
bins = [0, 0.2, 0.4, 0.6, 0.8, 1]
print(values)
pd.cut(values, bins)
pd.get_dummies(pd.cut(values, bins))

get_dummies() also accepts a DataFrame. By default all categorical variables (categorical in the statistical
sense, those with object or categorical dtype) are encoded as dummy variables.

df = pd.DataFrame({"A": ["a", "b", "a"], "B": ["c", "c", "b"],
                   "C": [1, 2,3]})
pd.get_dummies(df)

All non-object columns are included untouched in the output. You can control the columns that are encoded with the columns keyword.
pd.get_dummies(df, columns=["A"])

Notice that the B column is still included in the output, it just hasn’t been encoded. You can drop B before calling get_dummies if you don’t want to include it in the output.

As with the Series version, you can pass values for the prefix and prefix_sep. By default the column name is used as the prefix, and ‘_’ as the prefix separator. You can specify prefix and prefix_sep in 3 ways:
• string: Use the same value for prefix or prefix_sep for each column to be encoded.
• list: Must be the same length as the number of columns being encoded.
• dict: Mapping column name to prefix.

simple = pd.get_dummies(df, prefix="new_prefix")
from_list = pd.get_dummies(df, prefix=["from_A", "from_B"])
from_dict = pd.get_dummies(df, prefix={"B": "from_B", "A": "from_A"})

Sometimes it will be useful to only keep k-1 levels of a categorical variable to avoid collinearity when feeding the result to statistical models. You can switch to this mode by turn on drop_first.
s = pd.Series(list("abcaa"))
pd.get_dummies(s)
pd.get_dummies(s, drop_first=True)

When a column contains only one level, it will be omitted in the result.
df = pd.DataFrame({"A": list("aaaaa"), "B": list("ababc")})
pd.get_dummies(df)
pd.get_dummies(df, drop_first=True)

By default new columns will have np.uint8 dtype. To choose another dtype, use the dtype argument:
df = pd.DataFrame({"A": list("abc"), "B": [1.1, 2.2, 3.3]})
pd.get_dummies(df, dtype=bool).dtypes

2.8.9 Factorizing values
-------------------------
To encode 1-d values as an enumerated type use factorize():
It returns array of index of unique value(labels) and unique values.
x = pd.Series(["A", "A", np.nan, "B", 3.14, np.inf])
print(x)
pd.factorize(x)

Note that factorize is similar to numpy.unique, but differs in its handling of NaN:
print(np.unique([1,3,1,2,5,np.nan,np.inf, np.nan]))
print(pd.factorize([1,3,1,2,5,np.nan, np.inf, np.nan]))

Note: The following numpy.unique will fail under Python3 with a TypeError because of an ordering bug.
x = pd.Series(['A', 'A', np.nan, 'B', 3.14, np.inf])
pd.factorize(x, sort=True)
np.unique(x, return_inverse=True)[::-1] # TypeError: '<' not supported between instances of 'float' and 'str'

Note: If you just want to handle one column as a categorical variable (like R’s factor),you can used f["cat_col"] = pd.Categorical(df["col"]) or df["cat_col"] = df["col"].astype("category").

2.8.10 Examples
---------------
In this section, we will review frequently asked questions and examples. The column names and relevant column values are named to correspond with how this DataFrame will be pivoted in the answers below.
np.random.seed([3, 1415])
n = 20
cols = np.array(["key", "row", "item", "col"])
df = cols + pd.DataFrame((np.random.randint(5, size=(n, 4)) // [2, 1, 2, 1]).astype(str))
df.columns = cols
df = df.join(pd.DataFrame(np.random.rand(n, 2).round(2)).add_prefix("val"))
df

Pivoting with single aggregations
----------------------------------
Suppose we wanted to pivot df such that the col values are columns, row values are the index, and the mean of val0 are the values? In particular, the resulting DataFrame should look like:
df.pivot_table(columns='col', index='row', values='val0', aggfunc=np.mean)

Note that we can also replace the missing values by using the fill_value parameter.
df.pivot_table(columns='col', index='row', values='val0', fill_value=0)

Also note that we can pass in other aggregation functions as well. For example, we can also pass in sum.
df.pivot_table(columns='col', index='row', values='val0',fill_value=0, aggfunc=np.sum)

Another aggregation we can do is calculate the frequency in which the columns and rows occur together a.k.a. “cross tabulation”. To do this, we can pass size to the aggfunc parameter.
df.pivot_table(columns='col', index='row', values='val0',fill_value=0, aggfunc='size')

Pivoting with multiple aggregations
------------------------------------
We can also perform multiple aggregations. For example, to perform both a sum and mean, we can pass in a list to the aggfunc argument.
df.pivot_table(columns='col', index='row', values='val0', aggfunc=[np.sum, np.mean])

Note to aggregate over multiple value columns, we can pass in a list to the values parameter.
df.pivot_table(columns='col', index='row', values=['val0', 'val1'],)

Note to subdivide over multiple columns we can pass in a list to the columns parameter.
df.pivot_table(columns=['item','col'], index='row', values=['val0', 'val1'],)

2.8.11 Exploding a list-like column
------------------------------------
Sometimes the values in a column are list-like.
keys = ["panda1", "panda2", "panda3"]
values = [["eats", "shoots"], ["shoots", "leaves"], ["eats", "leaves"]]
df = pd.DataFrame({"keys": keys, "values": values})
df

We can ‘explode’ the values column, transforming each list-like to a separate row, by using explode(). This will replicate the index values from the original row:
df["values"].explode()

You can also explode the column in the DataFrame.
df.explode("values")

Series.explode() will replace empty lists with np.nan and preserve scalar entries. The dtype of the resulting Series is always object.
s = pd.Series([[1, 2, 3], "foo", [], ["a", "b"]])
s.explode().reset_index(drop=True)

Here is a typical usecase. You have comma separated strings in a column and want to expand this.
df = pd.DataFrame([{"var1": "a,b,c", "var2": 1},
                   {"var1": "d,e,f", "var2": 2}])
df

Creating a long form DataFrame is now straightforward using explode and chained operations
df = pd.DataFrame([{"var1": "a,b,c", "var2": 1},
                   {"var1": "d,e,f", "var2": 2}])
df.assign(var1=df.var1.str.split(',')).explode('var1')

===========================
2.9 Working with text data
===========================
New in version 1.0.0.
There are two ways to store text data in pandas:
1. object -dtype NumPy array.
2. StringDtype extension type.
We recommend using StringDtype to store text data.
Prior to pandas 1.0, object dtype was the only option. This was unfortunate for many reasons:
1. You can accidentally store a mixture of strings and non-strings in an object dtype array. It’s better to have a dedicated dtype.
2. object dtype breaks dtype-specific operations like DataFrame.select_dtypes(). There isn’t a clear way to select just text while excluding non-text but still object-dtype columns.
3. When reading code, the contents of an object dtype array is less clear than 'string'.
Currently, the performance of object dtype arrays of strings and arrays.StringArray are about the same. We expect future enhancements to significantly increase the performance and lower the memory overhead of StringArray .

 Warning: StringArray is currently considered experimental. The implementation and parts of the API may change without warning.

 For backwards-compatibility, object dtype remains the default type we infer a list of strings to
 pd.Series(["a", "b", "c"]) # object
 To explicitly request string dtype, specify the dtype
 pd.Series(["a", "b", "c"], dtype="string")
pd.Series(["a", "b", "c"], dtype=pd.StringDtype())

Or astype after the Series or DataFrame is created
s = pd.Series(["a", "b", "c"])
s.astype("string")

You can also use StringDtype/"string" as the dtype on non-string data and it will be converted to string dtype:
s = pd.Series(["a", 2, np.nan], dtype="string")
type(s[1]) # str

or convert from existing pandas data:
s1 = pd.Series([1, 2, np.nan], dtype="Int64")
s2 = s1.astype("string")

Behavior differences -- Differences between String and Object
---------------------
These are places where the behavior of StringDtype objects differ from object dtype
1. For StringDtype, string accessor methods that return numeric output will always return a nullable integer dtype, rather than either int or float dtype, depending on the presence of NA values. Methods returning boolean output will return a nullable boolean dtype.
s = pd.Series(["a", None, "b","a"], dtype="string")
s.str.count("a") # Int64
s.dropna().str.count("a") # Int64

Both outputs are Int64 dtype. Compare that with object-dtype
s = pd.Series(["a", None, "b","a"], dtype="object")
s.str.count("a") # float64
s.dropna().str.count("a") # Int64

When NA values are present, the output dtype is float64. Similarly for methods returning boolean values.
s = pd.Series(["a", None, "b","a"], dtype="string")
s.str.isdigit() # boolean
s.dropna().str.match("a") # boolean

With object dtype.
s = pd.Series(["a", None, "b","a"], dtype="object")
s.str.isdigit() # object
# s.dropna().str.match("a") # bool

2. Some string methods, like Series.str.decode() are not available on StringArray because StringArray only holds strings, not bytes.
3. In comparison operations, arrays.StringArray and Series backed by a StringArray will return an object with BooleanDtype, rather than a bool dtype object. Missing values in a StringArray will propagate in comparison operations, rather than always comparing unequal like numpy.nan.
Everything else that follows in the rest of this document applies equally to string and object dtype.

2.9.2 String methods
---------------------
Series and Index are equipped with a set of string processing methods that make it easy to operate on each element of the array. Perhaps most importantly, these methods exclude missing/NA values automatically. These are accessed via the str attribute and generally have names matching the equivalent (scalar) built-in string methods:
s = pd.Series(["A", "B", "C", "Aaba", "Baca", np.nan, "CABA", "dog", "cat"],
              dtype='string')
s.str.lower()
s.str.upper()
s.str.len()

idx = pd.Index([" jack", "jill ", " jesse ", "frank"])
idx.str.strip()
idx.str.lstrip()
idx.str.rstrip()

The string methods on Index are especially useful for cleaning up or transforming DataFrame columns. For instance, you may have columns with leading or trailing whitespace:
df = pd.DataFrame(np.random.randn(3, 2),
                  columns=[" Column A ", " Column B "],
                  index=range(3))
df

df = pd.DataFrame(np.random.randn(3, 2),
                  columns=[" Column A ", " Column B "],
                  index=range(3))
df.columns.str.strip()
df.columns.str.lower()

These string methods can then be used to clean up the columns as needed. Here we are removing leading and trailing whitespaces, lower casing all names, and replacing any remaining whitespaces with underscores:
df.columns = df.columns.str.strip().str.lower().str.replace(" ", "_")

Note: If you have a Series where lots of elements are repeated (i.e. the number of unique elements in the Series is a lot smaller than the length of the Series), it can be faster to convert the original Series to one of type category and then use .str.<method> or .dt.<property> on that. The performance difference comes from the fact that, for Series of type category, the string operations are done on the .categories and not on each element of the Series.
Please note that a Series of type category with string .categories has some limitations in comparison to Series of type string(e.g.you can’t add strings to each other:s + " " + s won’t work if s is a Series of type category). Also, .str methods which operate on elements of type list are not available on such a Series.

Generally speaking, the .str accessor is intended to work only on strings. With very few exceptions, other uses are not supported, and may be disabled at a later point.

2.9.3 Splitting and replacing strings
-------------------------------------
Methods like split return a Series of lists:
s2 = pd.Series(["a_b_c", "c_d_e", np.nan, "f_g_h"], dtype="string")
s2.str.split("_")
Elements in the split lists can be accessed using get or [] notation:
s2.str.split("_").str.get(1)
s2.str.split("_").str[1]

It is easy to expand this to return a DataFrame using expand.
s2.str.split("_", expand=True)

When original Series has StringDtype, the output columns will all be StringDtype as well. It is also possible to limit the number of splits:
s2.str.split("_", expand=True, n=1)

rsplit is similar to split except it works in the reverse direction, i.e., from the end of the string to the beginning of the string:
s2.str.rsplit("_", expand=True, n=1)

replace optionally uses regular expressions:
s3 = pd.Series(["base", "A", "B", "C", "Aaba", "Baca", "", np.nan, "CABA", "dog", "cat"],
               dtype='string')
print(s3)
s3.str.replace("^.a|dog", "XX-XX", case=False, regex=True)

Warning: Some caution must be taken when dealing with regular expressions! The current behavior is to treat single character patterns as literal strings, even when regex is set to True. This behavior is deprecated and will be removed in a future version so that the regex keyword is always respected.

If you want literal replacement of a string (equivalent to str.replace()), you can set the optional regex parameter to False, rather than escaping each character. In this case both pat and repl must be strings:

dollars = pd.Series(["12", "-$10", "$10,000"], dtype="string")
# These lines are equivalent
dollars.str.replace(r"-\$", "-", regex=True)
dollars.str.replace("-$", "-", regex=False)

The replace method can also take a callable as replacement. It is called on every pat using re.sub(). The callable should expect one positional argument (a regex object) and return a string.

# Reverse every lowercase alphabetic word
pat = r"[a-z]+"
def repl(m):
    return m.group(0)[::-1]
pd.Series(["foo 123", "bar baz", np.nan], dtype="string").str.replace(
    pat, repl, regex=True)

# Using regex groups
pat = r"(?P<one>\w+) (?P<two>\w+) (?P<three>\w+)"
def repl(m):
    return m.group("two").swapcase()

pd.Series(["Foo Bar Baz", np.nan], dtype="string").str.replace(pat, repl, regex=True)

The replace method also accepts a compiled regular expression object from re.compile() as a pattern. All flags should be included in the compiled regular expression object.
import re
regex_pat = re.compile(r"^.a|dog", flags=re.IGNORECASE)
s3.str.replace(regex_pat, "XX-XX ", regex=True)

Including a flags argument when calling replace with a compiled regular expression object will raise a ValueError.
s3.str.replace(regex_pat, 'XX-XX ', flags=re.IGNORECASE)
ValueError: case and flags cannot be set when pat is a compiled regex

2.9.4 Concatenation
--------------------
There are several ways to concatenate a Series or Index, either with itself or others, all based on cat(), resp. Index.str.cat.

Concatenating a single Series into a string
The content of a Series (or Index) can be concatenated:
s = pd.Series(['a','b','c','d'], dtype='string')
s.str.cat(sep=',')

If not specified, the keyword sep for the separator defaults to the empty string, sep='':
s.str.cat() # 'abcd'

By default, missing values are ignored. Using na_rep, they can be given a representation:
t = pd.Series(["a", "b", np.nan, "d"], dtype="string")
t.str.cat(sep=',', na_rep='-')
t.str.cat(sep=',', na_rep='Ramesh')

Concatenating a Series and something list-like into a Series
The first argument to cat() can be a list-like object, provided that it matches the length of the calling Series (or Index).
s.str.cat(['A', 'B', 'C', 'D'])

Missing values on either side will result in missing values in the result as well, unless na_rep is specified:
s.str.cat(t)
s.str.cat(t, na_rep='Suresh')

Concatenating a Series and something array-like into a Series
The parameter others can also be two-dimensional. In this case, the number or rows must match the lengths of the calling Series (or Index).
d = pd.concat([t, s], axis=1)
s.str.cat(d, na_rep='-')

Concatenating a Series and an indexed object into a Series, with alignment
For concatenation with a Series or DataFrame, it is possible to align the indexes before concatenation by setting the join-keyword.
u = pd.Series(["b", "d", "a", "c"], index=[1, 3, 0, 2], dtype="string")
s.str.cat(u)
s.str.cat(u, join='left')

Warning: If the join keyword is not passed,the method cat() will currently fallback to the behavior before version 0.23.0 (i.e. no alignment), but a FutureWarning will be raised if any of the involved indexes differ, since this default will change to join='left' in a future version.

The usual options are available for join (one of 'left', 'outer', 'inner', 'right'). In particular, alignment also means that the different lengths do not need to coincide anymore.
v = pd.Series(["z", "a", "b", "d", "e"], index=[-1, 0, 1, 3, 4], dtype="string")
s.str.cat(v, na_rep="-")
s.str.cat(v, join='left', na_rep="-")
s.str.cat(v, join='outer', na_rep="-")
s.str.cat(v, join='inner', na_rep="-")
s.str.cat(v, join='right', na_rep="-")

The same alignment can be used when others is a DataFrame:
f = d.loc[[3, 2, 1, 0], :]
s.str.cat(f, join='left', na_rep='-')

Concatenating a Series and many objects into a Series
Several array-like items (specifically: Series, Index, and 1-dimensional variants of np.ndarray) can be com- bined in a list-like container (including iterators, dict-views, etc.).
s.str.cat([u, u.to_numpy()], join="left")

All elements without an index (e.g. np.ndarray) within the passed list-like must match in length to the calling Series (or Index), but Series and Index may have arbitrary length (as long as alignment is not disabled with join=None):
s.str.cat([v, u, u.to_numpy()], join="outer", na_rep="-")

If using join='right' on a list-like of others that contains different indexes, the union of these indexes will be used as the basis for the final concatenation:
u.loc[[3]]
v.loc[[-1, 0]]
s.str.cat([u.loc[[3]], v.loc[[-1, 0]]], join="right", na_rep="-")

2.9.5 Indexing with .str
You can use [] notation to directly index by position locations. If you index past the end of the string, the result will be a NaN.
s = pd.Series(["A", "B", "C", "Aaba", "Baca", np.nan, "CABA", "dog", "cat"],
              dtype='string')
s.str[0]
s.str[1]

2.9.6 Extracting substrings
----------------------------
Extract first match in each subject (extract)
Warning: Before version 0.23, argument expand of the extract method defaulted to False. When expand=False, expand returns a Series, Index, or DataFrame, depending on the subject and regular expression pattern. When expand=True, it always returns a DataFrame, which is more consistent and less confusing from the perspective of a user. expand=True has been the default since version 0.23.0.

The extract method accepts a regular expression with at least one capture group.
Extracting a regular expression with more than one group returns a DataFrame with one column per group.

pd.Series(["a1", "b2", "c3"],dtype="string",).str.extract(r"([ab])(\d)", expand=False)

Elements that do not match return a row filled with NaN. Thus, a Series of messy strings can be “converted” into a like-indexed Series or DataFrame of cleaned-up or more useful strings, without necessitating get() to access tuples or re.match objects. The dtype of the result is always object, even if no match is found and the result only contains NaN.
Named groups like
pd.Series(["a1", "b2", "c3"], dtype="string").str.extract(
    r"(?P<letter>[ab])(?P<digit>\d)", expand=False)

and optional groups like
pd.Series(["a1", "b2", "3"],dtype="string",).str.extract(r"([ab])?(\d)", expand=False)
can also be used. Note that any capture group names in the regular expression will be used for column names; otherwise capture group numbers will be used.

Extracting a regular expression with one group returns a DataFrame with one column if expand=True.
pd.Series(["a1", "b2", "c3"], dtype="string").str.extract(r"[ab](\d)",expand=True)
type(pd.Series(["a1", "b2", "c3"], dtype="string").str.extract(r"[ab](\d)",expand=True)) # pandas.core.frame.DataFrame


It returns a Series if expand=False.
pd.Series(["a1", "b2", "c3"], dtype="string").str.extract(r"[ab](\d)",expand=False
type(pd.Series(["a1", "b2", "c3"], dtype="string").str.extract(r"[ab](\d)",expand=False)) # pandas.core.series.Series

Calling on an Index with a regex with exactly one capture group returns a DataFrame with one column if expand=True.
s.index.str.extract("(?P<letter>[a-zA-Z])", expand=True)

It returns an Index if expand=False.
s.index.str.extract("(?P<letter>[a-zA-Z])", expand=False)

Calling on an Index with a regex with more than one capture group returns a DataFrame if expand=Tru
s.index.str.extract("(?P<letter>[a-zA-Z])(\d+)", expand=True)

It raises ValueError if expand=False.
s.index.str.extract("(?P<letter>[a-zA-Z])(\d+)", expand=False)
ValueError: only one regex group is supported with Index

The table below summarizes the behavior of extract(expand=False) (input subject in first column, number of groups in regex in first row)
                    1 group             >1 group
Index               Index               ValueError
Series              Series              DataFrame

Extract all matches in each subject (extractall)
Unlike extract (which returns only the first match),
s = pd.Series(["a1a2", "b1", "c1"], index=["A", "B", "C"], dtype="string")
two_groups = "(?P<letter>[a-z])(?P<digit>[0-9])"
s.str.extract(two_groups, expand=True)

the extractall method returns every match. The result of extractall is always a DataFrame with a MultiIndex on its rows. The last level of the MultiIndex is named match and indicates the order in the subject.
s.str.extractall(two_groups)

When each subject string in the Series has exactly one match,
s = pd.Series(["a3", "b3", "c2"], dtype="string")
then
extractall(pat).xs(0, level='match')gives the same result as extract(pat).
extract_result = s.str.extract(two_groups, expand=True)
extractall_result = s.str.extractall(two_groups)
extractall_result.xs(0, level="match")

Index also supports .str.extractall. It returns a DataFrame which has the same result as a Series.str. extractall with a default index (starts from 0).
pd.Index(["a1a2", "b1", "c1"]).str.extractall(two_groups)
pd.Series(["a1a2", "b1", "c1"], dtype="string").str.extractall(two_groups)

2.9.7 Testing for strings that match or contain a pattern
----------------------------------------------------------
You can check whether elements contain a pattern:
pattern = r"[0-9][a-z]"
pd.Series(["1", "2", "3a", "3b", "03c", "4dx"],dtype="string",).str.contains(pattern)

Or whether elements match a pattern:
pd.Series(["1", "2", "3a", "3b", "03c", "4dx"],dtype="string",).str.match(pattern)
pd.Series(["1", "2", "3a", "3b", "03c", "4dx"],dtype="string",).str.fullmatch(pattern)

Note: The distinction between match, fullmatch, and contains is strictness: fullmatch tests whether the entire string matches the regular expression; match tests whether there is a match of the regular expression that begins at the first character of the string; and contains tests whether there is a match of the regular expression at any position within the string.

The corresponding functions in the re package for these three match modes are re.fullmatch, re.match, and re.search, respectively.

Methods like match, fullmatch, contains, startswith, and endswith take an extra na argument so missing values can be considered True or False:
s4 = pd.Series(["A", "B", "C", "Aaba", "Baca", np.nan, "CABA", "dog", "cat"], dtype="string")
s4.str.contains("A", na=False)

2.9.8 Creating indicator variables
-----------------------------------
You can extract dummy variables from string columns. For example if they are separated by a '|':
s = pd.Series(["a", "a|b", np.nan, "a|c"], dtype="string")
s.str.get_dummies(sep="|")

String Index also supports get_dummies which returns a MultiIndex.
idx = pd.Index(["a", "a|b", np.nan, "a|c"])
idx.str.get_dummies(sep="|")

2.9.9 Method summary
--------------------
Method                              Description
cat()           Concatenate strings
split()         Split strings on delimiter
rsplit()        Split strings on delimiter working from the end of the string
get()           Index into each element (retrieve i-th element)
join()          Join strings in each element of the Series with passed separator
get_dummies()   Split strings on the delimiter returning DataFrame of dummy variables
contains()      Return boolean array if each string contains pattern/regex
replace()       Replace occurrences of pattern/regex/string with some other string or the return value of a callable given the occurrence
repeat()        Duplicatevalues(s.str.repeat(3)equivalent to x * 3)
pad()           Add whitespace to left, right, or both sides of strings
center()        Equivalent to str.center
ljust()         Equivalent to str.ljust
rjust()         Equivalent to str.rjust
zfill()         Equivalent to str.zfill
wrap()          Split long strings into lines with length less than a given width
slice()         Slice each string in the Series
slice_replace() Replace slice in each string with passed value
count()         Count occurrences of pattern
startswith()    Equivalent to str.startswith(pat) for each element
endswith()      Equivalent to str.endswith(pat) for each element
findall()       Compute list of all occurrences of pattern/regex for each string
match()         Call re.match on each element, returning matched groups as list
extract()       Call re.search on each element, returning DataFrame with one row for each element and one column for each regex capture group
extractall()    Call re.findall on each element, returning DataFrame with one row for each match and one column for each regex capture group
len()           Compute string lengths
strip()         Equivalent to str.strip
rstrip()        Equivalent to str.rstrip
lstrip()        Equivalent to str.lstrip
partition()     Equivalent to str.partition
rpartition()    Equivalent to str.rpartition
lower()         Equivalent to str.lower
casefold()      Equivalent to str.casefold
upper()         Equivalent to str.upper
find()          Equivalent to str.find
rfind()         Equivalent to str.rfind
index()         Equivalent to str.index
rindex()        Equivalent to str.rindex
capitalize()    Equivalent to str.capitalize
swapcase()      Equivalent to str.swapcase
normalize()     Return Unicode normal form. Equivalent to unicodedata.normalize
translate()     Equivalent to str.translate
isalnum()       Equivalent to str.isalnum
isalpha()       Equivalent to str.isalpha
isdigit()       Equivalent to str.isdigit
isspace()       Equivalent to str.isspace
islower()       Equivalent to str.islower
isupper()       Equivalent to str.isupper
istitle()       Equivalent to str.istitle
isnumeric()     Equivalent to str.isnumeric
isdecimal()     Equivalent to str.isdecimal

================================
2.10 Working with missing data
================================
In this section, we will discuss missing (also referred to as NA) values in pandas.
Note: The choice of using NaN internally to denote missing data was largely for simplicity and performance reasons. Starting from pandas 1.0, some optional data types start experimenting with a native NA scalar using a mask-based approach.

2.10.1 Values considered “missing”
----------------------------------
As data comes in many shapes and forms, pandas aims to be flexible with regard to handling missing data. While NaN is the default missing value marker for reasons of computational speed and convenience, we need to be able to easily detect this value with data of different types: floating point, integer, boolean, and general object. In many cases, however, the Python None will arise and we wish to also consider that “missing” or “not available” or “NA”.

Note: If you want to consider inf and -inf to be “NA” in computations,you can set pandas.options.mode.use_inf_as_na = True.
df = pd.DataFrame(np.random.randn(5, 3),index=["a", "c", "e", "f", "h"],
                  columns=["one", "two", "three"],)
df["four"] = "bar"
df["five"] = df["one"] > 0
df2 = df.reindex(["a", "b", "c", "d", "e", "f", "g", "h"])
df2

To make detecting missing values easier (and across different array dtypes), pandas provides the isna() and notna() functions, which are also methods on Series and DataFrame objects:
pd.isna(df2["one"])
pd.notna(df2["one"])
pd.isnull(df2["one"])
pd.notnull(df2["one"])

df2.two.isna()
df2['three'].notna()
df2.isna()

Warning: One has to be mindful that in Python (and NumPy),the nan's don’t compare equal,but None's do. Note that pandas/NumPy uses the fact that np.nan != np.nan,and treats None like np.nan.

print(None == None) # True
print(None == np.nan) # False
print(np.nan == np.nan) # False
print(id(np.nan)) # 4451886096
print(id(np.nan)) # 4451886096
print(np.nan is np.nan) # True

So as compared to above, a scalar equality comparison versus a None/np.nan doesn’t provide useful information.
df2['one'] == np.nan

Integer dtypes and missing data
--------------------------------
Because NaN is a float, a column of integers with even one missing values is cast to floating-point dtype (see Support for integer NA for more). pandas provides a nullable integer array, which can be used by explicitly requesting the dtype:
pd.Series([1, 2, np.nan, 4], dtype=pd.Int64Dtype())

Datetimes
-----------
For datetime64[ns] types, NaT represents missing values. This is a pseudo-native sentinel value that can be represented by NumPy in a singular dtype (datetime64[ns]). pandas objects provide compatibility between NaT and NaN.
df2 = df.copy()
df2["timestamp"] = pd.Timestamp("20120101")
df2.loc[["a", "c", "h"], ["one", "timestamp"]] = np.nan
df2.dtypes.value_counts()

2.10.2 Inserting missing data
------------------------------
You can insert missing values by simply assigning to containers. The actual missing value used will be chosen based on the dtype.
For example, numeric containers will always use NaN regardless of the missing value type chosen:
s = pd.Series([1, 2, 3])
s.loc[0] = None
Likewise, datetime containers will always use NaT.

For object containers, pandas will use the value given:
s = pd.Series(["a", "b", "c"])
s.loc[0] = None
s.loc[1] = np.nan

2.10.3 Calculations with missing data
-------------------------------------
Missing values propagate naturally through arithmetic operations between pandas objects.
a = df2[['one', 'two']]
b = df2.loc[:, df2.dtypes == 'float64']
a+b

dft = pd.DataFrame({
    'a':[10, np.nan, 30],
    'b':[np.nan, 20, 30]
})

dft2 = pd.DataFrame({
    'a':[np.nan, 20, 30],
    'b':[np.nan, 20, np.nan]
})

dft + dft2
dft.add(dft2, fill_value=0)

• When summing data, NA (missing) values will be treated as zero.
• If the data are all NA, the result will be 0.
• Cumulative methods like cumsum() and cumprod() ignore NA values by default, but preserve them in the resulting arrays. To override this behaviour and include NA values, use skipna=False.

df2.one.sum()
df2.mean(1)
df2.cumsum()
df2.cumsum(skipna=False)

2.10.4 Sum/prod of empties/nans
-------------------------------
 Warning: This behavior is now standard as of v0.22.0 and is consistent with the default in numpy; previously sum/prod of all-NA or empty Series/DataFrames would return NaN. See v0.22.0 whatsnew for more.

The sum of an empty or all-NA Series or column of a DataFrame is 0.
pd.Series([np.nan]).sum()
pd.Series([], dtype="float64").sum()

The product of an empty or all-NA Series or column of a DataFrame is 1.
pd.Series([np.nan]).prod()
pd.Series([], dtype="float64").prod()

2.10.5 NA values in GroupBy
---------------------------
NA groups in GroupBy are automatically excluded. This behavior is consistent with R, for example:
df.groupby("one").mean()

Cleaning / filling missing data
pandas objects are equipped with various data manipulation methods for dealing with missing data.

2.10.6 Filling missing values: fillna
--------------------------------------
fillna() can “fill in” NA values with non-NA data in a couple of ways, which we illustrate:

Replace NA with a scalar value
-------------------------------
df2.fillna(0)
df2.one.fillna("missing")

Fill gaps forward or backward
------------------------------
Using the same filling arguments as reindexing, we can propagate non-NA values forward or backward:
df.fillna(method="pad")

Question:https://stackoverflow.com/questions/67197427/how-to-fill-a-pandas-df-with-rows-for-values-that-do-not-exist-in-a-column-and-t/67197506#67197506
ffill the items in the dataframe
dft = pd.DataFrame({'week': [1,2,4,5,7], 'items':[1,2,3,4,5]})
Many ways to do it using method=pad, ffill or ffill()
dft.set_index('week').reindex(range(1,8)).fillna(method='pad').reset_index()
%timeit dft.set_index('week').reindex(range(1,8)).fillna(method='ffill').reset_index()
%timeit dft.set_index('week').reindex(range(1,8)).ffill().reset_index()

Limit the amount of filling
----------------------------
If we only want consecutive gaps filled up to a certain number of data points, we can use the limit keyword:
df.fillna(method="pad", limit=1)

To remind you, these are the available filling methods:
Method                      Action
pad / ffill             Fill values forward
bfill / backfill        Fill values backward

With time series data, using pad/ffill is extremely common so that the “last known value” is available at every time point.
ffill() is equivalent to fillna(method='ffill') and bfill() is equivalent to fillna(method='bfill')

2.10.7 Filling with a PandasObject
----------------------------------
You can also fillna using a dict or Series that is alignable. The labels of the dict or index of the Series must match the columns of the frame you wish to fill. The use case of this is to fill a DataFrame with the mean of that column.
dff = pd.DataFrame(np.random.randn(10, 3), columns=list("ABC"))
dff.iloc[3:5, 0] = np.nan
dff.iloc[4:6, 1] = np.nan
dff.iloc[5:8, 2] = np.nan
dff.fillna(dff.mean())
dff.fillna(dff.mean()["B":"C"])

Same result as above, but is aligning the ‘fill’ value which is a Series in this case.
dff.where(pd.notna(dff), dff.mean(), axis="columns")

2.10.8 Dropping axis labels with missing data: dropna
------------------------------------------------------
You may wish to simply exclude labels from a data set which refer to missing data. To do this, use dropna():
df = df.iloc[:,:3]
df['one']=np.nan
df.dropna(axis=0)
df.dropna(axis=1)
df['one'].dropna()

An equivalent dropna() is available for Series. DataFrame.dropna has considerably more options than Series.dropna.

2.10.9 Interpolation
=====================
Both Series and DataFrame objects have interpolate() that, by default, performs linear interpolation at missing data points.
rng = pd.date_range('2000.01.31', periods=100, freq='BM')
ts = pd.Series(np.random.randn(len(rng)), index=rng)
ts[1:21]=np.nan
ts[44:58]=np.nan
ts.count() ## 66
ts.plot()

Index aware interpolation is available via the method keyword:
ts2.interpolate(method="time")
ts.interpolate(method='time').head()

For a floating-point index, use method='values':
ts2.interpolate(method="values")

You can also interpolate with a DataFrame:
df = pd.DataFrame({"A": [1, 2.1, np.nan, 4.7, 5.6, 6.8],"B": [0.25, np.nan, np.nan, 4, 12.2, 14.4],})
df.interpolate()

The method argument gives access to fancier interpolation methods. If you have scipy installed, you can pass the name of a 1-d interpolation routine to method. You’ll want to consult the full scipy interpolation documentation and reference guide for details. The appropriate interpolation method will depend on the type of data you are working with.
• If you are dealing with a time series that is growing at an increasing rate, method='quadratic' may be appropriate.
• If you have values approximating a cumulative distribution function, then method='pchip' should work well.
• To fill missing values with goal of smooth plotting, consider method='akima'.
ts.interpolate(method='quadratic').head()

2.10.10 Replacing generic values
--------------------------------
Often times we want to replace arbitrary values with other values.
replace() in Series and replace() in DataFrame provides an efficient yet flexible way to perform such replace- ments.
For a Series, you can replace a single value or a list of values by another value:
ser = pd.Series([4.0, 1.0, 0.0, 3.0, 2.0])
ser.replace(0,5)

You can replace a list of values by a list of other values:
ser.replace([0, 1, 2, 3, 4], [4, 3, 2, 1, 0])

You can also specify a mapping dict:
ser.replace({0: 10, 1: 100})

For a DataFrame, you can specify individual values by column:
df = pd.DataFrame({"a": [0, 1, 2, 3, 4], "b": [5, 6, 7, 8, 9]})
df.replace({"a": 0, "b": 5}, 100)

Instead of replacing with specified values, you can treat all given values as missing and interpolate over them:
ser.replace([1, 2, 3], method="pad") ## Replace non-np.nan values

2.10.11 String/regular expression replacement
---------------------------------------------
Note: Python strings prefixed with the r character such as r'hello world' are so-called “raw” strings. They have different semantics regarding backslashes than strings without this prefix. Backslashes in raw strings will be interpreted as an escaped backslash,e.g.,r'\' == '\\'.You should read about them if this is unclear - https://docs.python.org/3/reference/lexical_analysis.html#string-literals

Replace the ‘.’ with NaN (str -> str):
d = {"a": list(range(4)), "b": list("ab.."), "c": ["a", "b", np.nan, "d"]}
df = pd.DataFrame(d)
df.replace('.', np.nan)

Now do it with a regular expression that removes surrounding whitespace (regex -> regex):
df.replace(r"\s*\.\s*", np.nan, regex=True)

Replace a few different values (list -> list):
df.replace(["a", "."], ["b", np.nan])

list of regex -> list of regex:
df.replace([r"\.", r"(a)"], ["dot", r"\1stuff"], regex=True)

Only search in column 'b' (dict -> dict):
df.replace({"b": "."}, {"b": np.nan})

Same as the previous example, but use a regular expression for searching instead (dict of regex -> dict):
df.replace({"b": r"\s*\.\s*"}, {"b": np.nan}, regex=True)

You can pass nested dictionaries of regular expressions that use regex=True:
df.replace({"b": {"b": r""}}, regex=True)

Alternatively, you can pass the nested dictionary like so:
df.replace(regex={"b": {r"\s*\.\s*": np.nan}})

You can also use the group of a regular expression match when replacing (dict of regex -> dict of regex), this works for lists as well.
df.replace({"b": r"\s*(\.)\s*"}, {"b": r"\1ty"}, regex=True)

You can pass a list of regular expressions, of which those that match will be replaced with a scalar (list of regex -> regex).
df.replace([r"\s*\.\s*", r"a|b"], np.nan, regex=True)

All of the regular expression examples can also be passed with the to_replace argument as the regex argument. In this case the value argument must be passed explicitly by name or regex must be a nested dictionary. The previous example, in this case, would then be:
 df.replace(regex=[r"\s*\.\s*", r"a|b"], value=np.nan)

 This can be convenient if you do not want to pass regex=True every time you want to use a regular expression.
Note: Any where in the above replace examples that you see a regular expression a compiled regular expression is
valid as well.

2.10.12 Numeric replacement
============================
replace() is similar to fillna().
df = pd.DataFrame(np.random.randn(10, 2))
df[np.random.rand(df.shape[0]) > 0.5] = 1.5
df.replace(1.5, np.nan)

Replacing more than one value is possible by passing a list.
df00 = df.iloc[0, 0]
df.replace([1.5, df00], [np.nan, "a"])

You can also operate on the DataFrame in place:
df.replace(1.5, np.nan, inplace=True)

Missing data casting rules and indexing
---------------------------------------
While pandas supports storing arrays of integer and boolean type, these types are not capable of storing missing data. Until we can switch to using a native NA type in NumPy, we’ve established some “casting rules”. When a reindexing operation introduces missing data, the Series will be cast according to the rules introduced in the table below.

data type               Cast to
integer                 float
boolean                 object
float                   no cast
object                  no cast

For example:
s = pd.Series(np.random.randn(5), index=[0, 2, 4, 6, 7])
s > 0
(s > 0).dtype # dtype('bool')
crit = (s > 0).reindex(list(range(8)))
crit.dtype

Ordinarily NumPy will complain if you try to use an object array (even if it contains boolean values) instead of a boolean array to get or set values from an ndarray (e.g. selecting values based on some criteria). If a boolean vector contains NAs, an exception will be generated:
s = pd.Series(np.random.randn(5), index=[0, 2, 4, 6, 7])
reindexed = s.reindex(list(range(8))).fillna(0)
crit = (s > 0).reindex(list(range(8)))
reindexed[crit]
ValueError: Cannot mask with non-boolean array containing NA / NaN values

However, these can be filled in using fillna() and it will work fine:
reindexed[crit.fillna(False)]
reindexed[crit.fillna(True)]

pandas provides a nullable integer dtype, but you must explicitly request it when creating the series or column. Notice that we use a capital “I” in the dtype="Int64".
s = pd.Series([0, 1, np.nan, 3, 4], dtype="Int64")

2.10.13 Experimental NA scalar to denote missing values
-------------------------------------------------------
Warning: Experimental:the behaviour of pd.NA can still change without warning.
New in version 1.0.0.
Starting from pandas 1.0, an experimental pd.NA value (singleton) is available to represent scalar missing values. At
this moment, it is used in the nullable integer, boolean and dedicated string data types as the missing value indicator. The goal of pd.NA is provide a “missing” indicator that can be used consistently across data types (instead of np.
nan, None or pd.NaT depending on the data type).
For example, when having missing values in a Series with the nullable integer dtype, it will use pd.NA:
s = pd.Series([1, 2, None], dtype="Int64")

Currently, pandas does not yet use those data types by default (when creating a DataFrame or Series, or when reading in data), so you need to specify the dtype explicitly.

Propagation in arithmetic and comparison operations
---------------------------------------------------
n general, missing values propagate in operations involving pd.NA. When one of the operands is unknown, the outcome of the operation is also unknown.
For example, pd.NA propagates in arithmetic operations, similarly to np.nan:

np.nan + 1 # nan
pd.NA + 1 # <NA>
"a" * pd.NA # <NA>

There are a few special cases when the result is known, even when one of the operands is NA.
pd.NA ** 0
1 ** pd.NA

In equality and comparison operations, pd.NA also propagates. This deviates from the behaviour of np.nan, where comparisons with np.nan always return False.
pd.NA == 1
pd.NA == pd.NA
pd.NA < 2.5

To check if a value is equal to pd.NA, the isna() function can be used:
pd.isna(pd.NA)

An exception on this basic propagation rule are reductions (such as the mean or the minimum), where pandas defaults to skipping missing values.

Logical operations
------------------
For logical operations, pd.NA follows the rules of the three-valued logic (or Kleene logic, similarly to R, SQL and Julia). This logic means to only propagate missing values when it is logically required.
For example, for the logical “or” operation (|), if one of the operands is True, we already know the result will be True, regardless of the other value (so regardless the missing value would be True or False). In this case, pd.NA does not propagate:
True | False
True | pd.NA
pd.NA | True

NA in a boolean context
Since the actual value of an NA is unknown, it is ambiguous to convert NA to a boolean value. The following raises
an error:

Conversion
===========
If you have a DataFrame or Series using traditional types that have missing data represented using np.nan, there are convenience methods convert_dtypes() in Series and convert_dtypes() in DataFrame that can convert data to use the newer dtypes for integers, strings and booleans listed here. This is especially helpful after reading in data sets when letting the readers such as read_csv() and read_excel() infer default dtypes.
In this example, while the dtypes of all columns are changed, we show the results for the first 10 columns.
bb = pd.read_csv("data/baseball.csv", index_col="id")
bb[bb.columns[:10]].dtypes
bbn = bb.convert_dtypes()
bbn[bbn.columns[:10]].dtypes

df = DataFrame({
    'a':['abc', 'def'],
    'b':['abc10', '10def']
})
df1 = df.convert_dtypes()
print(df.dtypes)
print(df1.dtypes)

2.11 Duplicate Labels
======================
Index objects are not required to be unique; you can have duplicate row or column labels. This may be a bit confusing at first. If you’re familiar with SQL, you know that row labels are similar to a primary key on a table, and you would never want duplicates in a SQL table. But one of pandas’ roles is to clean messy, real-world data before it goes to some downstream system. And real-world data has duplicates, even in fields that are supposed to be unique.
This section describes how duplicate labels change the behavior of certain operations, and how prevent duplicates from arising during operations, or to detect them if they do.

Some pandas methods (Series.reindex() for example) just don’t work with duplicates present. The output can’t be determined, and so pandas raises.
s1 = pd.Series([0, 1, 2], index=["a", "b", "b"])
s1.reindex(["a", "b", "c"]) # ValueError: cannot reindex from a duplicate axis

Other methods, like indexing, can give very surprising results. Typically indexing with a scalar will reduce dimension- ality. Slicing a DataFrame with a scalar will return a Series. Slicing a Series with a scalar will return a scalar. But with duplicates, this isn’t the case.

df1 = pd.DataFrame([[0, 1, 2], [3, 4, 5]], columns=["A", "A", "B"])

We have duplicates in the columns. If we slice 'B', we get back a Series
df1['B']

But slicing 'A' returns a DataFrame
df1['A']

This applies to row labels as well
df2 = pd.DataFrame({"A": [0, 1, 2]}, index=["a", "a", "b"])
df2.loc["b", "A"] # a scalar
df2.loc["a", "A"] # a Series

2.11.2 Duplicate Label Detection
=================================
You can check whether an Index (storing the row or column labels) is unique with Index.is_unique:
df2.index.is_unique
df2.columns.is_unique

Note: Checking whether an index is unique is somewhat expensive for largedatasets.pandas does cache this result, so re-checking on the same index is very fast.

Index.duplicated() will return a boolean ndarray indicating whether a label is repeated.
df2.index.duplicated()
Which can be used as a boolean filter to drop duplicate rows.
df2.loc[~df2.index.duplicated(), :]

If you need additional logic to handle duplicate labels, rather than just dropping the repeats, using groupby() on the index is a common trick. For example, we’ll resolve duplicates by taking the average of all rows with the same label.
df2.groupby(level=0).mean()

2.11.3 Disallowing Duplicate Labels
------------------------------------
As noted above, handling duplicates is an important feature when reading in raw data. That said, you may want to avoid introducing duplicates as part of a data processing pipeline (from methods like pandas. concat(), rename(), etc.). Both Series and DataFrame disallow duplicate labels by calling . set_flags(allows_duplicate_labels=False). (the default is to allow them). If there are duplicate labels, an exception will be raised.
pd.Series([0, 1, 2], index=["a", "b", "b"]).set_flags(allows_duplicate_labels=False)

This applies to both row and column labels for a DataFrame
pd.DataFrame([[0, 1, 2], [3, 4, 5]], columns=["A", "B", "C"],).set_flags(allows_duplicate_labels=False)

This attribute can be checked or set with allows_duplicate_labels, which indicates whether that object can have duplicate labels.
df = pd.DataFrame({"A": [0, 1, 2, 3]}, index=["x", "y", "X", "Y"]).set_flags(allows_duplicate_labels=False)
df.flags.allows_duplicate_labels

DataFrame.set_flags() can be used to return a new DataFrame with attributes like allows_duplicate_labels set to some value
df2 = df.set_flags(allows_duplicate_labels=True)
df2.flags.allows_duplicate_labels

The new DataFrame returned is a view on the same data as the old DataFrame. Or the property can just be set directly on the same object
df2.flags.allows_duplicate_labels = False

When processing raw, messy data you might initially read in the messy data (which potentially has duplicate labels), deduplicate, and then disallow duplicates going forward, to ensure that your data pipeline doesn’t introduce duplicates.
raw = pd.read_csv("...")
deduplicated = raw.groupby(level=0).first() # remove duplicates
deduplicated.flags.allows_duplicate_labels = False # disallow going forward

This error message contains the labels that are duplicated, and the numeric positions of all the duplicates (including the “original”) in the Series or DataFrame

Duplicate Label Propagation
---------------------------
In general, disallowing duplicates is “sticky”. It’s preserved through operations.
s1 = pd.Series(0, index=["a", "b"]).set_flags(allows_duplicate_labels=False)

s1.head().rename({"a": "b"})

Warning: This is an experimental feature. Currently, many methods fail to propagate the allows_duplicate_labels value. In future versions it is expected that every method taking or returning one or more DataFrame or Series objects will propagate allows_duplicate_labels.

========================
2.12 Categorical data===
========================
This is an introduction to pandas categorical data type, including a short comparison with R’s factor.
Categoricals are a pandas data type corresponding to categorical variables in statistics. A categorical variable takes on a limited, and usually fixed, number of possible values (categories; levels in R). Examples are gender, social class, blood type, country affiliation, observation time or rating via Likert scales.
In contrast to statistical categorical variables, categorical data might have an order (e.g. ‘strongly agree’ vs ‘agree’ or ‘first observation’ vs. ‘second observation’), but numerical operations (additions, divisions, . . . ) are not possible.
All values of categorical data are either in categories or np.nan. Order is defined by the order of categories, not lexical order of the values. Internally, the data structure consists of a categories array and an integer array of codes which point to the real value in the categories array.
The categorical data type is useful in the following cases:
• A string variable consisting of only a few different values. Converting such a string variable to a categorical
variable will save some memory, see here.
• The lexical order of a variable is not the same as the logical order (“one”, “two”, “three”). By converting to a categorical and specifying an order on the categories, sorting and min/max will use the logical order instead of the lexical order, see here.
• As a signal to other Python libraries that this column should be treated as a categorical variable (e.g. to use suitable statistical methods or plot types).

2.12.1 Object creation Series creation
======================================
Series creation
---------------
Categorical Series or columns in a DataFrame can be created in several ways: By specifying dtype="category" when constructing a Series:
s = pd.Series(['a','b','c','a'], dtype='category')

By converting an existing Series or column to a category dtype:
df = pd.DataFrame({'A':['a','b','c','a']})
df["B"] = df.A.astype('category')
df.B.dtypes

By using special functions, such as cut(), which groups data into discrete bins. See the example on tiling in the docs.

pd.cut(np.array([1,2,3]), 3)
# range is 1-3 i.e 2 therefore 2/3 = 0.667 hence 1.-1.667-2.33,3.

df = pd.DataFrame({"value": np.random.randint(0, 100, 20)})
labels = ["{0} - {1}".format(i, i + 9) for i in range(0, 100, 10)]
df["group"] = pd.cut(df.value, range(0, 105, 10), right=False, labels=labels)
df

By passing a pandas.Categorical object to a Series or assigning it to a DataFrame.
raw_cat = pd.Categorical(["a", "b", "c", "a"], categories=["b", "c", "d"], ordered=False)
s = pd.Series(raw_cat)
df = pd.DataFrame({"A": ["a", "b", "c", "a"]})
df['B'] = raw_cat
df

Categorical data has a specific category dtype:
df.dtypes

DataFrame creation
-------------------
Similar to the previous section where a single column was converted to categorical, all columns in a DataFrame can be batch converted to categorical either during or after construction.
This can be done during construction by specifying dtype="category" in the DataFrame constructor:
df = pd.DataFrame({"A": list("abca"), "B": list("bccd")}, dtype="category")

Note that the categories present in each column differ; the conversion is done column by column, so only labels present in a given column are categories:

Analogously, all columns in an existing DataFrame can be batch converted using DataFrame.astype():
df = pd.DataFrame({"A": list("abca"), "B": list("bccd")})
df_cat = df.astype("category")
This conversion is likewise done column by column:
df_cat["A"]
df_cat["B"]

Controlling behavior
--------------------
In the examples above where we passed dtype='category', we used the default behavior:
1. Categories are inferred from the data.
2. Categories are unordered.
To control those behaviors, instead of passing 'category', use an instance of CategoricalDtype.

from pandas.api.types import CategoricalDtype
s = pd.Series(["a", "b", "c", "a"])
cat_type = CategoricalDtype(categories=["b", "c", "d"], ordered=True)
s_cat = s.astype(cat_type)

Similarly, a CategoricalDtype can be used with a DataFrame to ensure that categories are consistent among all columns.
from pandas.api.types import CategoricalDtype
df = pd.DataFrame({"A": list("abca"), "B": list("bccd")})
cat_type = CategoricalDtype(categories=list("abcd"), ordered=True)
df_cat = df.astype(cat_type)
df_cat["A"]
 df_cat["B"]

Note: To perform table-wise conversion, where all labels in the entire DataFrame are used as categories for each column,the categories parameter can be determined programmatically by categories = pd.unique(df. to_numpy().ravel()).

If you already have codes and categories, you can use the from_codes() constructor to save the factorize step during normal constructor mode:
splitter = np.random.choice([0, 1], 5, p=[0.5, 0.5])
splitter = np.random.choice([0, 1], 5, p=[0.4, 0.6])
s = pd.Series(pd.Categorical.from_codes(splitter,
                            categories=["train", "test"]))

Regaining original data
------------------------
To get back to the original Series or NumPy array, use Series.astype(original_dtype) or np. asarray(categorical):
s = pd.Series(["a", "b", "c", "a"])
s2 = s.astype("category")
s2.astype(str)
np.asarray(s2)

Note: In contrast to R’s factor function, categorical data is not converting input values to strings; categories will end up the same data type as the original values.
Note: In contrast to R’s factor function, there is currently no way to assign/change labels at creation time. Use categories to change the categories after creation time.

2.12.2 CategoricalDtype
-----------------------
2.12.2 CategoricalDtype
A categorical’s type is fully described by
1. categories: a sequence of unique values and no missing values
2. ordered: a boolean
This information can be stored in a CategoricalDtype. The categories argument is optional, which implies that the actual categories should be inferred from whatever is present in the data when the pandas.Categorical is created. The categories are assumed to be unordered by default.

from pandas.api.types import CategoricalDtype
CategoricalDtype(['a','b','c']) # CategoricalDtype(categories=['a', 'b', 'c'], ordered=False)
CategoricalDtype(["a", "b", "c"], ordered=True) # CategoricalDtype(categories=['a', 'b', 'c'], ordered=True)
CategoricalDtype() # CategoricalDtype(categories=None, ordered=False)

A CategoricalDtype can be used in any place pandas expects a dtype. For example pandas.read_csv(), pandas.DataFrame.astype(), or in the Series constructor.

Note: As a convenience,you can use the string 'category' inplace of a CategoricalDtype when you want the default behavior of the categories being unordered, and equal to the set values present in the array. In other words, dtype='category' is equivalent to dtype=CategoricalDtype().

Equality semantics
-------------------
Two instances of CategoricalDtype compare equal whenever they have the same categories and order. When comparing two unordered categoricals, the order of the categories is not considered.
CategoricalDtype('a b c'.split(), ordered=False)
# Equal, since order is not considered when ordered=False
CategoricalDtype('b a c'.split(), ordered=False)

# Unequal, since the second CategoricalDtype is ordered
CategoricalDtype('b a c'.split(), ordered=True)

All instances of CategoricalDtype compare equal to the string 'category'.
c1 == "category" # True

Warning: Since dtype='category' is essentially CategoricalDtype(None, False), and since all instances CategoricalDtype compare equal to 'category', all instances of CategoricalDtype compare equal to aCategoricalDtype(None, False),regard less of categories ordered.

2.12.3 Description
-------------------
Using describe() on categorical data will produce similar output to a Series or DataFrame of type string.
cat = pd.Categorical(['a', 'c', 'c', np.nan], categories=['b','a', 'c'])
df = pd.DataFrame({
    'cat':cat,
    's':['a','c','c',np.nan]
})
df.describe()

2.12.4 Working with categories
-------------------------------
Categorical data has a categories and a ordered property, which list their possible values and whether the ordering matters or not. These properties are exposed as s.cat.categories and s.cat.ordered. If you don’t manually specify categories and ordering, they are inferred from the passed arguments.
c=CategoricalDtype(['a','b','c'], True)
s = pd.Series(['a','b','c'], dtype=c)
s.cat.categories # Index(['a', 'b', 'c'], dtype='object')
s.cat.ordered # False
c= None
s

It’s also possible to pass in the categories in a specific order:
s = pd.Series(pd.Categorical(['a','b','c'], categories=['b','c','a']))
s.cat.categories
s.cat.ordered

Note: New categorical data are not automatically ordered. You must explicitly pass ordered=True to indicate an ordered Categorical.
Note: The result of unique() is not always the same as Series.cat.categories, because Series. unique() has a couple of guarantees, namely that it returns categories in the order of appearance, and it only includes values that are actually present.

s = pd.Series(list("babc")).astype(CategoricalDtype(list("abcd")))
s.cat.categories # Index(['a', 'b', 'c', 'd'], dtype='object')
s.unique() # ['b', 'a', 'c']

Renaming categories
--------------------
Renaming categories is done by assigning new values to the Series.cat.categories property or by using the rename_categories() method:
s=pd.Series(['a','b','c','a'], dtype='category')
s.cat.categories = [f'Group {i}'for i in s.cat.categories]
s = s.cat.rename_categories(['Ramesh', 'Suresh', 'Chandu'])
# You can also pass a dict-like object to map the renaming
s = s.cat.rename_categories({'Ramesh':'R', 'Suresh':'S', 'Chandu':'C'})

Note: In contrast to R’s factor, categorical data can have categories of other types than string.
Note: Be aware that assigning new categories is an inplace operation, while most other operations under Series. cat per default return a new Series of dtype category.

Categories must be unique or a ValueError is raised:
try:
    s.cat.categories = [1,1,1]
except ValueError as e:
    print('ValueError', str(e)) # ValueError Categorical categories must be unique

Categories must also not be NaN or a ValueError is raised:
try:
    s.cat.categories = [1,1,np.nan]
except ValueError as e:
    print('ValueError', str(e)) # ValueError Categorical categories cannot be null

Appending new categories
-------------------------
Appending categories can be done by using the add_categories() method:
s = s.cat.add_categories(5)
s.cat.categories # Index(['R', 'S', 'C', 4, 5], dtype='object')

Removing categories
-------------------
Removing categories can be done by using the remove_categories() method. Values which are removed are replaced by np.nan.:
s = s.cat.remove_categories([4])

Removing unused categories
---------------------------
Removing unused categories can also be done:
s = pd.Series(pd.Categorical(['a','b','a'], categories=['a','b','c','d']))
s.cat.remove_unused_categories()

Setting categories
-------------------
If you want to do remove and add new categories in one step (which has some speed advantage), or simply set the categories to a predefined scale, use set_categories().
s = pd.Series(["one", "two", "four", "-"], dtype="category")
s = s.cat.set_categories(["one", "two", "three", "four"])
s

Note: Be aware that Categorical.set_categories() cannot know whether some category is omitted in- tentionally or because it is misspelled or (under Python3) due to a type difference (e.g., NumPy S1 dtype and Python strings). This can result in surprising behaviour!

2.12.5 Sorting and order
========================
If categorical data is ordered (s.cat.ordered == True), then the order of the categories has a meaning and certain operations are possible. If the categorical is unordered, .min()/.max() will raise a TypeError.
s = pd.Series(pd.Categorical(["a", "b", "c", "a"], ordered=False))
s.sort_values(inplace=True)
s.min() # TypeError: Categorical is not ordered for operation min

s = pd.Series(pd.Categorical(["a", "b", "c", "a"], ordered=True))
s.sort_values(inplace=True)
s.min() # 'a'

You can set categorical data to be ordered by using as_ordered() or unordered by using as_unordered(). These will by default return a new object.
s.cat.as_ordered()
s.cat.as_unordered()

Sorting will use the order defined by categories, not any lexical order present on the data type. This is even true for strings and numeric data:
s = pd.Series([1, 2, 3, 1], dtype="category")
s = s.cat.set_categories([2, 3, 1], ordered=True)
s.sort_values(inplace=True)

Reordering
----------
Reordering the categories is possible via the Categorical.reorder_categories() and the Categorical.set_categories() methods. For Categorical.reorder_categories(), all old categories must be included in the new categories and no new categories are allowed. This will necessarily make the sort order the same as the categories order.

s = pd.Series([1,2,3,1], dtype='category')
s = s.cat.reorder_categories([2,3,1], ordered=True)
s.sort_values(inplace=True)
s.min(), s.max()

Note: Note the difference between assigning new categories and reordering the categories:the first renames categories and therefore the individual values in the Series, but if the first position was sorted last, the renamed value will still be sorted last. Reordering means that the way values are sorted is different afterwards, but not that individual values in the Series are changed.

Note: If the Categorical is not ordered, Series.min() and Series.max() will raise TypeError. Nu- meric operations like +, -, *, / and operations based on them (e.g. Series.median(), which would need to compute the mean between two values if the length of an array is even) do not work and raise a TypeError.

Multi column sorting
--------------------
A categorical dtyped column will participate in a multi-column sort in a similar manner to other columns. The ordering of the categorical is determined by the categories of that column.
df = pd.DataFrame({
    "A":pd.Categorical(list('bbeebbaa'),
                       categories=['e','a','b'], ordered=True),
    "B": [1, 2, 1, 2, 2, 1, 2, 1],
})
df.sort_values(["A", "B"])

Reordering the categories changes a future sort.
df['A'] = df.A.cat.reorder_categories(["a", "b", "e"])
df.sort_values(["A", "B"])

2.12.6 Comparisons
-------------------
Comparing categorical data with other objects is possible in three cases:
• Comparingequality(==and!=)toalist-likeobject(list,Series,array,...)ofthesamelengthasthecategorical data.
• All comparisons (==, !=, >, >=, <, and <=) of categorical data to another categorical Series, when ordered==True and the categories are the same.
• All comparisons of a categorical data to a scalar.

All other comparisons, especially “non-equality” comparisons of two categoricals with different categories or a cate- gorical with any list-like object, will raise a TypeError.

Note: Any “non-equality” comparisons of categorical data with a Series, np.array, list or categorical data with different categories or ordering will raise a TypeError because custom categories ordering could be interpreted in two ways: one with taking into account the ordering and one without.

cat = pd.Series([1, 2, 3]).astype(CategoricalDtype([3, 2, 1], ordered=True))
cat_base = pd.Series([2, 2, 2]).astype(CategoricalDtype([3, 2, 1],ordered=True))
cat_base2 = pd.Series([2, 2, 2]).astype(CategoricalDtype(ordered=True))

Comparing to a categorical with the same categories and ordering or to a scalar works:
cat > cat_base
cat > 2

Equality comparisons work with any list-like object of same length and scalars:
cat == cat_base

cat == np.array([1, 2, 3])
cat == 2

This doesn’t work because the categories are not the same:
cat > cat_base2 # TypeError: Categoricals can only be compared if 'categories' are the same.

If you want to do a “non-equality” comparison of a categorical series with a list-like object which is not categorical data, you need to be explicit and convert the categorical data back to the original values:
base = np.array([1, 2, 3])
cat > base # TypeError: Cannot compare a Categorical for op __gt__ with type

np.asarray(cat) > base

When you compare two unordered categoricals with the same categories, the order is not considered:
c1 = pd.Categorical(["a", "b"], categories=["a", "b"], ordered=False)
c2 = pd.Categorical(["a", "b"], categories=["b", "a"], ordered=False)
c1 == c2

2.12.7 Operations
===================
Apart from Series.min(), Series.max() and Series.mode(), the following operations are possible with categorical data:
Series methods like Series.value_counts() will use all categories, even if some categories are not present in the data:
s = pd.Series(pd.Categorical(["a", "b", "c", "c"], categories=["c", "a", "b", "d"]))
s.value_counts()

DataFrame methods like DataFrame.sum() also show “unused” categories.
columns = pd.Categorical(["One", "One", "Two"], categories=["One", "Two", "Three"], ordered=True)
df = pd.DataFrame(data=[[1, 2, 3], [4, 5, 6]],columns=pd.MultiIndex.from_arrays([["A", "B", "B"], columns]),)
df.sum(axis=1, level=1)

Groupby will also show “unused” categories:
cats = pd.Categorical(
.....: ["a", "b", "b", "b", "c", "c", "c"], categories=["a", "b", "c", "d"] .....: )
.....:
In [137]: df = pd.DataFrame({"cats": cats, "values": [1, 2, 2, 2, 3, 4, 5]})
In [138]: df.groupby("cats").mean() Out[138]:
values cats
a        1.0
b        2.0
c        4.0
d        NaN
In [139]: cats2 = pd.Categorical(["a", "a", "b", "b"], categories=["a", "b", "c"])

2.12.8 Data munging
--------------------
The optimized pandas data access methods .loc, .iloc, .at, and .iat, work as normal. The only difference is the return type (for getting) and that only values already in categories can be assigned.
Getting
If the slicing operation returns either a DataFrame or a column of type Series, the category dtype is preserved.
idx = pd.Index(["h", "i", "j", "k", "l", "m", "n"])
In [146]: cats = pd.Series(["a", "b", "b", "b", "c", "c", "c"], dtype="category",
˓→index=idx)
In [147]: values = [1, 2, 2, 2, 3, 4, 5]
In [148]: df = pd.DataFrame({"cats": cats, "values": values}, index=idx)
df.iloc[2:4, :]
df.loc["h":"j", "cats"]
df[df["cats"] == "b"]
An example where the category type is not preserved is if you take one single row: the resulting Series is of dtype object:
df.loc["h", :]
Returning a single item from categorical data will also return the value, not a categorical of length “1”.
df.iat[0, 0]
To get a single value Series of type category, you pass in a list with a single value:
df.loc[["h"], "cats"]

String and datetime accessors
The accessors .dt and .str will work if the s.cat.categories are of an appropriate type:
str_s = pd.Series(list("aabb"))
str_cat = str_s.astype("category")
str_cat.str.contains("a")

date_s = pd.Series(pd.date_range("1/1/2015", periods=5))
date_cat = date_s.astype("category")
date_cat.dt.day

Note: The returned Series (or DataFrame) is of the same type as if you used the .str.<method> / .dt. <method> on a Series of that type (and not of type category!).

Merging / concatenation
------------------------
By default, combining Series or DataFrames which contain the same categories results in category dtype, otherwise results will depend on the dtype of the underlying categories. Merges that result in non-categorical dtypes will likely have higher memory usage. Use .astype or union_categoricals to ensure category results.

If both df/series are category type then result is category as object or other type.
s1 = pd.Series(["a", "b"], dtype="category")
s2 = pd.Series(["a", "b", "a"], dtype="category")
pd.concat([s1, s2])

s3 = pd.Series(["b", "c"], dtype="category")
pd.concat([s1, s3])

# Output dtype is inferred based on categories values
int_cats = pd.Series([1, 2], dtype="category")
float_cats = pd.Series([3.0, 4.0], dtype="category")
pd.concat([int_cats, float_cats])

pd.concat([s1, s3]).astype("category")

The following table summarizes the results of merging Categoricals:
arg1                    arg2                identical       result
category                category            True            category
category (object)       category (object)   False           object (dtype is inferred)
category (int)          category (float)    False           float (dtype is inferred)

Unioning
If you want to combine categoricals that do not necessarily have the same categories, the union_categoricals() function will combine a list-like of categoricals. The new categories will be the union of the categories being combined.
from pandas.api.types import union_categoricals
a = pd.Categorical(["b", "c"])
b = pd.Categorical(["a", "b"])
union_categoricals([a, b])

By default, the resulting categories will be ordered as they appear in the data. If you want the categories to be lexsorted, use sort_categories=True argument.
union_categoricals([a, b], sort_categories=True)

union_categoricals also works with the “easy” case of combining two categoricals of the same categories and order information (e.g. what you could also append for).
a = pd.Categorical(["a", "b"], ordered=True)
b = pd.Categorical(["a", "b", "a"], ordered=True)
union_categoricals([a, b])

The below raises TypeError because the categories are ordered and not identical.

2.12.10 Missing data
pandas: powerful Python data analysis toolkit, Release 1.2.3
 pandas primarily uses the value np.nan to represent missing data. It is by default not included in computations. See the Missing Data section.
Missing values should not be included in the Categorical’s categories, only in the values. Instead, it is under- stood that NaN is different, and is always a possibility. When working with the Categorical’s codes, missing values will always have a code of -1.
s = pd.Series(["a", "b", np.nan, "a"], dtype="category")
s.cat.codes

Methods for working with missing data, e.g. isna(), fillna(), dropna(), all work normally:
s = pd.Series(["a", "b", np.nan], dtype="category")
pd.isna(s)
s.fillna("a")

2.12.12 Gotchas Memory usage
-----------------------------
The memory usage of a Categorical is proportional to the number of categories plus the length of the data. In contrast, an object dtype is a constant times the length of the data.
s = pd.Series(["foo", "bar"] * 2)
s.nbytes # 32
s.astype('category').nbytes # 20

Note: If the number of categories approaches the length of the data,the Categorical will use nearly the same or more memory than an equivalent object dtype representation.
s = pd.Series(["foo%04d" %i for i in range(2)])
s.nbytes # 16
s.astype('category').nbytes # 18

Categorical is not a numpy array
Currently, categorical data and the underlying Categorical is implemented as a Python object and not as a low-
level NumPy array dtype. This leads to some problems. NumPy itself doesn’t know about the new dtype:
np.dtype("category") #TypeError: data type 'category' not understood
dtype = pd.Categorical(["a"]).dtype # TypeError: Cannot interpret 'CategoricalDtype(categories=['a'], ordered=False)' as a ˓→data type

To check if a Series contains Categorical data,use hasattr(s, 'cat'):

hasattr(pd.Series(["a"], dtype="category"), "cat")
hasattr(pd.Series(["a"]), "cat")

Using NumPy functions on a Series of type category should not work as Categoricals are not numeric data (even in the case that .categories is numeric).
s = pd.Series(pd.Categorical([1, 2, 3, 4]))
np.sum(s) # TypeError: 'Categorical' does not implement reduction 'sum'
behavior is same as symbol in kdb

dtype in apply
--------------
pandas currently does not preserve the dtype in apply functions: If you apply along rows you get a Series of object dtype (same as getting a row -> getting one element will return a basic type) and applying along columns will also convert to object. NaN values are unaffected. You can use fillna to handle missing values before applying a function.
df = pd.DataFrame({
    "a": [1, 2, 3, 4],
    "b": ["a", "b", "c", "d"],
    "cats": [1, 2, 3, 2],
})
df.cats = df.cats.astype('category')
df.apply(lambda row: type(row["cats"]), axis=1)
df.apply(lambda col: col.dtype, axis=0)

Categorical index
CategoricalIndex is a type of index that is useful for supporting indexing with duplicates. This is a container around a Categorical and allows efficient indexing and storage of an index with a large number of duplicated elements. See the advanced indexing docs for a more detailed explanation.
Setting the index will create a CategoricalIndex:

cats = pd.Categorical([1, 2, 3, 4], categories=[4, 2, 3, 1])
strings = ["a", "b", "c", "d"]
values = [4, 2, 3, 1]
df = pd.DataFrame({"strings": strings, "values": values}, index=cats)
# This now sorts by the categories order
df.sort_index() Out[263]:

Side effects
Constructing a Series from a Categorical will not copy the input Categorical. This means that changes to the Series will in most cases change the original Categorical:
cat = pd.Categorical([1, 2, 3, 10], categories=[1, 2, 3, 4, 10])
s = pd.Series(cat, name="cat")
s.iloc[0:2] = 10
cat

df = pd.DataFrame(s)
df["cat"].cat.categories = [1, 2, 3, 4, 5]
Use copy=True to prevent such a behaviour or simply don’t reuse Categoricals:

cat = pd.Categorical([1, 2, 3, 10], categories=[1, 2, 3, 4, 10])
s = pd.Series(cat, name="cat", copy=True)
s.iloc[0:2] = 10

Note: This also happens in some cases when you supply a NumPy array instead of a Categorical: using an int array (e.g. np.array([1,2,3,4])) will exhibit the same behavior, while using a string array (e.g. np.array([ "a","b","c","a"])) will not.


Visualization
==============
We use the standard convention for referencing the matplotlib API:
In [1]: import matplotlib.pyplot as plt
In [2]: plt.close("all")

We provide the basics in pandas to easily create decent looking plots. See the ecosystem section(https://pandas.pydata.org/pandas-docs/stable/ecosystem.html#ecosystem-visualization) for visualization libraries that go beyond the basics documented here.

Basic plotting: plot
--------------------
We will demonstrate the basics, see the cookbook for some advanced strategies.
The plot method on Series and DataFrame is just a simple wrapper around plt.plot():
ts = pd.Series(np.random.randn(1000), index=pd.date_range('2010.01.01', periods=1000))
ts = ts.cumsum()
ts.plot()

If the index consists of dates, it calls gcf().autofmt_xdate() to try to format the x-axis nicely as per above.
On DataFrame, plot() is a convenience to plot all of the columns with labels:
df = DataFrame(np.random.randn(1000, 4), index=pd.date_range(
                '2010.01.01', periods=1000))
df = df.cumsum()
df.plot()

You can plot one column versus another using the x and y keywords in plot():
df = pd.DataFrame(np.random.randn(1000,2), columns=['A','B']).cumsum()
df['C'] = pd.Series(range(len(df)))
df.plot(x='C', y=['A', 'B'])

Other plots
-----------
Plotting methods allow for a handful of plot styles other than the default line plot. These methods can be provided as the kind keyword argument to plot(), and include:
‘bar’ or ‘barh’ for bar plots
‘hist’ for histogram
‘box’ for boxplot
‘kde’ or ‘density’ for density plots
‘area’ for area plots
‘scatter’ for scatter plots
‘hexbin’ for hexagonal bin plots
‘pie’ for pie plots

plt.figure()
df.iloc[:5].plot(kind='hist')

You can also create these other plots using the methods DataFrame.plot.<kind> instead of providing the kind keyword argument. This makes it easier to discover plot methods and the specific arguments they use:
df.iloc[:5].plot.barh()

In addition to these kind s, there are the DataFrame.hist(), and DataFrame.boxplot() methods, which use a separate interface.
Finally, there are several plotting functions in pandas.plotting that take a Series or DataFrame as an argument. These include:

Scatter Matrix
Andrews Curves
Parallel Coordinates
Lag Plot
Autocorrelation Plot
Bootstrap Plot
RadViz
Plots may also be adorned with errorbars or tables.

Bar plots - Categorical data
-----------------------------
For labeled, non-time series data, you may wish to produce a bar plot:
plt.figure()
df.iloc[5].plot.bar()
plt.axhline(0, color='k')

Calling a DataFrame’s plot.bar() method produces a multiple bar plot:
df2 = DataFrame(np.random.randn(5, 4), columns=list('abcd'))
df2.plot.bar()

To get horizontal bar plots, use the barh method:
df2 = DataFrame(np.random.randn(5, 4), columns=list('abcd'))
df2.plot.barh()
df2.plot.barh(stacked=True)

Histograms
-----------
Histograms can be drawn by using the DataFrame.plot.hist() and Series.plot.hist() methods.
df4 = pd.DataFrame(
   {
         "a": np.random.randn(1000) + 1,
         "b": np.random.randn(1000),
         "c": np.random.randn(1000) - 1,
     },
     columns=["a", "b", "c"],
 )
df4.plot.hist(alpha=.7)

A histogram can be stacked using stacked=True. Bin size can be changed using the bins keyword.
df4.plot.hist(alpha=.7, stacked=True, bins=20)

You can pass other keywords supported by matplotlib hist. For example, horizontal and cumulative histograms can be drawn by orientation='horizontal' and cumulative=True.
df4["a"].plot.hist(orientation="horizontal", cumulative=True);

The existing interface DataFrame.hist to plot histogram still can be used.
plt.figure();
df.a.diff().hist()

DataFrame.hist() plots the histograms of the columns on multiple subplots:
plt.figure();
df.diff().hist(color='k', alpha=0.5, bins=50)

The by keyword can be specified to plot grouped histograms:
data = pd.Series(np.random.randn(1000))
data.hist(by=np.random.randint(0,4,1000), figsize=(10,6))

rolling dice hist:

plt.figure()
df = pd.DataFrame({
    'a':pd.Series(np.random.randint(1,7,6000))
})
df['b'] = df['a'] + np.random.randint(1, 7, 6000)
df.plot.hist()

Box plots
==========
Boxplot can be drawn calling Series.plot.box() and DataFrame.plot.box(), or DataFrame.boxplot() to visualize the distribution of values within each column.

For instance, here is a boxplot representing five trials of 10 observations of a uniform random variable on [0,1).
df = pd.DataFrame(np.random.randn(10, 5), columns=list('ABCDE'))
df.boxplot()
df.plot.box()

Boxplot can be colorized by passing color keyword. You can pass a dict whose keys are boxes, whiskers, medians and caps. If some keys are missing in the dict, default colors are used for the corresponding artists. Also, boxplot has sym keyword to specify fliers style.
When you pass other type of arguments via color keyword, it will be directly passed to matplotlib for all the boxes, whiskers, medians and caps colorization.
The colors are applied to every boxes to be drawn. If you want more complicated colorization, you can get each drawn artists by passing return_type.

color = {
    'caps' : 'gray',
    'whiskers' : 'red',
    'boxes' : 'green',
    'medians' : 'DarkBlue'
}
df.plot.box(color=color, sym="r+")

Also, you can pass other keywords supported by matplotlib boxplot. For example, horizontal and custom-positioned boxplot can be drawn by vert=False and positions keywords.
df.plot.box(vert=False, positions=[1, 4, 5, 6, 9])

See the boxplot method and the matplotlib boxplot documentation for more.
The existing interface DataFrame.boxplot to plot boxplot still can be used.
df = pd.DataFrame(np.random.rand(10, 5))
bp = df.boxplot()

You can create a stratified boxplot using the by keyword argument to create groupings. For instance,
df = pd.DataFrame(np.random.rand(10, 2), columns=["Col1", "Col2"])
df["X"] = pd.Series(["A", "A", "A", "A", "A", "B", "B", "B", "B", "B"])
bp = df.boxplot(by='X')

You can also pass a subset of columns to plot, as well as group by multiple columns:
In [49]: df = pd.DataFrame(np.random.rand(10, 3), columns=["Col1", "Col2", "Col3"])
In [50]: df["X"] = pd.Series(["A", "A", "A", "A", "A", "B", "B", "B", "B", "B"])
In [51]: df["Y"] = pd.Series(["A", "B", "A", "B", "A", "B", "A", "B", "A", "B"])
In [52]: plt.figure();
In [53]: bp = df.boxplot(column=["Col1", "Col2"], by=["X", "Y"])

In boxplot, the return type can be controlled by the return_type, keyword. The valid choices are {"axes", "dict", "both", None}. Faceting, created by DataFrame.boxplot with the by keyword, will affect the output type as well:

return_type             Faceted         Output type
None                    No              axes
None                    Yes             2-D ndarray of axes
'axes'                  No              axes
'axes'                  Yes             Series of axes
'dict'                  No              dict of artists
'dict'                  Yes             Series of dicts of artists
'both'                  No              namedtuple
'both'                  Yes             Series of namedtuples

Groupby.boxplot always returns a Series of return_type.
In [54]: np.random.seed(1234)
In [55]: df_box = pd.DataFrame(np.random.randn(50, 2))
In [56]: df_box["g"] = np.random.choice(["A", "B"], size=50)
In [57]: df_box.loc[df_box["g"] == "B", 1] += 3
In [58]: bp = df_box.boxplot(by="g")

np.random.seed(1234)
df_box = pd.DataFrame(np.random.rand(50, 2), columns=['col1', 'col2'])
df_box["g"] = np.random.choice(["A", "B"], size=50)
df_box.loc[df_box["g"] == "B", "col1"] += 3
bp = df_box.boxplot(by="g")

The subplots above are split by the numeric columns first, then the value of the g column. Below the subplots are first split by the value of g, then by the numeric columns.
bp = df_box.groupby("g").boxplot()

Area plot
=========
You can create area plots with Series.plot.area() and DataFrame.plot.area(). Area plots are stacked by default. To produce stacked area plot, each column must be either all positive or all negative values.
When input data contains NaN, it will be automatically filled by 0. If you want to drop or fill by different values, use dataframe.dropna() or dataframe.fillna() before calling plot.
df = pd.DataFrame(np.random.rand(10, 4), columns=["a", "b", "c", "d"])
df.plot.area()

To produce an unstacked plot, pass stacked=False. Alpha value is set to 0.5 unless otherwise specified:
df.plot.area(stacked=False)

Scatter plot
=============
Scatter plot can be drawn by using the DataFrame.plot.scatter() method. Scatter plot requires numeric columns for the x and y axes. These can be specified by the x and y keywords.
df = pd.DataFrame(np.random.rand(50, 4), columns=["a", "b", "c", "d"])
df.plot.scatter(x="a", y="b");

To plot multiple column groups in a single axes, repeat plot method specifying target ax. It is recommended to specify color and label keywords to distinguish each groups.
ax = df.plot.scatter(x="a", y="b", color="DarkBlue", label="Group 1")
df.plot.scatter(x="c", y="d", color="DarkGreen", label="Group 2", ax=ax);

The keyword c may be given as the name of a column to provide colors for each point:
 df.plot.scatter(x="a", y="b", c="c", s=50);
You can pass other keywords supported by matplotlib scatter. The example below shows a bubble chart using a column of the DataFrame as the bubble size.
df.plot.scatter(x="a", y="b", s=df["c"] * 200);

Pie plot
==========
You can create a pie plot with DataFrame.plot.pie() or Series.plot.pie(). If your data includes any NaN, they will be automatically filled with 0. A ValueError will be raised if there are any negative values in your data.
series = pd.Series(3 * np.random.rand(4), index=["a", "b", "c", "d"], name="series")
series.plot.pie(figsize=(6, 6));

For pie plots it’s best to use square figures, i.e. a figure aspect ratio 1. You can create the figure with equal width and height, or force the aspect ratio to be equal after plotting by calling ax.set_aspect('equal') on the returned axes object.
Note that pie plot with DataFrame requires that you either specify a target column by the y argument or subplots=True. When y is specified, pie plot of selected column will be drawn. If subplots=True is specified, pie plots for each column are drawn as subplots. A legend will be drawn in each pie plots by default; specify legend=False to hide it.
df = pd.DataFrame(3 * np.random.rand(4, 2),
                  index=["a", "b", "c", "d"], columns=["x", "y"])
df.plot.pie(subplots=True, figsize=(8, 4));

You can use the labels and colors keywords to specify the labels and colors of each wedge.
Most pandas plots use the label and color arguments (note the lack of “s” on those). To be consistent with matplotlib.pyplot.pie() you must use labels and colors.

If you want to hide wedge labels, specify labels=None. If fontsize is specified, the value will be applied to wedge labels. Also, other keywords supported by matplotlib.pyplot.pie() can be used.
series.plot.pie(
   labels=["AA", "BB", "CC", "DD"],
   colors=["r", "g", "b", "c"],
   autopct="%.2f",
   fontsize=20,
   figsize=(6, 6),
   );

If you pass values whose sum total is less than 1.0, matplotlib draws a semicircle.
series = pd.Series([0.1] * 4, index=["a", "b", "c", "d"], name="series2")
series.plot.pie(figsize=(6, 6))

Plotting with missing data
pandas tries to be pragmatic about plotting DataFrames or Series that contain missing data. Missing values are dropped, left out, or filled depending on the plot type.

Plot Type               NaN Handling
Line                    Leave gaps at NaNs
Line (stacked)          Fill 0’s
Bar                     Fill 0’s
Scatter                 Drop NaNs
Histogram               Drop NaNs (column-wise)
Box                     Drop NaNs (column-wise)
Area                    Fill 0’s
KDE                     Drop NaNs (column-wise)
Hexbin                  Drop NaNs
Pie                     Fill 0’s
If any of these defaults are not what you want, or if you want to be explicit about how missing values are handled, consider using fillna() or dropna() before plotting.

Setting the plot style
=======================
From version 1.5 and up, matplotlib offers a range of pre-configured plotting styles. Setting the style can be used to easily give plots the general look that you want. Setting the style is as easy as calling matplotlib.style.use(my_plot_style) before creating your plot. For example you could write matplotlib.style.use('ggplot') for ggplot-style plots.

You can see the various available style names at matplotlib.style.available and it’s very easy to try them out
General plot style arguments
Most plotting methods have a set of keyword arguments that control the layout and formatting of the returned plot:
plt.figure();
ts.plot(style="k--", label="Series");

For each kind of plot (e.g. line, bar, scatter) any additional arguments keywords are passed along to the corresponding matplotlib function (ax.plot(), ax.bar(), ax.scatter()). These can be used to control additional styling, beyond what pandas provides

Controlling the legend
-----------------------
You may set the legend argument to False to hide the legend, which is shown by default.

=============================
Nullable integer data type===
=============================
IntegerArray is currently experimental. Its API or implementation may change without warning.
In Working with missing data, we saw that pandas primarily uses NaN to represent missing data. Because NaN is a float, this forces an array of integers with any missing values to become floating point. In some cases, this may not matter much. But if your integer column is, say, an identifier, casting to float can be problematic. Some integers cannot even be represented as floating point numbers.

Construction
pandas can represent integer data with possibly missing values using arrays.IntegerArray. This is an extension types implemented within pandas.
arr = pd.array([1,2,None], dtype='Int64')

Or the string alias "Int64" (note the capital "I", to differentiate from NumPy’s 'int64' dtype
arr = pd.array([1,2,np.nan], dtype='Int64')

All NA-like values are replaced with pandas.NA.
pd.array([1, 2, np.nan, None, pd.NA], dtype="Int64")

This array can be stored in a DataFrame or Series like any NumPy array.
pd.Series(arr)

You can also pass the list-like object to the Series constructor with the dtype.
Warning
Currently pandas.array() and pandas.Series() use different rules for dtype inference. pandas.array() will infer a nullable- integer dtype
In [6]: pd.array([1, None])
Out[6]:
<IntegerArray>
[1, <NA>]
Length: 2, dtype: Int64
In [7]: pd.array([1, 2])
Out[7]:
<IntegerArray>
[1, 2]
Length: 2, dtype: Int64

For backwards-compatibility, Series infers these as either integer or float dtype
In [8]: pd.Series([1, None])
Out[8]:
0    1.0
1    NaN
dtype: float64
In [9]: pd.Series([1, 2])
Out[9]:
0    1
1    2
dtype: int64

We recommend explicitly providing the dtype to avoid confusion.
In [10]: pd.array([1, None], dtype="Int64")
Out[10]:
<IntegerArray>
[1, <NA>]
Length: 2, dtype: Int64
In [11]: pd.Series([1, None], dtype="Int64")
Out[11]:
0       1
1    <NA>
dtype: Int64
In the future, we may provide an option for Series to infer a nullable-integer dtype.

Operations
-----------
Operations involving an integer array will behave similar to NumPy arrays. Missing values will be propagated, and the data will be coerced to another dtype if needed.
s = pd.Series([1, 2, None], dtype="Int64")

# arithmetic
s+1

# comparison
s==1

# indexing
s.iloc[1:3]

# operate with other dtypes
s + s.iloc[1:3].astype('Int8')

# coerce when needed
s+.001

These dtypes can operate as part of DataFrame.
df = pd.DataFrame({"A": s, "B": [1, 1, 3], "C": list("aab")})
df.dtypes

These dtypes can be merged & reshaped & casted.
pd.concat([df[["A"]], df[["B", "C"]]], axis=1).dtypes
df["A"].astype(float)

Reduction and groupby operations such as ‘sum’ work as well.
df.sum()
df.groupby("B").A.sum()

arrays.IntegerArray uses pandas.NA as its scalar missing value. Slicing a single element that’s missing will return pandas.NA
a = pd.array([1, None], dtype="Int64")

===========================
Nullable Boolean data type=
===========================
Indexing with NA values
-----------------------
pandas allows indexing with NA values in a boolean array, which are treated as False.

s = pd.Series([1, 2, 3])
mask = pd.array([True, False, pd.NA], dtype="boolean")
s[mask]

If you would prefer to keep the NA values you can manually fill them with fillna(True).
s[mask.fillna(True)]

Kleene logical operations
--------------------------
arrays.BooleanArray implements Kleene Logic (sometimes called three-value logic) for logical operations like & (and), | (or) and ^ (exclusive-or).

This table demonstrates the results for every combination. These operations are symmetrical, so flipping the left- and right-hand side makes no difference in the result.

Expression          Result
True & True         True
True & False        False
True & NA           NA
False & False       False
False & NA          False
NA & NA             NA
True | True         True
True | False        True
True | NA           True
False | False       False
False | NA          NA
NA | NA             NA
True ^ True         False
True ^ False        True
True ^ NA           NA
False ^ False       False
False ^ NA          NA
NA ^ NA             NA

When an NA is present in an operation, the output value is NA only if the result cannot be determined solely based on the other input. For example, True | NA is True, because both True | True and True | False are True. In that case, we don’t actually need to consider the value of the NA.
On the other hand, True & NA is NA. The result depends on whether the NA really is True or False, since True & True is True, but True & False is False, so we can’t determine the output.
This differs from how np.nan behaves in logical operations. pandas treated np.nan is always false in the output.

In or
-----
pd.Series([True, False, np.nan], dtype="object") | True
pd.Series([True, False, np.nan], dtype="boolean") | True

In and
-------
pd.Series([True, False, np.nan], dtype="object") & True
pd.Series([True, False, np.nan], dtype="boolean") & True

===========================
2.16 Computational tools===
===========================
2.16.1 Statistical functions
----------------------------

Percent change
---------------
Series and DataFrame have a method pct_change() to compute the percent change over a given number of periods (using fill_method to fill NA/null values before computing the percent change).
ser = pd.Series(np.random.randn(8))
ser.pct_change()

df = DataFrame(np.random.rand(10, 4))
df.pct_change(3)

s = pd.Series(range(1,6))
print(s)
s.pct_change(3)

Covariance
-----------
If two assets are moving together in the same direction then they are said to have positive covariance else negative covariance.

Series.cov() can be used to compute covariance between series (excluding missing values).
s1 = pd.Series(range(1,11))
s2 = pd.Series(range(2,21,2))
s1.cov(s2)

Analogously, DataFrame.cov() to compute pairwise covariances among the series in the DataFrame, also exclud- ing NA/null values.

Note: Assuming the missing data are missing at random this results in an estimate for the covariance matrix which is unbiased. However, for many applications this estimate may not be acceptable because the estimated covariance matrix is not guaranteed to be positive semi-definite. This could lead to estimated correlations having absolute values which are greater than one, and/or a non-invertible covariance matrix.
frame = pd.DataFrame(np.random.randn(1000, 5), columns=["a", "b", "c", "d", "e"])
frame.cov()

DataFrame.cov also supports an optional min_periods keyword that specifies the required minimum number of observations for each column pair in order to have a valid result.
frame = pd.DataFrame(np.random.randn(20, 3), columns=["a", "b", "c"])
frame.loc[frame.index[:5], "a"] = np.nan
frame.loc[frame.index[5:10], "b"] = np.nan
frame.cov()
frame.cov(min_periods=12)

s1 = pd.Series(range(1,6))
s2 = pd.Series(range(6,11))
s2[:3] = np.nan
s1.cov(s2, min_periods=2)

Correlation
------------
Correlation may be computed using the corr() method. Using the method parameter, several methods for com- puting correlations are provided:
Method name                             Description
pearson(default)                    Standard correlation coefficient
kendall                             Kendall Tau correlation coefficient
spearman                            Spearman rank correlation coefficient

All of these are currently computed using pairwise complete observations. Wikipedia has articles covering the above correlation coefficients:
• Pearson correlation coefficient
• Kendall rank correlation coefficient
• Spearman’s rank correlation coefficient

Note: Please see the caveats associated with this method of calculating correlation matrices in the covariance section.

Note that non-numeric columns will be automatically excluded from the correlation calculation. Like cov, corr also supports the optional min_periods keyword:

frame = pd.DataFrame(np.random.randn(20, 3), columns=["a", "b", "c"])
frame.loc[frame.index[:5], "a"] = np.nan
frame.loc[frame.index[5:10], "b"] = np.nan
frame.corr()
frame.corr(min_periods=12)

The method argument can also be a callable for a generic correlation calculation. In this case, it should be a single function that produces a single value from two ndarray inputs. Suppose we wanted to compute the correlation based on histogram intersection:
# histogram intersection
def histogram_intersection(a, b):
    return np.minimum(np.true_divide(a, a.sum()),
                      np.true_divide(b, b.sum())).sum()

frame.corr(method=histogram_intersection)

A related method corrwith() is implemented on DataFrame to compute the correlation between like-labeled Series contained in different DataFrame objects.
index = ["a", "b", "c", "d", "e"]
columns = ["one", "two", "three", "four"]
df1 = pd.DataFrame(np.random.randn(5, 4), index=index, columns=columns)
df2 = pd.DataFrame(np.random.randn(4, 4), index=index[:4], columns=columns)

df1.corrwith(df2)
df2.corrwith(df1, axis=1)

Data ranking
------------
The rank() method produces a data ranking with ties being assigned the mean of the ranks (by default) for the group:
s = pd.Series(np.random.randn(5), index=list("abcde"))
s["d"] = s["b"] # so there's a tie
s.rank()

rank() is also a DataFrame method and can rank either the rows (axis=0) or the columns (axis=1). NaN values are excluded from the ranking
df = pd.DataFrame(np.random.randn(10, 6))
df[4] = df[2][:5] # some ties
df.rank(1, ascending=False)

rank optionally takes a parameter ascending which by default is true; when false, data is reverse-ranked, with larger values assigned a smaller rank.

rank supports different tie-breaking methods, specified with the method parameter: • average : average rank of tied group
• min : lowest rank in the group
• max : highest rank in the group
• first : ranks assigned in the order they appear in the array

=====================================
2.17 Group by: split-apply-combine===
=====================================
By “group by” we are referring to a process involving one or more of the following steps: • Splitting the data into groups based on some criteria.
• Applying a function to each group independently.
• Combining the results into a data structure.
Out of these, the split step is the most straightforward. In fact, in many situations we may wish to split the data set into groups and do something with those groups. In the apply step, we might wish to do one of the following:
• Aggregation: compute a summary statistic (or statistics) for each group. Some examples: – Compute group sums or means.
– Compute group sizes / counts.
• Transformation: perform some group-specific computations and return a like-indexed object. Some examples: – Standardize data (zscore) within a group.
– Filling NAs within groups with a value derived from each group.
• Filtration: discard some groups, according to a group-wise computation that evaluates True or False. Some examples:
– Discard data that belongs to groups with only a few members.
– Filter out data based on the group sum or mean.
• Some combination of the above: GroupBy will examine the results of the apply step and try to return a sensibly
combined result if it doesn’t fit into either of the above two categories.
Since the set of object instance methods on pandas data structures are generally rich and expressive, we often simply want to invoke, say, a DataFrame function on each group. The name GroupBy should be quite familiar to those who have used a SQL-based tool (or itertools), in which you can write code like:

 SELECT Column1, Column2, mean(Column3), sum(Column4) FROM SomeTable
GROUP BY Column1, Column2

We aim to make operations like this natural and easy to express using pandas. We’ll address each area of GroupBy functionality then provide some non-trivial examples / use cases.

2.17.1 Splitting an object into groups
=======================================
pandas objects can be split on any of their axes. The abstract definition of grouping is to provide a mapping of labels to group names. To create a GroupBy object (more on what the GroupBy object is later), you may do the following:

df = pd.DataFrame([
    ("bird", "Falconiformes", 389.0),
    ("bird", "Psittaciformes", 24.0),
    ("mammal", "Carnivora", 80.2),
    ("mammal", "Primates", np.nan),
    ("mammal", "Carnivora", 58),
],
   index=["falcon", "parrot", "lion", "monkey", "leopard"],
   columns=("class", "order", "max_speed"),
)
df

# default is axis=0
grouped = df.groupby('class')
grouped = df.groupby('order', axis='columns')
grouped = df.groupby(["class", "order"])

The mapping can be specified many different ways:
• A Python function, to be called on each of the axis labels.
• A list or NumPy array of the same length as the selected axis.
• A dict or Series, providing a label -> group name mapping.
• For DataFrame objects, a string indicating either a column name or an index level name to be used to group. • df.groupby('A') is just syntactic sugar for df.groupby(df['A']).
• A list of any of the above things.
Collectively we refer to the grouping objects as the keys. For example, consider the following DataFrame:

Note: A string passed to groupby may refer to either a column or an index level.If a string matches both a column
name and an index level name, a ValueError will be raised.

On a DataFrame, we obtain a GroupBy object by calling groupby(). We could naturally group by either the A or B columns, or both:
grouped = df.groupby("A")
grouped = df.groupby(["A", "B"])

If we also have a MultiIndex on columns A and B, we can group by all but the specified columns
df2 = df.set_index(["A", "B"])
grouped = df2.groupby(df2.index.names.difference(["B"]))
grouped.sum()

These will split the DataFrame on its index (rows). We could also split by the columns:
def get_letter_type(letter):
    if letter.lower() in 'aeiou':
        return 'vowel'
    else:
        return 'consonant'
df.groupby(get_letter_type, axis=1).apply(lambda x: x.values)

pandas Index objects support duplicate values. If a non-unique index is used as the group key in a groupby operation, all values for the same index value will be considered to be in one group and thus the output of aggregation functions will only contain unique index values:
lst = [1, 2, 3, 1, 2, 3]
s = pd.Series([1, 2, 3, 10, 20, 30], lst)
grouped = s.groupby(level=0)
grouped.first()
grouped.last()
grouped.sum()

Note that no splitting occurs until it’s needed. Creating the GroupBy object only verifies that you’ve passed a valid mapping.

Note: Many kinds of complicated data manipulations can be expressed in terms of GroupByoperations (though can’t be guaranteed to be the most efficient). You can get quite creative with the label mapping functions.

GroupBy sorting
---------------
By default the group keys are sorted during the groupby operation. You may however pass sort=False for potential speedups:
df2 = pd.DataFrame({"X": ["B", "B", "A", "A"], "Y": [1, 2, 3, 4]})
df2.groupby('X', sort=False).sum()
df2.groupby('X').sum()

Note that groupby will preserve the order in which observations are sorted within each group. For example, the groups created by groupby() below are in the order they appeared in the original DataFrame:
df3 = pd.DataFrame({"X": ["A", "B", "A", "B"], "Y": [1, 4, 3, 2]})
df3.groupby('X').get_group('A')
df3.groupby('X').get_group('B')

GroupBy dropna
--------------
By default NA values are excluded from group keys during the groupby operation. However, in case you want to include NA values in group keys, you could pass dropna=False to achieve it.
df_list = [[1, 2, 3], [1, None, 4], [2, 1, 3], [1, 2, 2]]
df_dna = pd.DataFrame(df_list, columns=["a", "b", "c"])
df_dna

# Default ``dropna`` is set to True, which will exclude NaNs in keys
df_dna.groupby(by="b").sum()
# In order to allow NaN in keys, set ``dropna`` to False
df_dna.groupby(by="b", dropna=False).sum()

GroupBy object attributes
-------------------------
The groups attribute is a dict whose keys are the computed unique groups and corresponding values being the axis labels belonging to each group. In the above example we have:
df.groupby('A').groups
df.groupby(get_letter_type, axis=1).groups
df.groupby(get_letter_type, axis=1).groups

Calling the standard Python len function on the GroupBy object just returns the length of the groups dict, so it is largely just a convenience:
grouped = df.groupby(['A','B'])
grouped.groups
len(grouped)

GroupBy will tab complete column names (and other attributes):
d="""date,height,weight,gender
2000-01-01,42.849980,157.500553,male
2000-01-02,49.607315,177.340407,male
2000-01-03,56.293531,171.524640,male
2000-01-04,48.421077,144.251986,female
2000-01-05,46.556882,152.526206,male
2000-01-06,68.448851,168.272968,female
2000-01-07,70.757698,136.431469,male
2000-01-08,58.909500,176.499753,female
2000-01-09,76.435631,174.094104,female
2000-01-10,45.306120,177.540920,male"""
df=g(d)
df['date'] = pd.to_datetime(df.date)
df = df.set_index('date')
gb = df.groupby('gender')
gb.<TAB>

GroupBy with MultiIndex
------------------------
With hierarchically-indexed data, it’s quite natural to group by one of the levels of the hierarchy. Let’s create a Series with a two-level MultiIndex.

arrays = [
    ["bar", "bar", "baz", "baz", "foo", "foo", "qux", "qux"],
    ["one", "two", "one", "two", "one", "two", "one", "two"],
]
index = pd.MultiIndex.from_arrays(arrays, names=['first', 'second'])
s = pd.Series(np.random.rand(8), index=index)
s

# We can then group by one of the levels in s.
grouped = s.groupby(level=1)
grouped.sum()

# If the MultiIndex has names specified, these can be passed instead of the level number:
s.groupby(level='second').sum()


The aggregation functions such as sum will take the level parameter directly. Additionally, the resulting index will be named according to the chosen level:
s.sum(level='second')

Grouping with multiple levels is supported.
s.groupby(level=["first", "second"]).sum()

Index level names may be supplied as keys.
s.groupby(["first", "second"]).sum()

Grouping DataFrame with Index levels and columns
-------------------------------------------------
arrays = [["bar", "bar", "baz", "baz", "foo", "foo", "qux", "qux"],
          ["one", "two", "one", "two", "one", "two", "one", "two"],
         ]
index = pd.MultiIndex.from_arrays(arrays, names=['first', 'second'])

df = pd.DataFrame({"A": [1, 1, 1, 1, 2, 2, 3, 3], "B": np.arange(8)},
                  index=index)
df
The following example groups df by the second index level and the A column.
df.groupby(['A', pd.Grouper(level=1)]).sum()

Index levels may also be specified by name.
df.groupby(['A', pd.Grouper(level='second')]).sum()

Index level names may be specified as keys directly to groupby.
df.groupby(['second','A',]).sum()

DataFrame column selection in GroupBy
--------------------------------------
Once you have created the GroupBy object from a DataFrame, you might want to do something different for each of the columns. Thus, using [] similar to getting a column from a DataFrame, you can do:

df = pd.DataFrame({
    "A": ["foo", "bar", "foo", "bar", "foo", "bar", "foo", "foo"],
    "B": ["one", "one", "two", "three", "two", "two", "one", "three"],
    "C": np.random.randn(8),
    "D": np.random.randn(8),
})
grouped = df.groupby(["A"])
grouped['C']
grouped.D

This is mainly syntactic sugar for the alternative and much more verbose:
df["C"].groupby(df["A"])

Additionally this method avoids recomputing the internal grouping information derived from the passed key.

2.17.2 Iterating through groups
-------------------------------
With the GroupBy object in hand, iterating through the grouped data is very natural and functions similarly to itertools.groupby():
grouped = df.groupby('A')
for name, group in grouped:
    print(name)
    print(group)

In the case of grouping by multiple keys, the group name will be a tuple:
for name, group in df.groupby(['A','B']):
    print(name)
    print(group)

2.17.3 Selecting a group
=========================
A single group can be selected using get_group():
grouped = df.groupby(["A"])
grouped.get_group('bar')

Or for an object grouped on multiple columns:
df.groupby(['A','B']).get_group(('bar', 'one'))

2.17.4 Aggregation
===================
Once the GroupBy object has been created, several methods are available to perform a computation on the grouped data. These operations are similar to the aggregating API, window API, and resample API.
An obvious one is aggregation via the aggregate() or equivalently agg() method:
grouped = df.groupby("A")
grouped.aggregate(np.sum)

grouped = df.groupby(['A', 'B'])
grouped.aggregate(np.sum)

As you can see, the result of the aggregation will have the group names as the new index along the grouped axis. In the case of multiple keys, the result is a MultiIndex by default, though this can be changed by using the as_index option:
grouped = df.groupby(["A", "B"], as_index=False)
grouped.aggregate(sum)
df.groupby("A", as_index=False).sum()

Note that you could use the reset_index DataFrame function to achieve the same result as the column names are stored in the resulting MultiIndex:
df.groupby(["A", "B"]).sum().reset_index()

Another simple aggregation example is to compute the size of each group. This is included in GroupBy as the size method. It returns a Series whose index are the group names and whose values are the sizes of each group.
grouped.size()
grouped.describe()

Another aggregation example is to compute the number of unique values of each group. This is similar to the value_counts function, except that it only counts unique values.
ll = [['foo', 1], ['foo', 2], ['foo', 2], ['bar', 1], ['bar', 1]]
df4 = pd.DataFrame(ll, columns=["A", "B"])
print(df4)
print(df4.groupby('A').nunique())
df4.groupby('A').B.value_counts()

Note: Aggregation functions will not return the groups that you are aggregating over if they are named columns, when as_index=True, the default. The grouped columns will be the indices of the returned object.
Passing as_index=False will return the groups that you are aggregating over, if they are named columns.

Aggregating functions are the ones that reduce the dimension of the returned objects. Some common aggregating
functions are tabulated below:
Function            Description
mean()              Compute mean of groups
sum()               Compute sum of group values
size()              Compute group sizes
count()             Compute count of group
std()               Standard deviation of groups
var()               Compute variance of groups
sem()               Standard error of the mean of groups
describe()          Generates descriptive statistics
first()             Compute first of group values
last()              Compute last of group values
nth()               Take nth value, or a subset if n is a list
min()               Compute min of group values
max()               Compute max of group values

df4.groupby('A').agg(['count', 'size'])

The aggregating functions above will exclude NA values. Any function which reduces a Series to a scalar value is an aggregation function and will work, a trivial example is df.groupby('A').agg(lambda ser: 1). Note that nth() can act as a reducer or a filter.

Applying multiple functions at once
With grouped Series you can also pass a list or dict of functions to do aggregation with, outputting a DataFrame:
grouped = df.groupby("A")
grouped.C.agg([np.sum, np.mean, np.std])

The resulting aggregations are named for the functions themselves. If you need to rename, then you can add in a chained operation for a Series like this:
grouped.C.agg([np.sum, np.mean, np.std]).rename(columns={'sum':'foo',
                'mean':'bar', 'std':'baz'})

For a grouped DataFrame, you can rename in a similar manner:
grouped.agg([np.sum, np.mean, np.std]).rename(
       columns={"sum": "foo", "mean": "bar", "std": "baz"}
)

Note: In general, the output column names should be unique. You can’t apply the same function (or two functions with the same name) to the same column.
grouped["C"].agg(["sum", "sum"])

pandas does allow you to provide multiple lambdas. In this case, pandas will mangle the name of the (nameless) lambda functions, appending _<i> to each subsequent lambda.
grouped["C"].agg([lambda x: x.max() - x.min(), lambda x: x.median() - x.mean()])

Named aggregation
==================
New in version 0.25.0.
To support column-specific aggregation with control over the output column names, pandas accepts the special syntax
in GroupBy.agg(), known as “named aggregation”, where
• The keywords are the output column names
• The values are tuples whose first element is the column to select and the second element is the aggregation to apply to that column. pandas provides the pandas.NamedAgg namedtuple with the fields ['column', 'aggfunc'] to make it clearer what the arguments are. As usual, the aggregation can be a callable or a string alias.

animals.groupby("kind").agg(min_height=pd.NamedAgg(column="height", aggfunc="min"),
                max_height=pd.NamedAgg(column="height", aggfunc="max"),
                average_weight=pd.NamedAgg(column="weight", aggfunc=np.mean),
                )

pandas.NamedAgg is just a namedtuple. Plain tuples are allowed as well.
animals.groupby('kind').agg(min_height = ('height', min),
                           max_height = ('height', max),
                           avg_weight = ('weight', 'mean'))

If your desired output column names are not valid Python keywords, construct a dictionary and unpack the keyword arguments
animals.groupby("kind").agg(
    **{
        "total weight": pd.NamedAgg(column="weight", aggfunc=sum)
    })

# Syntactic sugar
animals.groupby("kind").agg(
    **{
        "total weight": ("weight", sum)
    })

Additional keyword arguments are not passed through to the aggregation functions. Only pairs of (column, aggfunc) should be passed as **kwargs. If your aggregation functions requires additional arguments, partially apply them with functools.partial().

Note: For Python 3.5 and earlier, the order of **kwargs in a functions was not preserved. This means that the output column ordering would not be consistent. To ensure consistent ordering, the keys (and so output columns) will always be sorted for Python 3.5.

Named aggregation is also valid for Series groupby aggregations. In this case there’s no column selection, so the values are just the functions.
animals.groupby('kind').height.agg(min_height = min, max_height=max)

Applying different functions to DataFrame columns
==================================================
By passing a dict to aggregate you can apply a different aggregation to the columns of a DataFrame:
grouped.agg({'C':np.sum, 'D':lambda x:np.std(x, ddof=1)})

The function names can also be strings. In order for a string to be valid it must be either implemented on GroupBy or available via dispatching:
grouped.agg({"C": "sum", "D": "std"})

Cython-optimized aggregation functions
---------------------------------------
Some common aggregations, currently only sum, mean, std, and sem, have optimized Cython implementations:
df.groupby('A').sum()
df.groupby(['A', 'B']).mean()

Of course sum and mean are implemented on pandas objects, so the above code would work even without the special versions via dispatching (see below).

2.17.5 Transformation
======================
The transform method returns an object that is indexed the same (same size) as the one being grouped. The transform function must:
• Return a result that is either the same size as the group chunk or broadcastable to the size of the group chunk (e.g.,ascalar,grouped.transform(lambda x: x.iloc[-1])).
• Operate column-by-column on the group chunk. The transform is applied to the first group chunk using chunk.apply.
• Notperformin-placeoperationsonthegroupchunk.Groupchunksshouldbetreatedasimmutable,andchanges to a group chunk may produce unexpected results. For example, when using fillna, inplace must be False(grouped.transform(lambda x: x.fillna(inplace=False))).
• (Optionally) operates on the entire group chunk. If this is supported, a fast path is used starting from the second chunk
For example, suppose we wished to standardize the data within each group:

index = pd.date_range("10/1/1999", periods=1100)
ts = pd.Series(np.random.normal(0.5, 2, 1100), index)
ts = ts.rolling(window=100, min_periods=100).mean().dropna()
ts.head()
ts.tail()
transformed = ts.groupby(lambda x: x.year).transform(
    lambda x: (x - x.mean()) / x.std())
transformed

We would expect the result to now have mean 0 and standard deviation 1 within each group, which we can easily check:

# Original Data
grouped = ts.groupby(lambda x: x.year)
grouped.mean()
grouped.std()

# Transformed Data
grouped_trans = transformed.groupby(lambda x: x.year)
grouped_trans.mean()
grouped_trans.std()

We can also visually compare the original and transformed data sets.
compare = pd.DataFrame({
    'Original':ts,
    'Transformed':transformed
})
compare.plot()

Transformation functions that have lower dimension outputs are broadcast to match the shape of the input array.
ts.groupby(lambda x: x.year).transform(lambda x: x.max() - x.min())

Alternatively, the built-in methods could be used to produce the same outputs.
max = ts.groupby(lambda x: x.year).transform('max')
min = ts.groupby(lambda x: x.year).transform('min')
max - min

Another common data transform is to replace missing data with the group mean.
data_df = pd.DataFrame(np.random.randn(1000,3), columns=list('ABC'))
data_df[1:5] = np.nan
data_df

countries = np.array(['US', 'UK', 'GR', 'JP'])
key = countries[np.random.randint(0,4,1000)]
grouped = data_df.groupby(key)
grouped.count()

transformed = grouped.transform(lambda x: x.fillna(x.mean()))
transformed

We can verify that the group means have not changed in the transformed data and that the transformed data contains no NAs.
data_df = pd.DataFrame(np.random.randn(1000,3), columns=list('ABC'))
data_df[1:500] = np.nan
data_df

countries = np.array(['US', 'UK', 'GR', 'JP'])
key = countries[np.random.randint(0,4,1000)]
grouped = data_df.groupby(key)
grouped.count()

transformed = grouped.transform(lambda x: x.fillna(x.mean()))
transformed

grouped_trans = transformed.groupby(key)
grouped.mean() # original group means
grouped_trans.mean()
grouped.count()
grouped_trans.count()

grouped.size()
grouped_trans.size()

### Difference between size and count
Size includes np.nan but count does not.
df = pd.DataFrame({
    'a':[10,20,40,np.nan,50,60],
    'c':['UK','US','US','US','UK','US']
})
df.groupby('c').count()
	a
c
UK	2
US	3
df.groupby('c').size()
c
UK    2
US    4

Note: Some functions will automatically transform the input when applied to a GroupBy object, but returning an object of the same shape as the original. Passing as_index=False will not affect these transformation methods.
For example: fillna, ffill, bfill, shift..
grouped.ffill()

Example:
To apply gb on year of provided dates.
df = DataFrame({
    'date' : pd.date_range('2017.01.01', periods=1000),
    'vol' : np.random.choice(100,1000)
})
df = df.set_index('date')
df.groupby(lambda x: x.year).transform('count')

Window and resample operations
-------------------------------
It is possible to use resample(), expanding() and rolling() as methods on groupbys.
The example below will apply the rolling() method on the samples of the column B based on the groups of
column A.
df_re = pd.DataFrame({
    'A':np.repeat([1,5],5),
    'B':np.arange(10)
})
df_re.groupby('A').rolling(4).B.mean()
df_re.groupby('A').expanding(4).B.mean()

The expanding() method will accumulate a given operation (sum() in the example) for all the members of each particular group.
df_re.groupby("A").expanding().sum()

Suppose you want to use the resample() method to get a daily frequency in each group of your dataframe and wish to complete the missing values with the ffill() method.
df_re = pd.DataFrame({
    "date": pd.date_range(start="2016-01-01", periods=4, freq="W"),
    "group": [1, 1, 2, 2],
    "val": [5, 6, 7, 8],
}).set_index("date")
df_re.groupby("group", as_index=False).resample('1D').ffill()

2.17.6 Filtration
------------------
The filter method returns a subset of the original object. Suppose we want to take only elements that belong to groups with a group sum greater than 2.
sf = pd.Series([1, 1, 2, 2, 3, 3, 3])
print(sf.groupby(sf).groups)
sf.groupby(sf).filter(lambda x: x.sum() > 2)

The argument of filter must be a function that, applied to the group as a whole, returns True or False. Another useful operation is filtering out elements that belong to groups with only a couple members.
dff = pd.DataFrame({"A": np.arange(8), "B": list("aabbbbcc")})
print(dff.groupby('B').groups)
dff.groupby('B').filter(lambda x:len(x) > 2)

Alternatively, instead of dropping the offending groups, we can return a like-indexed objects where the groups that do not pass the filter are filled with NaNs.
dff.groupby('B').filter(lambda x:len(x) > 2, dropna=False)

For DataFrames with multiple columns, filters should explicitly specify a column as the filter criterion.
dff["C"] = np.arange(8)
dff.groupby('B').filter(lambda x: len(x.C) > 2)

 Note: Some functions when applied to a groupby object will act as a filter on the input,returning a reduced shape of the original (and potentially eliminating groups), but with the index unchanged. Passing as_index=False will not affect these transformation methods.
dff.groupby('B', as_index=False).filter(lambda x: len(x.C) > 2)

To fetch columns matching provided filter.
dff = pd.DataFrame({"AB": np.arange(8), "BC": list("aabbbbcc"),
                   "AC":np.arange(8)})
dff.filter(like='A')

dff = pd.DataFrame({"AB": np.arange(8), "BC": list("aabbbbcc"),
                   "AC":np.arange(8), "EZ":np.arange(8)})
dff.filter(regex=('[A-C]'))

2.17.7 Dispatching to instance methods
--------------------------------------
When doing an aggregation or transformation, you might just want to call an instance method on each data group. This is pretty easy to do by passing lambda functions:
df = pd.DataFrame({
    "A": ["foo", "bar", "foo", "bar", "foo", "bar", "foo", "foo"],
    "B": ["one", "one", "two", "three", "two", "two", "one", "three"],
    "C": np.random.randn(8),
    "D": np.random.randn(8),
})
grouped = df.groupby("A")
grouped.agg(lambda x: x.std())

But, it’s rather verbose and can be untidy if you need to pass additional arguments. Using a bit of metaprogramming cleverness, GroupBy now has the ability to “dispatch” method calls to the groups:
grouped.std()

What is actually happening here is that a function wrapper is being generated. When invoked, it takes any passed arguments and invokes the function with any arguments on each group (in the above example, the std function). The results are then combined together much in the style of agg and transform (it actually uses apply to infer the gluing, documented next). This enables some operations to be carried out rather succinctly:
tsdf = pd.DataFrame(np.random.randn(1000, 3),
                    index=pd.date_range("1/1/2000", periods=1000),
                    columns=["A", "B", "C"],
                   )
tsdf.iloc[::2] = np.nan
grouped = tsdf.groupby(lambda x: x.year)
grouped.fillna(method='pad').bfill()

In this example, we chopped the collection of time series into yearly chunks then independently called fillna on the groups.
The nlargest and nsmallest methods work on Series style groupbys:
s = pd.Series([9, 8, 7, 5, 19, 1, 4.2, 3.3])
g = pd.Series(list("abababab"))
gb = s.groupby(g)
gb.nlargest(2)
gb.nsmallest(2)

2.17.8 Flexible apply
-----------------------
Some operations on the grouped data might not fit into either the aggregate or transform categories. Or, you may simply want GroupBy to infer how to combine the results. For these, use the apply function, which can be substituted for both aggregate and transform in many standard use cases. However, apply can handle some exceptional use cases, for example:
df = pd.DataFrame({
    "A": ["foo", "bar", "foo", "bar", "foo", "bar", "foo", "foo"],
    "B": ["one", "one", "two", "three", "two", "two", "one", "three"],
    "C": np.random.randn(8),
    "D": np.random.randn(8),
})
grouped = df.groupby('A')

# could also just call .describe()
grouped.C.apply(lambda x: x.describe())

The dimension of the returned result can also change:
grouped = df.groupby('A').C

def f(group):
    print(group)
    return pd.DataFrame({
        'original':group,
        'deamed':group - group.mean()
    })

grouped.apply(f)

apply on a Series can operate on a returned value from the applied function, that is itself a series, and possibly upcast the result to a DataFrame:
def f(x):
    return pd.Series([x, x**2], index=["x", "x^2"])

def f1(x):
    return pd.Series(x, index=['x'])

s = pd.Series(np.random.rand(5))
s.apply(f)

# When we apply apply on a series, it returns df
s.apply(f1)

Note: apply can act as a reducer, transformer, or filter function, depending on exactly what is passed to it. So depending on the path taken, and exactly what you are grouping. Thus the grouped columns(s) may be included in the output as well as set the indices.

2.17.9 Numba Accelerated Routines
==================================
New in version 1.1.
If Numba is installed as an optional dependency, the transform and aggregate methods support engine='numba' and engine_kwargs arguments. The engine_kwargs argument is a dictionary of key- word arguments that will be passed into the numba.jit decorator. These keyword arguments will be applied to the passed function. Currently only nogil, nopython, and parallel are supported, and their default values are set to False, True and False respectively.
The function signature must start with values, index exactly as the data belonging to each group will be passed into values, and the group index will be passed into index.

Note: In terms of performance, the first time a function is run using the Numba engine will be slow as Numba will have some function compilation overhead. However, the compiled functions are cached, and subsequent calls will be fast. In general, the Numba engine is performant with a larger amount of data points (e.g. 1+ million).

Warning: When using engine='numba', there will be no “fall back” behavior internally. The group data and group index will be passed as NumPy arrays to the JITed user defined function, and no alternative execution attempts will be tried.

In [1]: N = 10 ** 3
In [2]: data = {0: [str(i) for i in range(100)] * N, 1: list(range(100)) * N} In [3]: df = pd.DataFrame(data, columns=[0, 1])
In [4]: def f_numba(values, index):
...:
...:
...:
...:
...:
...:
...:
...:
total = 0
for i, value in enumerate(values):
ifi%2: total
else: total
return total
+=value+5 +=value*2
In [5]: def f_cython(values):
total = 0
for i, value in enumerate(values):
ifi%2: total
else: total
return total
+=value+5 +=value*2
In [6]: groupby = df.groupby(0)
# Run the first time, compilation time will affect performance
In [7]: %timeit -r 1 -n 1 groupby.aggregate(f_numba, engine='numba') # noqa: E225 2.14 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)
# Function is cached and performance will improve
In [8]: %timeit groupby.aggregate(f_numba, engine='numba')
4.93 ms ± 32.3 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)
In [9]: %timeit groupby.aggregate(f_cython, engine='cython')
18.6 ms ± 84.8 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)

2.17.10 Other useful features
==============================
Automatic exclusion of “nuisance” columns
Again consider the example DataFrame we’ve been looking at:

we don’t care about the data in column B. We refer to this as a “nuisance” column. If the passed aggregation function can’t be applied to some columns, the troublesome columns will be (silently) dropped. Thus, this does not pose any problems:
df.groupby('A').std()

Note that df.groupby('A').colname.std(). is more efficient than df.groupby('A').std(). colname, so if the result of an aggregation function is only interesting over one column (here colname), it may be filtered before applying the aggregation function.

Note: Any object column, also if it contains numerical values such as Decimal objects,is considered as a “nuisance” columns. They are excluded from aggregate functions automatically in groupby.
If you do wish to include decimal or object columns in an aggregation with other non-nuisance data types, you must do so explicitly.

from decimal import Decimal

df_dec = pd.DataFrame({
    'id':np.tile([1,2],2),
    'int_column':np.arange(1,5),
    'decimal_column':[Decimal('0.5'), Decimal('0.15'),
                     Decimal('0.25'), Decimal('0.4')]
})
df_dec

# Decimal columns can be sum'd explicitly by themselves...
df_dec.groupby('id', as_index=False)[['decimal_column']].sum()
df_dec.groupby('id', as_index=False).decimal_column.sum()

# ...but cannot be combined with standard data types or they will be excluded
df_dec.groupby(["id"])[["int_column", "decimal_column"]].sum()

# Use .agg function to aggregate over standard and "nuisance" data types
# at the same time

df_dec.groupby(["id"]).agg({"int_column": "sum", "decimal_column": "sum"})

Handling of (un)observed Categorical values
============================================
When using a Categorical grouper (as a single grouper, or as part of multiple groupers), the observed keyword controls whether to return a cartesian product of all possible groupers values (observed=False) or only those that are observed groupers (observed=True).
Show all values
pd.Series([1, 1, 1]).groupby(
    pd.Categorical(np.repeat('a',3), categories=['a','b']),
    observed=False).count()

Show only the observed values:
pd.Series([1, 1, 1]).groupby(
    pd.Categorical(np.repeat('a',3), categories=['a','b']),
    observed=True).count()

The returned dtype of the grouped will always include all of the categories that were grouped.
s = (pd.Series([1, 1, 1]).groupby(
    pd.Categorical(np.repeat('a',3), categories=['a','b']),
    observed=False).count())
s.index.dtype # CategoricalDtype(categories=['a', 'b'], ordered=False)

NA and NaT group handling
--------------------------
If there are any NaN or NaT values in the grouping key, these will be automatically excluded. In other words, there will never be an “NA group” or “NaT group”. This was not the case in older versions of pandas, but users were generally discarding the NA group anyway (and supporting it was an implementation headache).

Grouping with ordered factors
-------------------------------
qcut -
df = pd.DataFrame({
    'a':[.2,.1,.5,.9]
})

pd.qcut(df.a, [0,.25,.5,.75, 1])

Categorical variables represented as instance of pandas’s Categorical class can be used as group keys. If so, the order of the levels will be preserved:
data = pd.Series(np.random.randn(100))
factor = pd.qcut(data, [0,.25,.5,.75, 1])
data.groupby(factor).mean()

Grouping with a grouper specification
--------------------------------------
You may need to specify a bit more data to properly group. You can use the pd.Grouper to provide this local control
Groupby a specific column with the desired frequency. This is like resampling.
import datetime
df = pd.DataFrame({
     "Branch": "A A A A A A A B".split(),
     "Buyer": "Carl Mark Carl Carl Joe Joe Joe Carl".split(),
     "Quantity": [1, 3, 5, 1, 8, 1, 9, 3],
     "Date": [
         datetime.datetime(2013, 1, 1, 13, 0),
         datetime.datetime(2013, 1, 1, 13, 5),
         datetime.datetime(2013, 10, 1, 20, 0),
         datetime.datetime(2013, 10, 2, 10, 0),
         datetime.datetime(2013, 10, 1, 20, 0),
         datetime.datetime(2013, 10, 2, 10, 0),
         datetime.datetime(2013, 12, 2, 12, 0),
         datetime.datetime(2013, 12, 2, 14, 0),
     ]
 })
df.groupby([pd.Grouper(freq='1M', key='Date'), "Buyer"]).sum()

You have an ambiguous specification in that you have a named index and a column that could be potential groupers.
df = df.set_index('Date')
df['Date'] = df.index + pd.offsets.MonthEnd(2)

## Column and Index have same name

## when we want to group by column
df.groupby([pd.Grouper(freq='6M', key='Date'), "Buyer"]).sum()

## when we want to group by index
df.groupby([pd.Grouper(freq='6M', level='Date'), "Buyer"]).sum()

Taking the first rows of each group
------------------------------------
Just like for a DataFrame or Series you can call head and tail on a groupby:
df = pd.DataFrame([[1, 2], [1, 4], [5, 6]], columns=["A", "B"])
df.groupby('A').head(1)
df.groupby('A').tail(1)
This shows the first or last n rows from each group.

Taking the nth row of each group
--------------------------------
To select from a DataFrame or Series the nth item, use nth(). This is a reduction method, and will return a single row (or no row) per group if you pass an int for n:
df = pd.DataFrame([[1, np.nan], [1, 4], [5, 6]], columns=["A", "B"])
df.groupby('A', as_index=False).nth(0)
df.groupby('A', as_index=False).nth(-1)
df.groupby('A', as_index=False).nth(1)

If you want to select the nth not-null item, use the dropna kwarg. For a DataFrame this should be either 'any' or 'all' just like you would pass to dropna:
df.groupby('A').nth(0, dropna='any')

# nth(0) is the same as g.first()
g = df.groupby('A')
g.first()

# nth(-1) is the same as g.last()
g.nth(-1, dropna="any") # NaNs denote group exhausted when using dropna
g.last()

g.B.nth(0, dropna='all')

As with other methods, passing as_index=False, will achieve a filtration, which returns the grouped row.
df = pd.DataFrame([[1, np.nan], [1, 4], [5, 6]], columns=["A", "B"])
print(df)
g = df.groupby("A", as_index=False)
g.nth(0)
g.nth(-1)

You can also select multiple rows from each group by specifying multiple nth values as a list of ints.
business_dates = pd.date_range(start="5/1/2021", end="6/30/2021", freq="B")
df = pd.DataFrame(1, index=business_dates, columns=["a", "b"])
# get the first, 4th, and last date index for each month
df.groupby([df.index.year, df.index.month]).nth([0,3,-1])

Enumerate group items
----------------------
To see the order in which each row appears within its group, use the cumcount method:
dfg = pd.DataFrame(list("aaabba"), columns=["A"])
dfg.groupby('A').cumcount()
dfg.groupby("A").cumcount(ascending=False)

Enumerate groups
-----------------
To see the ordering of the groups (as opposed to the order of rows within a group given by cumcount) you can use ngroup().
Note that the numbers given to the groups match the order in which the groups would be seen when iterating over the groupby object, not the order they are first observed.
dfg = pd.DataFrame(list("aaabba"), columns=["A"])
dfg.groupby('A').ngroup()
dfg.groupby("A").ngroup(ascending=False)

Plotting
---------
Groupby also works with some plotting methods. For example, suppose we suspect that some features in a DataFrame may differ by group, in this case, the values in column 1 where the group is “B” are 3 higher on average.
np.random.seed(1234)
df = pd.DataFrame(np.random.randn(50, 2))
df["g"] = np.random.choice(["A", "B"], size=50)
df.loc[df["g"] == "B", 1] += 3
df

# We can easily visualize this with a boxplot:
df.groupby("g").boxplot()

The result of calling boxplot is a dictionary whose keys are the values of our grouping column g (“A” and “B”). The values of the resulting dictionary can be controlled by the return_type keyword of boxplot. See the visualization documentation for more.

   Warning: For historical reasons, df.groupby("g").boxplot() is not equivalent to df.boxplot(by= "g"). See https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html#visualization-box-return for an explanation.

Piping function calls
---------------------
Similar to the functionality provided by DataFrame and Series, functions that take GroupBy objects can be chained together using a pipe method to allow for a cleaner, more readable syntax. To read about .pipe in general terms, see here.
Combining .groupby and .pipe is often useful when you need to reuse GroupBy objects.
As an example, imagine having a DataFrame with columns for stores, products, revenue and quantity sold. We’d like to do a groupwise calculation of prices (i.e. revenue/quantity) per store and per product. We could do this in a multi-step operation, but expressing it in terms of piping can make the code more readable. First we set the data:

n = 1000
df = pd.DataFrame({
    "Store": np.random.choice(["Store_1", "Store_2"], n),
    "Product": np.random.choice(["Product_1", "Product_2"], n),
    "Revenue": (np.random.random(n) * 50 + 10).round(2),
    "Quantity": np.random.randint(1, 10, size=n),
})
df.head(2)

Now, to find prices per store/product, we can simply do:

df.groupby(['Store', 'Product']).pipe(
    lambda grp: grp.Revenue.sum() / grp.Quantity.sum()).unstack().round(2)

Same thing without pipe
grp = df.groupby(['Store', 'Product'])
(grp.Revenue.sum() / grp.Quantity.sum()).unstack().round(2)

Piping can also be expressive when you want to deliver a grouped object to some arbitrary function, for example:
def mean(groupby):
    return groupby.mean()

df.groupby(["Store", "Product"]).pipe(mean)

ALternative
df.groupby(["Store", "Product"]).mean()

where mean takes a GroupBy object and finds the mean of the Revenue and Quantity columns respectively for each Store-Product combination. The mean function can be any function that takes in a GroupBy object; the .pipe will pass the GroupBy object as a parameter into the function you specify.

2.17.11 Examples
Regrouping by factor
----------------------
Regroup columns of a DataFrame according to their sum, and sum the aggregated ones.
df = pd.DataFrame({"a": [1, 0, 0], "b": [0, 1, 0],
                   "c": [1, 0, 0], "d": [2, 3, 4]})
df.groupby(df.sum(), axis=1).sum()

Multi-column factorization
--------------------------
By using ngroup(), we can extract information about the groups in a way similar to factorize() (as described further in the reshaping API) but which applies naturally to multiple columns of mixed type and different sources. This can be useful as an intermediate categorical-like step in processing, when the relationships between the group rows are more important than their content, or as input to an algorithm which only accepts the integer encoding

dfg = pd.DataFrame({"A": [1, 1, 2, 3, 2], "B": list("aaaba")})
print(dfg)
dfg.groupby(['A','B']).ngroup()
dfg.groupby(["A", [0, 0, 0, 1, 1]]).ngroup()

Groupby by indexer to ‘resample’ data
--------------------------------------
Resampling produces new hypothetical samples (resamples) from already existing observed data or from a model that generates data. These new samples are similar to the pre-existing samples.
In order to resample to work on indices that are non-datetimelike, the following procedure can be utilized.
In the following examples, df.index // 5 returns a binary array which is used to determine what gets selected for the groupby operation.
Note: The below example shows how we can down sample by consolidation of samples into fewer samples.Hereby using df.index // 5, we are aggregating the samples in bins. By applying std() function, we aggregate the information contained in many samples into a small subset of values which is their standard deviation thereby reducing the number of samples.
df = pd.DataFrame(np.random.randn(10, 2))
df.index // 5
df.groupby(df.index // 5).std()

Returning a Series to propagate names
--------------------------------------
Group DataFrame columns, compute a set of metrics and return a named Series. The Series name is used as the name for the column index. This is especially useful in conjunction with reshaping operations such as stacking in which the column index name will be used as the name of the inserted column:
df = pd.DataFrame({
    "a": [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2],
    "b": [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1],
    "c": [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0],
    "d": [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1],

 })
df

def compute_metrics(x):
    result = {"b_sum": x["b"].sum(), "c_mean": x["c"].mean()}
    return pd.Series(result, name="metrics")

result = df.groupby("a").apply(compute_metrics)
result.stack()

=============================
2.18 Windowing Operations====
=============================
pandas contains a compact set of APIs for performing windowing operations - an operation that performs an aggregation over a sliding partition of values. The API functions similarly to the groupby API in that Series and DataFrame call the windowing method with necessary parameters and then subsequently call the aggregation function.
s = pd.Series(range(5))
print(s)
s.rolling(window=2).sum()

The windows are comprised by looking back the length of the window from the current observation. The result above can be derived by taking the sum of the following windowed partitions of data:
for window in s.rolling(2):
    print(window)

Overview
---------
pandas supports 4 types of windowing operations:
Rolling window: Generic fixed or variable sliding window over the values.
Weighted window: Weighted, non-rectangular window supplied by the scipy.signal library.
Expanding window: Accumulating window over the values.
Exponentially Weighted window: Accumulating and exponentially weighted window over the values.

Concept             Method        Returned Object     Supports time-based windows     Supports chained groupby
Rolling window      rolling       Rolling             Yes                             Yes
Weighted window     rolling       Window                No                              No
Expanding window    expanding       Expanding           No                              Yes
Exponentially
Weighted window    ewm              ExponentialMovingWindow  No                         Yes (as of version 1.2)

As noted above, some operations support specifying a window based on a time offset:
s = pd.Series(np.arange(5), index=pd.date_range('2020.01.01', periods=5))
s.rolling(2).sum()

Additionally, some methods support chaining a groupby operation with a windowing operation which will first group the data by the specified keys and then perform a windowing operation per group.
df = DataFrame({
    'A':list('ababa'),
    'B':range(5)
})
df.groupby('A').expanding().sum().droplevel(1).reset_index()
Note: Windowing operations currently only support numeric data (integer and float) and will always return float64 values.

Warning: Some windowing aggregation, mean, sum, var and std methods may suffer from numerical imprecision due to the underlying windowing algorithms accumulating sums. When values differ with magnitude 1/𝑛𝑝.𝑓𝑖𝑛𝑓𝑜(𝑛𝑝.𝑑𝑜𝑢𝑏𝑙𝑒).𝑒𝑝𝑠 this results in truncation. It must be noted, that large values may have an impact on windows, which do not include these values. Kahan summation is used to compute the rolling sums to preserve accuracy as much as possible.

All windowing operations support a min_periods argument that dictates the minimum amount of non-np.nan values a window must have; otherwise, the resulting value is np.nan. min_peridos defaults to 1 for time-based windows and window for fixed windows
s = pd.Series([np.nan, 1, 2, np.nan, np.nan, 3])
s.rolling(3, min_periods=1).sum()
s.rolling(3, min_periods=1).sum()
s.rolling(3).sum()
s.rolling(window=3, min_periods=2).sum()

# Equivalent to min_periods=3
s.rolling(window=3, min_periods=None).sum()

Additionally, all windowing operations supports the aggregate method for returning a result of multiple aggrega- tions applied to a window.
df = pd.DataFrame({"A": range(5), "B": range(10, 15)})
df.expanding().agg([np.sum, np.mean, np.std])
df.A.expanding(3).corr(df.B)

2.18.2 Rolling window
---------------------
Generic rolling windows support specifying windows as a fixed number of observations or variable number of obser- vations based on an offset. If a time based offset is provided, the corresponding time based index must be monotonic.
times = ['2020-01-01', '2020-01-03', '2020-01-04', '2020-01-05', '2020-01-29']
s = pd.Series(range(5), index=pd.DatetimeIndex(times))
# Window with 2 observations
print(s)
s.rolling(2).sum()

# Window with 2 days worth of observations
s.rolling('2D').sum()

For all supported aggregation functions, see - https://pandas.pydata.org/pandas-docs/stable/reference/window.html#api-functions-rolling

Centering windows
------------------
By default the labels are set to the right edge of the window, but a center keyword is available so the labels can be set at the center.
s = pd.Series(range(10))
s.rolling(5).mean()
s.rolling(5, center=True).mean()

Rolling window endpoints
------------------------
The inclusion of the interval endpoints in rolling window calculations can be specified with the closed parameter:
Value       Behavior
right'      close right endpoint
'left'      close left endpoint
'both'      close both endpoints
'neither'   open endpoints

df = pd.DataFrame({"x": 1},index=[
    pd.Timestamp("20130101 09:00:01"),
    pd.Timestamp("20130101 09:00:02"),
    pd.Timestamp("20130101 09:00:03"),
    pd.Timestamp("20130101 09:00:04"),
    pd.Timestamp("20130101 09:00:06"),
])
df['default']=df.rolling("2s").x.sum() # default
df["right"] = df.rolling("2s", closed="right").x.sum() # default
df["left"] = df.rolling("2s", closed="left").x.sum()
df["both"] = df.rolling("2s", closed="both").x.sum()
df["neither"] = df.rolling("2s", closed="neither").x.sum()
df

Custom window rolling
---------------------
In addition to accepting an integer or offset as a window argument, rolling also accepts a BaseIndexer subclass that allows a user to define a custom method for calculating window bounds. The BaseIndexer subclass will need to define a get_window_bounds method that returns a tuple of two arrays, the first being the starting indices of the windows and second being the ending indices of the windows. Additionally, num_values, min_periods, center, closed and will automatically be passed to get_window_bounds and the defined method must always accept these arguments.
For example, if we have the following :class:DataFrame:
use_expanding = [True, False, True, False, True]
df = pd.DataFrame({"values": range(5)})
and we want to use an expanding window where use_expanding is True otherwise a window of size 1, we can create the following BaseIndexer subclass:
use_expanding = [True, False, True, False, True]
df = pd.DataFrame({"values": range(5)})
df

and we want to use an expanding window where use_expanding is True otherwise a window of size 1, we can create the following BaseIndexer subclass:

from pandas.api.indexers import BaseIndexer
class CustomIndexer(BaseIndexer):
    def get_window_bounds(self, num_values, min_periods, center, closed):
        start = np.empty(num_values, dtype=np.int64)
        end = np.empty(num_values, dtype=np.int64)
        for i in range(num_values):
            if self.use_expanding[i]:
                start[i] = 0
                end[i] = i + 1
            else:
                start[i] = i
                end[i] = i + self.window_size
        return start, end

indexer = CustomIndexer(window_size=1, use_expanding=use_expanding)
df.rolling(indexer).sum()

You can view other examples of BaseIndexer subclasses https://github.com/pandas-dev/pandas/blob/master/pandas/core/window/indexers.py
    New in version 1.1.

One subclass of note within those examples is the VariableOffsetWindowIndexer that allows rolling opera- tions over a non-fixed offset like a BusinessDay.
from pandas.api.indexers import VariableOffsetWindowIndexer

df = pd.DataFrame(range(15), index=pd.date_range('2021-05-19', periods=15))
df

offset = pd.offsets.BDay(1)
indexer = VariableOffsetWindowIndexer(index=df.index, offset=offset)

print(df)
df.rolling(indexer).sum()

For some problems knowledge of the future is available for analysis. For example, this occurs when each data point is a full time series read from an experiment, and the task is to extract underlying conditions. In these cases it can be useful to perform forward-looking rolling window computations. FixedForwardWindowIndexer class is available for this purpose. This BaseIndexer subclass implements a closed fixed-width forward-looking rolling window, and we can use it as follows:

Rolling apply
--------------
The apply() function takes an extra func argument and performs generic rolling computations. The func ar- gument should be a single function that produces a single value from an ndarray input. raw specifies whether the windows are cast as Series objects (raw=False) or ndarray objects (raw=True).

def mad(x):
    return np.fabs(x - x.mean()).mean()

s = pd.Series(range(10))
s.rolling(4).apply(mad, raw=True)

Binary window functions
-----------------------
cov() and corr() can compute moving window statistics about two Series or any combination of DataFrame/Series or DataFrame/DataFrame. Here is the behavior in each case:
• two Series: compute the statistic for the pairing.
• DataFrame/Series: compute the statistics for each column of the DataFrame with the passed Series, thus
returning a DataFrame.
• DataFrame/DataFrame: by default compute the statistic for matching column names, returning a DataFrame. If the keyword argument pairwise=True is passed then computes the statistic for each pair of columns, returning a MultiIndexed DataFrame whose index are the dates in question

df = pd.DataFrame(
....: np.random.randn(10, 4),
....: index=pd.date_range("2020-01-01", periods=10),
....: columns=["A", "B", "C", "D"], ....: )
....:
In [42]: df = df.cumsum() In [43]: df2 = df[:4]
In [44]: df2.rolling(window=2).corr(df2["B"])

Computing rolling pairwise covariances and correlations
In financial data analysis and other fields it’s common to compute covariance and correlation matrices for a collection of time series. Often one is also interested in moving-window covariance and correlation matrices. This can be done by passing the pairwise keyword argument, which in the case of DataFrame inputs will yield a MultiIndexed DataFrame whose index are the dates in question. In the case of a single DataFrame argument the pairwise argument can even be omitted:
Note: Missing values are ignored and each entry is computed using the pairwise complete observations. Please see the covariance section for caveats associated with this method of calculating covariance and correlation matrices.

df = pd.DataFrame(
    np.random.randn(10, 4),
    index=pd.date_range("2020-01-01", periods=10),
    columns=["A", "B", "C", "D"],)

df[["B", "C", "D"]].rolling(window=4).cov(df[["A", "B", "C"]], pairwise=True).dropna()

2.18.3 Weighted window
-----------------------
The win_type argument in .rolling generates a weighted windows that are commonly used in filtering and spectral estimation. win_type must be string that corresponds to a scipy.signal window function. Scipy must be installed in order to use these windows, and supplementary arguments that the Scipy window methods take must be specified in the aggregation function.
s = pd.Series(range(10))
s.rolling(window=5).mean()
s.rolling(window=5, win_type="triang").mean()

# Supplementary Scipy arguments passed in the aggregation function
In [50]: s.rolling(window=5, win_type="gaussian").mean(std=0.1)

https://pandas.pydata.org/pandas-docs/stable/reference/window.html#api-functions-window

Expanding window
An expanding window yields the value of an aggregation statistic with all the data available up to that point in time. Since these calculations are a special case of rolling statistics, they are implemented in pandas such that the following two calls are equivalent:

In [51]: df = pd.DataFrame(range(5))
In [52]: df.rolling(window=len(df), min_periods=1).mean()
In [53]: df.expanding(min_periods=1).mean()
https://pandas.pydata.org/pandas-docs/stable/reference/window.html#api-functions-expanding




====================================================================================
====================================================================================
Attributes:
pd.Index.name
.array - get the series or index in the form of Extension array
.cat.categories
df.T
.freqstr
.freq
.dt.month, .dt.year. .dt.hour, .dt.day, .dt.weekday
.index
columns
shape
dtypes - get the datatypes of a df
dtype - get the datatype of a series
loc[] - get values based on label
iloc[] - get values based on index
at[] - whenever you need a scalar value go with at or iat. use at in case of labels. It is faster.
iat[] - get the scalar values based on index

Methods:
TimeSeries:
Period Index is capable of storing values outside Timestamp(dt64[ns]) daterange.
    pd.Timestamp.max # Timestamp('2262-04-11 23:47:16.854775807')
    pd.Timestamp.min # Timestamp('1677-09-21 00:12:43.145225')

ts.resample() - resample Timestamp to a different frequency.
tz_convert('US/Pacific') - covert timezone to a different timezone.
tz_localize('Europe/Berlin') - Localize time of a DateTimeIndex.
pd.date_range('date', periods=, freq=, tz=) - generate date starting from given dates with n periods for provided freq.
pd.bdate_range(start, end, period, freq) - generate business date range for provided start and end date.
pd.Timestamp() - provide a date in any other format and it will convert it o to dt64[ns]
to_datetime() - converts a list of datetime objects like np.datetime64, 'date', datetime.datetime to pandas Timestamp object.
pd.DatetimeIndex(dates) - Converts the provided dates to DatetimeIndex.
pd.Index(dates) - will also convert the dates to DatetimeIndex.
pd.Period() - define a timespan in frequency, eg -pd.Period('2020.01.01', freq='M') # Period('2020-01', 'M')
to_period(dateTime) - convert datetime objects to periods with provided frequency.
pd.period_range(start='2020.01.01', end='2020.01.03') - returns Period Index with period type and necessary freq.
df.truncate(before=, after=) fetch values based on ts, after values are not provided in output.
idx.isocalendar() - converts datetime index to iso format providing year, week, day
.day_name() - provides the day for a given date

## rollforwarrd and rollbackward methods are present only with offsets and not timedelta.
offset.rollforward(ts) - rollforward a date offset from the given timeseries
offset.rollbackward(ts) - rollforward a date offset from the given timeseries

timestamp.normalize() - normalize a given timestamp to 00:00.
ts.shift() - used to shift index or values forward or backward.
ts.asfreq() - can be used as reindex to fill missing timestamps and can also be used to fill values for nans.

pd.to_timedelta() - convert a scaler, series to Timedelta and TimedeltaIndex type respectively.
pf.Timedelta('delta') - convert a timedelta, string ,np.timedelta64 object to Timedelta.

s.cat.set_category() - to set a category i.e to remove and add new category have performance benefits
s.cat.remove_unused_category() - to remove unused category
s.cat.add_category() - to add a category
s.cat.remove_category() - to remove a category
s.cat.rename_category() - to rename a category
s.cat.ordered - returns boolean telling that the category is ordered or not.
s.cat.categories - to get all the categories in a series
pd.Categorical(categories=[]) - create a categorical dtype
convert_dtypes() - convert object dtype to string dtype
notna() - boolean True if the value in the cell is na
isna() - boolean True if the value in the cell is na
.str.fullmatch(pattern) - element matches a pattern from the start of the string till end of the string.
.str.match(pattern) - element matches a pattern from the start of the string.
.str.contains(pattern) - elements contains a pattern, return bool
.str.extractall() - extracts all groups from a string and returns a multiindex dataframe.
.str.extract(expand=True) - extract elements from a series or df based on regex. if expand=False then returns Series else DF.
.str.cat(sep=, na_rep=) - concatenate a series with other series or series like objects of string type.
.str.lower, upper, len, replace(with regex), split - string methods
factorize() - similar to np.unique to get the unique values and their index from a series/df.col
get_dummies(columnName) - Similar to enumeration in kdb, it replaces the categorical values with their index.
pd.cut(data, bins) - provide the bins for present data, look for example if not clear.
crosstab(idex, col, values, aggfunc, margin, normalize) - Compute a simple cross tabulation of two (or more) factors. By default computes a frequency table of the factors unless an array of values and an aggregation function are passed.
melt(id_vars, value_vars, vars_name, value_name) - melt a dataframe from width to height.
unstack() - opposite of stack() i.e height to width on multiindex df
stack() - opposite to pivot i.e width to height on multiindex df
df.pivot_table(index, column, values, aggfunc) - apply pivot tables as in spread sheets.
pivot(index, columns, values) - move from height to width form
df.compare(df2, align_axis=0, keep_shape=True) - compare two dfs and get only the difference.
pd.merge_asof(trades, quotes, on='time', by='ticker', tolerance="pd.TimeDelta('5ms')", allow_exact_matches=False) - merge timeseries dfs on previous time match on the left df.
pd.merge_ordered - same as merge but on ordered data like timeseries, default outer.
pd.join(df1, df2) - join on indexes, rest same as merge, default - left.
pd.merge(df1, df2, on=, how=, left_on, suffixes, sort) - merge two dfs on inner, outer, left, right join. default inner.
df1.append(df2) - append row of one df to another, short for concat on axis=0.
pd.concat([df1,df2], on, how)- concatenate two dataframes
take() - fetch rows and columns from a df, much faster than loc
is_lexsorted - check if the multi index dataframe has lex sorted index
df.reorder_level - similar to the swap level to reorder levels in df
df.swap_level() - Change the row and column index of dataframe
pd.MultiIndex.from_arrays(), from_tuples(), from_product(), from_frame() - create a Multiindex from any of the sequences.
df.get_option() - get current option for SettingsWithCopyWarning.
df.set_option("mode.chained_assignment", "raise"|"warn";"None") - set option for SettingsWithCopyWarning.
union() - set union, all values from both indexes
symmetrical_difference() - uncommon values from both index
difference() - set difference between two index
pd.Index.rename() - rename a index, returns a copy
pd.Index.set_levels() - assign levels to multiindex
pd.Index.set_names() - assign name to Index
df.drop_duplicates(keep=first(default)|last|False) - return df with dropped duplicates.
df.duplicated(keep=first(default)|last|False) - return boolean with the same length of df stating which elements are duplicated.
np.select(condition, choices, default=) - can be used for more than one if condition
np.where(condition,true,false) - can be used for if else conditions in the dataframe
df.mask() - exactly opposite of df.where
np.where(condition, df, replacement) - apply condition on a df and replace it with values in replacement, faster than df.where
df.where(condition, replacement, axis) - apply condition on a df and replace it with values in replacement
sample(n, frac, weights, random_state) - get sample from a dataframe
df.columns.get_indexers(['A', 'B']) - get multiple columns from a dataframe
df.columns.get_loc('A') - get columns of a dataframe
select_dtypes(include=[], exclude=[]) - select columns of particular datatype from a df
to_datetime(), to_timedelta() - similar to to_numeric() but for datetime and timedelta type.
to_numeric(errors='raise|coerce', downcast='integer|float') - convert a series, array to numeric value of certain type.
astype() - convert a series, df column to other datatype
copy() - copy() the dataframe and series, it is used for deepcopy.
nsmallest(), nlargest() - get n smallest or n largest numbers from a series or df.
searchsorted() - in a sorted array tell the index where the value can be placed to maintain the sorting order. It is similar to np.searchsorted().
sort_values() - sort values by columns, custom functions can be applied with key argument.
sort_index() - sort index, row wise or column wise or both.
itertuples() - iterates over the rows, return namedTuple of index, colValues of a df
iterrows() - iterates over the rows, return index, colValues of a df
items() - similar to dict, return index, colValues of a df
rename_axis() - rename the name of axis labels i.e index and column for multiindex df.
rename() - rename the name of axis labels i.e index and column.
align() - to get the values of both df, series in the form of tuple by join across various options.
reindex_like() - change the shape of a df, series similar to another df, series
reindex() - reindex the index or columns, it is faster than normal rearrange of columns.
applymap(), map() - since all functions are not vectorized, these functions are applied on elements. applymap on dict and map on series.
.transform() - aggregating the data, similar to agg but the index remains same
.agg(), .aggregate() -- aggregate the series or dataframe based on the function provided
.apply() -- apply the function row-wise or column-wise.
.pipe() -- apply the function on a dataframe by passing the dataframe as an arg to the function.
cut(), qcut() - discretize and quantile continuous data
mode() - get the most occurring value in a series or df
duplicated() - get the duplicated rows in a df
value_counts() - count the occurrence of an element in a series or df
idxmin(), idxmax() - get the index of the min and max value in the series or dataframe
describe([percentiles=[], include=[]) - summary desc stats, you can modify the percentiles as per your needs.
nunique - provide the count of the unique elements in a series/df excluding np.nan
combine - combine two dataframes based on a provided function
combine_first() - get the non-NAN values from both df while merging with same column names
df1.update(df2) - same as combine_first with a difference that this updates df1 with result in place.
gt(),lt(),ne() - greater than, less than, not equal
divmod() - provide the quotient and remainder in the form of a tuple
equals() - compare NDFrames at once, == will return boolean values per element
fill_value - fill the nan with a value within a method
.fillna() - fill the nan with a value
pd.set_option() - console width, height. Not use any dependent lib.
np.asarray(s,df) - convert a dataframe or series to numpy
to_numpy() - convert a dataframe or series to numpy
query() - query a dataframe, condition/filter can be applied on df
assign() - assign a column to a df using calculation or lambda
insert(position_index, column_name, value) - insert a column at a particular position
rename() - rename a series
.cat.set_categories()
period.to_timestamp - get the first day of the month
timestamp.to_period - convert date to the freq like in Months
tz_convert - convert timezone to another timezone
tz_localize - specify timezone of a particular country
pd.MultiIndex.from_tuples() 
stack()
df.rsub - opposite of sub(), first apply sub and then multiply values with -1
df.sub() - subtract
isna()
date_range('date', periods=, freq=) - generate date starting from given dates with n periods for provided freq.
idxmax() - get the max index
replace({from:to})
read_csv(parse_dates=['colName'])
to_datetime() - converts a list of datetime objects like np.datetime64, 'date', datetime.datetime to pandas Timestamp object.
merge(,,how, left_on, right_on)
concat(, axis=, keys=)
melt(id_var=, value_var=, value_name=, var_name=)
set_index()
reset_index()
pivot_table(colums=, values=, index=, aggfunc=, margins=)
pivot(columns=, values=)
size()
count()
sort_index()
sort_values()
value_counts()
mean(), median(), mode(), skew(), min(), max()
groupby().aggregateFunction()
agg({col=[fn1, fn2]}) - aggreagate the data based on columns of a dataframe
rename(columns={'currentCol':'newCol'})
fig.savefig()
plt.subplot()
plot()
notna()
isin()
max()
head()
tail()
describe()
info()



==================2.19 Timeseries / date functionality=========================
pandas contains extensive capabilities and features for working with time series data for all domains. Using the NumPy datetime64 and timedelta64 dtypes, pandas has consolidated a large number of features from other Python libraries like scikits.timeseries as well as created a tremendous amount of new functionality for manipulating time series data.
For example, pandas supports:
Parsing time series information from various sources and formats
dti = pd.to_datetime(
    ['1.1.2018', np.datetime64('2018-01-01'), datetime.datetime(2018,1,1)]
)

Generate sequences of fixed-frequency dates and time spans:
dti = pd.date_range('2018-01-01', periods=3, freq='2D')
dti = pd.date_range('2018-01-01', periods=3, freq='4H')

Manipulating and converting date times with timezone information
dti.tz_localize('UTC')
dti.tz_localize('Europe/Berlin')
dti.tz_convert("US/Pacific")
dti.tz_convert('US/Central')

Resampling or converting a time series to a particular frequency
idx = pd.date_range("2018-01-01", periods=5, freq="H")
ts = pd.Series(range(len(idx)), index=idx)
print(ts)
ts.resample("2H").count()
print(ts.resample("2H").mean())
ts.resample("2H").median()

Performing date and time arithmetic with absolute or relative time increments
friday = pd.Timestamp("2018-01-05")
friday.day_name()
saturday = friday + pd.Timedelta("1 day")
saturday.day_name() # 'Saturday'

# Add 1 business day (Friday --> Monday)
monday = friday + pd.offsets.BDay()
(friday + pd.offsets.Day()).day_name()
monday.day_name()
pandas provides a relatively compact and self-contained set of tools for performing the above tasks and more.

2.19.1 Overview
pandas captures 4 general time related concepts:
1. Date times: A specific date and time with timezone support. Similar to datetime.datetime from the standard library.
2. Time deltas: An absolute time duration. Similar to datetime.timedelta from the standard library.
3. Time spans: A span of time defined by a point in time and its associated frequency.
4. Date offsets: A relative time duration that respects calendar arithmetic. Similar to dateutil. relativedelta.relativedelta from the dateutil package.

Concept     Scalar Class    Array Class             pandas Data Type                    Primary Creation Method
Date times  Timestamp       DatetimeIndex     datetime64[ns] or datetime64[ns, tz]  to_datetime or date_range
Time deltas Timedelta       TimedeltaIndex    timedelta64[ns]                       to_timedelta or timedelta_range
Time spans  Period          PeriodIndex       period[freq]                           Period or period_range
Date offsets DateOffset     None              None                                   DateOffset

For time series data, it’s conventional to represent the time component in the index of a Series or DataFrame so manipulations can be performed with respect to the time element.
s = pd.Series(range(3), index=pd.date_range('2020', periods=3))

However, Series and DataFrame can directly also support the time component as data itself.
pd.Series(pd.date_range('2020', periods=3))

Series and DataFrame have extended data type support and functionality for datetime, timedelta and Period data when passed into those constructors. DateOffset data however will be stored as object data.
pd.Series(pd.period_range('1/1/2021', freq='M', periods=3))
pd.Series([pd.DateOffset(1), pd.DateOffset(2)])
pd.Series(pd.date_range("1/1/2011", freq="M", periods=3))

Lastly, pandas represents null date times, time deltas, and time spans as NaT which is useful for representing missing or null date like values and behaves similar as np.nan does for float data.
pd.Timestamp(pd.NaT)
pd.Timedelta(pd.NaT)
pd.Period(pd.NaT)

-- Problem --
Stackoverflow - https://stackoverflow.com/questions/67225547/pandas-create-a-new-column-with-complex-condition-of-multiple-other-columns/67239962#67239962
 df=pd.DataFrame({'Date':['2020-01-01', '2020-01-23', '2020-02-22', '2020-03-04'],  'ID':[1001,1002,1002,1003]})
I need a table with extra column Wed, that gives me last Wednesday of Date, and if ID is 1002, returns 2 Wednesday before that Date

Solution -
df["Date"] = pd.to_datetime(df["Date"])
df['Wed'] = np.where(df.ID == 1002,
         df.Date - pd.offsets.Week(3, weekday=2),
         df.Date - pd.offsets.Week(weekday=2))

-------------------------------------------

2.19.2 Timestamps vs. time spans
================================
Timestamped data is the most basic type of time series data that associates values with points in time. For pandas objects it means using the points in time.
pd.Timestamp("2012-05-01")
pd.Timestamp(datetime.datetime(2012, 5, 1))
pd.Timestamp(2012, 5, 1)

However, in many cases it is more natural to associate things like change variables with a time span instead. The span represented by Period can be specified explicitly, or inferred from datetime string format.
For example:
pd.Period("2011-01")
pd.Period("2011.01", 'D')
pd.Period("2011.01", freq='D')

Timestamp and Period can serve as an index. Lists of Timestamp and Period are automatically coerced to DatetimeIndex and PeriodIndex respectively.
dates = pd.date_range('2020/01/01', periods=3)
ts = pd.Series(np.arange(3), dates)
ts.index
type(ts.index) # pandas.core.indexes.datetimes.DatetimeIndex

periods = pd.date_range('2021-01', periods=3, freq='M')
DatetimeIndex(['2021-01-31', '2021-02-28', '2021-03-31'], dtype='datetime64[ns]', freq='M')
periods = periods.to_period()
type(periods) # pandas.core.indexes.period.PeriodIndex
ts = pd.Series(np.arange(3), periods)
ts.index # PeriodIndex(['2021-01', '2021-02', '2021-03'], dtype='period[M]', freq='M')
type(ts.index) #pandas.core.indexes.period.PeriodIndex

pandas allows you to capture both representations and convert between them. Under the hood, pandas represents timestamps using instances of Timestamp and sequences of timestamps using instances of DatetimeIndex. For regular time spans, pandas uses Period objects for scalar values and PeriodIndex for sequences of spans. Better support for irregular intervals with arbitrary start and end points are forth-coming in future releases.


Key Takeaways from TimeStamps and Period:
------------------------------------------
Timestamp = pd.Timestamp()
    range - pd.to_datetime(), pd.date_range()
    Index - DateTimeIndex(under the hood for sequence of Timestamps)
Timespan - pd.Period()
    convert from timestamp to timespan - ts.to_period()
    Index - PeriodIndex


2.19.3 Converting to timestamps
--------------------------------
To convert a Series or list-like object of date-like objects e.g. strings, epochs, or a mixture, you can use the to_datetime function. When passed a Series, this returns a Series (with the same index), while a list-like is converted to a DatetimeIndex:

pd.to_datetime(pd.Series(["Jul 31, 2020","2010-01-10", None]))
pd.to_datetime(["Jul 31, 2020","2010-01-10", None]) # DatetimeIndex(['2020-07-31', '2010-01-10', 'NaT'], dtype='datetime64[ns]', freq=None)

If you use dates which start with the day first (i.e. European style), you can pass the dayfirst flag:
pd.to_datetime('04-01-2021',dayfirst=True)
pd.to_datetime('04-18-2021',dayfirst=True) # Timestamp('2021-04-18 00:00:00')

 Warning: You see in the above example that dayfirst isn’t strict, so if a date can’t be parsed with the day being first it will be parsed as if dayfirst were False.

 If you pass a single string to to_datetime, it returns a single Timestamp. Timestamp can also accept string input, but it doesn’t accept string parsing options like dayfirst or format, so use to_datetime if these are required.
 pd.to_datetime("2010/11/12")
 pd.Timestamp("2010/11/12")

 You can also use the DatetimeIndex constructor directly:
 pd.DatetimeIndex(["2018-01-01", "2018-01-03", "2018-01-05"])

The string ‘infer’ can be passed in order to set the frequency of the index as the inferred frequency upon creation:
pd.DatetimeIndex(["2018-01-01", "2018-01-03", "2018-01-05"], freq="infer") # DatetimeIndex(['2018-01-01', '2018-01-03', '2018-01-05'], dtype='datetime64[ns]', freq='2D')
pd.DatetimeIndex(["2018-01-01", "2018-01-03", "2018-01-06"], freq="infer") # DatetimeIndex(['2018-01-01', '2018-01-03', '2018-01-06'], dtype='datetime64[ns]', freq=None)

Providing a format argument
-----------------------------
In addition to the required datetime string, a format argument can be passed to ensure specific parsing. This could also potentially speed up the conversion considerably.
pd.to_datetime("2020/11/12", format="%Y/%m/%d")
pd.to_datetime("12-11-2010 00:00", format="%d-%m-%Y %H:%M")

For more information on the choices available when specifying the format option -https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior

Assembling datetime from multiple DataFrame columns
----------------------------------------------------
You can also pass a DataFrame of integer or string columns to assemble into a Series of Timestamps.
df = DataFrame({
    'year': [2021, 2021],
    'month': [4,5],
    'day':[30, 1],
    'hours':[23, 1]
})
pd.to_datetime(df)
0   2021-04-30 23:00:00
1   2021-05-01 01:00:00
dtype: datetime64[ns]

You can pass only the columns that you need to assemble.
pd.to_datetime(df[['year', 'month','day']])

pd.to_datetime looks for standard designations of the datetime component in the column names, including:
• required: year, month, day
• optional: hour, minute, second, millisecond, microsecond, nanosecond

pd.to_datetime(df[['year', 'month']]) # ValueError: to assemble mappings requires at least that [year, month, day] be specified: [day] is missing

Invalid data
The default behavior, errors='raise', is to raise when unparsable:
pd.to_datetime(['2020/01/01', '2021.01.02', 'asd']) # ParserError: Unknown string format: asd
pd.to_datetime(['2020/01/01', '2021.01.02', 'asd'], errors='raise') # ParserError: Unknown string format: asd

Pass errors='ignore' to return the original input when unparsable:
pd.to_datetime(['2020/01/01', '2021.01.02', 'asd'], errors='ignore')

Pass errors='coerce' to convert unparsable data to NaT (not a time):
pd.to_datetime(['2020/01/01', '2021.01.02', 'asd'], errors='coerce')

Epoch timestamps
=================
pandas supports converting integer or float epoch times to Timestamp and DatetimeIndex. The default unit is nanoseconds, since that is how Timestamp objects are stored internally. However, epochs are often stored in another unit which can be specified. These are computed from the starting point specified by the origin parameter.
pd.to_datetime([1349720105, 1349806505, 1349892905,
                1349979305, 1350065705], unit="s")
pd.to_datetime([1349720105100, 1349720105200, 1349720105300,
                1349720105400,1349720105500], unit='ms')
pd.to_datetime([1618866007161886600, 1618866007161886601], unit='ns')
pd.to_datetime([1618866007161886600, 1618866007161886601]) ### Default of unit is ns

Note: The unit parameter does not use the same strings as the format parameter that was discussed above).The available units are listed on the documentation for pandas.to_datetime(). -- https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html

Constructing a Timestamp or DatetimeIndex with an epoch timestamp with the tz argument specified will raise a ValueError. If you have epochs in wall time in another timezone, you can read the epochs as timezone-naive timestamps and then localize to the appropriate timezone:
pd.Timestamp(1262347200000000000).tz_localize("US/Pacific")
Timestamp('2010-01-01 12:00:00-0800', tz='US/Pacific')

Note: Epoch times will be rounded to the nearest nano second.
Warning: Conversion of float epoch times can lead to inaccurate and unexpected results. Python floats have about 15 digits precision in decimal. Rounding during conversion from float to high precision Timestamp is unavoidable. The only way to achieve exact precision is to use a fixed-width types (e.g. an int64).
pd.to_datetime([1490195805.433, 1490195805.433502912], unit="s")
pd.to_datetime(1490195805433502912, unit="ns")

If provided float precision, it might lead to disaster.
pd.to_datetime([1618866007.161886600, 1618866007161886601]) ## First element is float
DatetimeIndex(['1970-01-01 00:00:01.618866007', '2021-04-19 21:00:07.161886601'], dtype='datetime64[ns]', freq=None)

From timestamps to epoch
------------------------
To invert the operation from above, namely, to convert from a Timestamp to a ‘unix’ epoch:
We subtract the epoch (midnight at January 1, 1970 UTC) and then floor divide by the “unit” (1 second).
stamps = pd.date_range("2012-10-08 18:15:05", periods=4, freq="D")
(stamps - pd.Timestamp("1970-01-01")) // pd.Timedelta("1s") ## Timedelta unit defaults to ns

Using the origin Parameter
--------------------------
Using the origin parameter, one can specify an alternative starting point for creation of a DatetimeIndex. For
example, to use 1960-01-01 as the starting date:
pd.to_datetime([1, 2, 3], unit="D", origin=pd.Timestamp("1960-01-01"))
pd.to_datetime([0, 1, 2], unit="D", origin=pd.Timestamp("2000-01-01"))  ## kdb date origin.

The default is set at origin='unix', which defaults to 1970-01-01 00:00:00. Commonly called ‘unix epoch’ or POSIX time.
pd.to_datetime([1, 2, 3], unit="D")

2.19.4 Generating ranges of timestamps
---------------------------------------
To generate an index with timestamps, you can use either the DatetimeIndex or Index constructor and pass in a list of datetime objects:
dates = [
    datetime.datetime(2020,1,3),
    datetime.datetime(2020,1,4),
    datetime.datetime(2020,1,5),
]
dates
# Note the frequency information
index = pd.DatetimeIndex(dates)
index # DatetimeIndex(['2020-01-03', '2020-01-04', '2020-01-05'], dtype='datetime64[ns]', freq=None)

# Automatically converted to DatetimeIndex
index = pd.Index(dates)
index # DatetimeIndex(['2020-01-03', '2020-01-04', '2020-01-05'], dtype='datetime64[ns]', freq=None)

In practice this becomes very cumbersome because we often need a very long index with a large number of timestamps. If we need timestamps on a regular frequency, we can use the date_range() and bdate_range() functions to create a DatetimeIndex. The default frequency for date_range is a calendar day while the default for bdate_range is a business day:

start = datetime.datetime(2011, 1, 1)
end = datetime.datetime(2012, 1, 1)
pd.date_range(start, end, freq='M')
pd.period_range(start, end, freq='M')
pd.bdate_range(start, end, freq='M')

Convenience functions like date_range and bdate_range can utilize a variety of frequency aliases:
pd.date_range(start, periods=5, freq='QS')
pd.bdate_range(start, periods=5, freq='BQS')
pd.period_range(start, periods=4, freq='W-MON')

date_range and bdate_range make it easy to generate a range of dates using various combinations of parame- ters like start, end, periods, and freq. The start and end dates are strictly inclusive, so dates outside of those specified will not be generated:
pd.date_range(start, end, freq="BM") ## value in freq takes priority
pd.bdate_range(start, end, freq="M")
pd.bdate_range(end=end, periods=3, freq='BM')
pd.bdate_range(start=start, periods=20)

Specifying start, end, and periods will generate a range of evenly spaced dates from start to end inclusively, with periods number of elements in the resulting DatetimeIndex:
pd.date_range("2018-01-01", "2018-01-05", periods=5)

Custom frequency ranges
------------------------
bdate_range can also generate a range of custom frequency dates by using the weekmask and holidays parameters. These parameters will only be used if a custom frequency string is passed.
weekmask = "Mon Wed Fri"
holidays = [datetime.datetime(2011, 1, 5), datetime.datetime(2011, 3, 14)]
pd.bdate_range(datetime.datetime(2011, 1, 1),
              datetime.datetime(2011, 4, 30), freq='C',
             weekmask=weekmask, holidays=holidays)

-----------------------------------
Convert dates from dd MMM YYYY format to datetime
    pd.to_datetime('10 Feb 2021', format="%d %b %Y")

Add days to existing datetime
    pd.to_datetime('10.02.2021', dayfirst=True) + pd.DateOffset(days=84)

Convert a julian date to normal date
df = pd.DataFrame({'Date': ['16234', '16003']})
df['Date'] = pd.to_datetime(df['Date'], format='%y%j')

Convert Integer days to timedelta and then divide by years
df['age_in_years'] = pd.to_timedelta(df.age_in_day, unit='D') / np.timedelta64(1,'Y')
------------------------------------

2.19.5 Timestamp limitations
-----------------------------
Since pandas represents timestamps in nanosecond resolution, the time span that can be represented using a 64-bit integer is limited to approximately 584 years:
pd.Timestamp.min # Timestamp('1677-09-21 00:12:43.145225')
pd.Timestamp.max # Timestamp('2262-04-11 23:47:16.854775807')

2.19.6 Indexing
----------------
One of the main uses for DatetimeIndex is as an index for pandas objects. The DatetimeIndex class contains many time series related optimizations:
• A large range of dates for various offsets are pre-computed and cached under the hood in order to make gener- ating subsequent date ranges very fast (just have to grab a slice).
• Fast shifting using the shift method on pandas objects.
• Unioning of overlapping DatetimeIndex objects with the same frequency is very fast (important for fast
data alignment).
• Quick access to date fields via properties such as year, month, etc.
• Regularization functions like snap and very fast asof logic.
DatetimeIndex objects have all the basic functionality of regular Index objects, and a smorgasbord of advanced time series specific methods for easy frequency processing.

Note: While pandas does not force you to have a sorted dateindex,some of these methods may have unexpected or incorrect behavior if the dates are unsorted.

DatetimeIndex can be used like a regular index and offers all of its intelligent functionality like selection, slicing, etc.
rng = pd.bdate_range(start, end, freq="BM")
ts = pd.Series(np.random.randn(len(rng)), index=rng)
ts[:5]
ts[:5].index
ts[::2].index

Partial string indexing
-----------------------
Dates and strings that parse to timestamps can be passed as indexing parameters:
ts["1/31/2011"]
ts[datetime.datetime(2011, 12, 25):]
ts["10/31/2011":"12/31/2011"]

To provide convenience for accessing longer time series, you can also pass in the year or year and month as strings:
ts['2011']
ts['2011-5']

This type of slicing will work on a DataFrame with a DatetimeIndex as well. Since the partial string selection is a form of label slicing, the endpoints will be included. This would include matching times on an included date:

Warning: Indexing DataFrame rows with a single string with getitem (e.g. frame[dtstring]) is depre- cated starting with pandas 1.2.0 (given the ambiguity whether it is indexing the rows or selecting a column) and will be removed in a future version. The equivalent with .loc (e.g. frame.loc[dtstring]) is still supported.

df = ts.to_frame()
df.rename(columns={(0):'col'}, inplace=True)
df['2011-01-31'] # KeyError: '2011-01-31'
df.loc['2011-01-31'] # col   -0.100624
dft.loc['2013']

This starts on the very first time in the month, and includes the last date and time for the month:
dft["2013-1":"2013-2"]
This specifies a stop time that includes all of the times on the last day:
dft["2013-1":"2013-2-28"]
This specifies an exact stop time (and is not the same as the above):
dft["2013-1":"2013-2-28 00:00:00"]
We are stopping on the included end-point as it is part of the index:
dft["2013-1-15":"2013-1-15 12:30:00"]
DatetimeIndex partial string indexing also works on a DataFrame with a MultiIndex:
dft2 = pd.DataFrame(
    np.random.randn(20,1),
    columns=['A'],
    index=pd.MultiIndex.from_product([
        pd.date_range("2020.01.01", periods=10, freq='12H'),
        ['a','b']
    ])
)
dft2.loc['20200105']
idx = pd.IndexSlice
dft2 = dft2.swaplevel(0,1).sort_index()
dft2.loc[idx[:,'20200105'], :]
dft2.loc[idx['a','20200105'], :]

New in version 0.25.0.
Slicing with string indexing also honors UTC offset.

df = pd.DataFrame([0],
        index=pd.DatetimeIndex(["2019-01-01"],tz="US/Pacific"))
df["2019-01-01 12:00:00+04:00":"2019-01-01 13:00:00+04:00"]

Slice vs. exact match
----------------------
The same string used as an indexing parameter can be treated either as a slice or as an exact match depending on the resolution of the index. If the string is less accurate than the index, it will be treated as a slice, otherwise as an exact match.
Consider a Series object with a minute resolution index:

series_minute = pd.Series(np.arange(1,4),
            pd.DatetimeIndex(["2011-12-31 23:59:00", "2012-01-01 00:00:00",
                              "2012-01-01 00:02:00"],))
series_minute.index.resolution # 'minute'
# A timestamp string less accurate than a minute gives a Series object.
series_minute['2011-12-31 23'] # 2011-12-31 23:59:00

A timestamp string with minute resolution (or more accurate), gives a scalar instead, i.e. it is not casted to a slice.
series_minute["2011-12-31 23:59"] # 1
series_minute["2011-12-31 23:59:00"] # 1
series_minute['2012-01-01 00:00'] # 2

If index resolution is second, then the minute-accurate timestamp gives a Series.
series_second = pd.Series([1, 2, 3],pd.DatetimeIndex(
    ["2011-12-31 23:59:59", "2012-01-01 00:00:00", "2012-01-01 00:00:01"],
     ))
series_second.index.resolution # 'second'
series_second["2011-12-31 23:59"] # 2011-12-31 23:59:59    1
series_second["2011-12-31 23:59:59"] # 1

If the timestamp string is treated as a slice, it can be used to index DataFrame with .loc[] as well.
dft_minute = pd.DataFrame(
    {"a": [1, 2, 3], "b": [4, 5, 6]}, index=series_minute.index
)
dft_minute.loc['2012-01-01 00:02']

Warning: However,if the string is treated as an exact match,the selection in DataFrame’s [] will be column- wise and not row-wise, see Indexing Basics. For example dft_minute['2011-12-31 23:59'] will raise KeyError as '2012-12-31 23:59' has the same resolution as the index and there is no column with such name:
To always have unambiguous selection, whether the row is treated as a slice or a single selection, use .loc.

dft_minute.loc["2011-12-31 23:59"]

Note also that DatetimeIndex resolution cannot be less precise than day.
series_monthly = pd.Series(
    [1, 2, 3], pd.DatetimeIndex(["2011-12", "2012-01", "2012-02"])
)
series_monthly.index.resolution # 'day'
series_monthly["2011-12"] # returns Series - 2011-12-01
series_monthly["2011-12-01"] # returns scaler

Exact indexing
---------------
As discussed in previous section, indexing a DatetimeIndex with a partial string depends on the “accuracy” of the period, in other words how specific the interval is in relation to the resolution of the index. In contrast, indexing with Timestamp or datetime objects is exact, because the objects have exact meaning. These also follow the semantics of including both endpoints.
These Timestamp and datetime object shave exact hours, minutes,and seconds,even though they were not explicitly specified (they are 0).
dft[datetime.datetime(2013, 1, 1): datetime.datetime(2013, 2, 28)]

With no defaults.
dft[datetime.datetime(2013, 1, 1, 10, 12, 0): datetime.datetime(2013, 2, 28, 10, 12, 0)]

Truncating & fancy indexing
----------------------------
A truncate() convenience function is provided that is similar to slicing. Note that truncate assumes a 0 value for any unspecified date component in a DatetimeIndex in contrast to slicing which returns any partially matching dates:

rng2 = pd.date_range("2020-01-01", "2021-01-01", freq="W")
rng2
ts2 = pd.Series(np.random.randn(len(rng2)), index=rng2)
ts2.truncate(before='2020-1', after='2020-02') # after month is not included
ts2['2020-01':'2020-02']

Even complicated fancy indexing that breaks the DatetimeIndex frequency regularity will result in a DatetimeIndex, although frequency is lost:
ts2[[0,2,6]]
ts2[[0,2,6]].index # Freq is lost
ts2[[0,2,4]].index # Freq is present

2.19.7 Time/date components
----------------------------
There are several time/date properties that one can access from Timestamp or a collection of timestamps like a DatetimeIndex.

Property    Description
year        The year of the datetime
month       The month of the datetime
day         The days of the datetime
hour        The hour of the datetime
minute      The minutes of the datetime
second      The seconds of the datetime
microsecond     The microseconds of the datetime
nanosecond      The nanoseconds of the datetime
date            Returns datetime.date (does not contain timezone information)
time            Returns datetime.time (does not contain timezone information)
timetz          Returns datetime.time as local time with timezone information
dayofyear       The ordinal day of year
day_of_year     The ordinal day of year
weekofyear      The week ordinal of the year
week            The week ordinal of the year
dayofweek       The number of the day of the week with Monday=0, Sunday=6
day_of_week     The number of the day of the week with Monday=0, Sunday=6
weekday         The number of the day of the week with Monday=0, Sunday=6
quarter         Quarter of the date: Jan-Mar = 1, Apr-Jun = 2, etc.
days_in_month   The number of days in the month of the datetime
is_month_start  Logical indicating if first day of month (defined by frequency)
is_month_end    Logical indicating if last day of month (defined by frequency)
is_quarter_start    Logical indicating if first day of quarter (defined by frequency)
is_quarter_end      Logical indicating if last day of quarter (defined by frequency)
is_year_start       Logical indicating if first day of year (defined by frequency)
is_year_end         Logical indicating if last day of year (defined by frequency)
is_leap_year        Logical indicating if the date belongs to a leap year

Furthermore, if you have a Series with datetimelike values, then you can access these properties via the .dt accessor,
You may obtain the year, week and day components of the ISO year from the ISO 8601 standard:
idx = pd.date_range(start="2019-12-29", freq="D", periods=4)
idx.isocalendar()
idx.to_series().dt.isocalendar()

2.19.8 DateOffset objects
--------------------------
In the preceding examples, frequency strings (e.g. 'D') were used to specify a frequency that defined: • how the date times in DatetimeIndex were spaced when using date_range()
• the frequency of a Period or PeriodIndex
These frequency strings map to a DateOffset object and its subclasses. A DateOffset is similar to a Timedelta that represents a duration of time but follows specific calendar duration rules. For example, a Timedelta day will always increment datetimes by 24 hours, while a DateOffset day will increment datetimes to the same time the next day whether a day represents 23, 24 or 25 hours due to daylight savings time. However, all DateOffset subclasses that are an hour or smaller (Hour, Minute, Second, Milli, Micro, Nano) behave like Timedelta and respect absolute time.
The basic DateOffset acts similar to dateutil.relativedelta (relativedelta documentation) that shifts a date time by the corresponding calendar duration specified. The arithmetic operator (+) or the apply method can be used to perform the shift.

# This particular day contains a day light savings time transition
In [144]: ts = pd.Timestamp("2016-10-30 00:00:00", tz="Europe/Helsinki")
# Respects absolute time
In [145]: ts + pd.Timedelta(days=1)
Out[145]: Timestamp('2016-10-30 23:00:00+0200', tz='Europe/Helsinki')
# Respects calendar time
In [146]: ts + pd.DateOffset(days=1)
Out[146]: Timestamp('2016-10-31 00:00:00+0200', tz='Europe/Helsinki')

friday = pd.Timestamp('2021.05.07')
friday.day_name()

# Add 2 business days (Friday --> Tuesday)
two_business_days = 2 * pd.offsets.BDay()
two_business_days.apply(friday)
two_business_days + friday
(two_business_days + friday).day_name()

Most DateOffsets have associated frequencies strings, or offset aliases, that can be passed into freq keyword arguments. The available date offsets and associated frequency strings can be found below:

Date Offset                 Frequency String            Description
DateOffset                  None                Generic offset class, defaults to absolute 24 hours
BDay or BusinessDay         'B'                 business day (weekday)
CDay or CustomBusinessDay   'C'                 custom business day
Week                        'W'                 one week, optionally anchored on a day of the week
WeekOfMonth                 'WOM'               the x-th day of the y-th week of each month
LastWeekOfMonth             'LWOM'              the x-th day of the last week of each month
MonthEnd                    'M'                 calendar month end
MonthBegin                  'MS'                calendar month begin
BMonthEnd or BusinessMonthEnd       'BM'         business month end
BMonthBegin or BusinessMonthBegin   'BMS'        business month begin
CBMonthEnd 'CBM'
or CustomBusinessMonthEnd
custom business month end
CBMonthBegin                        'CBMS' custom business month begin or
CustomBusinessMonthBegin
SemiMonthEnd                        'SM'    15th (or other day_of_month) and calendar month end
SemiMonthBegin'                     'SMS'   15th (or other day_of_month) and calendar month begin
QuarterEnd                           'Q'    calendar quarter end
QuarterBegin                        'QS'   calendar quarter begin
BQuarterEnd                 'BQ             business quarter end
BQuarterBegin               'BQS'           business quarter begin
FY5253Quarter'REQ'
retail (aka 52-53 week) quarter
YearEnd                     'A'             calendar year end
YearBegin                   'AS' or 'BYS'   calendar year begin
BYearEnd                    'BA'            business year end
BYearBegin                  'BAS'           business year begin
FY5253
'RE'
retail (aka 52-53 week) year
Easter                      None            Easter holiday
BusinessHour                'BH'           business hour
CustomBusinessHour          'CBH'           custom business hour
Day                         'D'             one absolute day
Hour                        'H'             one hour
Minute                      'T' or 'min'    one minute
Second                      'S'             one second
Milli                   'L' or 'ms'         one millisecond
Micro                   'U' or 'us'         one microsecond
Nano                    'N'                 one nanosecond

DateOffsets additionally have rollforward() and rollback() methods for moving a date forward or back- ward respectively to a valid offset date relative to the offset. For example, business offsets will roll dates that land on the weekends (Saturday and Sunday) forward to Monday since business offsets operate on the weekdays.

ts = pd.Timestamp("2018-01-06 00:00:00")
ts.day_name()

ts = pd.Timestamp("2021-05-08 00:00:00")

# BusinessHour's valid offset dates are Monday through Friday
offset = pd.offsets.BusinessHour(start="09:00")

# Bring the date to the closest offset date (Monday
offset.rollforward(ts) # Timestamp('2018-01-08 09:00:00')

# Date is brought to the closest offset date first and then the hour is added
ts + offset # Timestamp('2021-05-10 10:00:00')

These operations preserve time (hour, minute, etc) information by default. To reset time to midnight, use normalize() before or after applying the operation (depending on whether you want the time information included in the operation).
ts = pd.Timestamp("2014-01-01 09:00")
day = pd.offsets.Day()
day.apply(ts) # Timestamp('2014-01-02 09:00:00')
day.apply(ts).normalize() # Timestamp('2014-01-02 00:00:00')

ts = pd.Timestamp("2014-01-01 22:00")
hour = pd.offsets.Hour()
(ts + hour).normalize() # Timestamp('2014-01-01 00:00:00')

hour.apply(pd.Timestamp("2014-01-01 23:30")).normalize() # Timestamp('2014-01-02 00:00:00')

Parametric offsets
-------------------
Some of the offsets can be “parameterized” when created to result in different behaviors. For example, the Week offset for generating weekly data accepts a weekday parameter which results in the generated dates always lying on a particular day of the week:

d = datetime.datetime(2021, 5, 4, 9, 0)
d = pd.Timestamp('2021.05.04 09:00')
d + pd.offsets.Week(weekday=4) # Timestamp('2021-05-07 09:00:00')

d = datetime.datetime(2021, 5, 4, 9, 0)
d = pd.Timestamp('2021.05.04 09:00')
d + pd.offsets.Week(weekday=4) # Timestamp('2021-05-07 09:00:00')
(d + pd.offsets.Week(weekday=4)).weekday() # 4
(d + pd.offsets.Week(weekday=4)).day_name() # 'Friday'
(d + pd.offsets.Week(weekday=4)).month_name() # 'May'

The normalize option will be effective for addition and subtraction.
d - pd.offsets.Week(normalize=True) # Timestamp('2021-05-11 00:00:00')
d - pd.offsets.Week(normalize=True) # Timestamp('2021-04-27 00:00:00')

Another example is parameterizing YearEnd with the specific ending month:
d + pd.offsets.YearEnd() # Timestamp('2021-12-31 09:00:00')
d + pd.offsets.YearEnd(normalize=True) # Timestamp('2021-12-31 00:00:00')
d + pd.offsets.YearEnd(month=6) # Timestamp('2021-06-30 09:00:00')

Using offsets with Series / DatetimeIndex
------------------------------------------
Offsets can be used with either a Series or DatetimeIndex to apply the offset to each element.
rng = pd.date_range("2012-01-01", "2012-01-03")
s = pd.Series(rng)
rng + pd.DateOffset(months=2)
s + pd.DateOffset(months=2)
s - pd.DateOffset(months=2)

-----------Difference between DateOffsets and Offsets--------------------------
DateOffsets are relative whereas Offsets are absolute
d = pd.date_range('2021.05.04', periods=2)
d + pd.DateOffset(months=1) ## DatetimeIndex(['2021-06-04', '2021-06-05'], dtype='datetime64[ns]', freq=None)
d + pd.offsets.MonthEnd() ## DatetimeIndex(['2021-05-31', '2021-05-31'], dtype='datetime64[ns]', freq=None)

where can they be used interchangable, when dealing with days
d - pd.offsets.Day(2) ## DatetimeIndex(['2021-05-02', '2021-05-03'], dtype='datetime64[ns]', freq=None)
d - pd.DateOffset(2) ## DatetimeIndex(['2021-05-02', '2021-05-03'], dtype='datetime64[ns]', freq=None)

--------------------------------------------------------------------------------

If the offset class maps directly to a Timedelta (Day, Hour, Minute, Second, Micro, Milli, Nano) it can be used exactly like a Timedelta -
s - pd.offsets.Day(2)
td = s - pd.Series(pd.date_range('2011.12.28', '2011.12.30'))
td + pd.offsets.Minute(15)

Note that some offsets (such as BQuarterEnd) do not have a vectorized implementation. They can still be used but may calculate significantly slower and will show a PerformanceWarning
rng + pd.offsets.BQuarterEnd()

Custom business days
---------------------
The CDay or CustomBusinessDay class provides a parametric BusinessDay class which can be used to create customized business day calendars which account for local holidays and local weekend conventions.
As an interesting example, let’s look at Egypt where a Friday-Saturday weekend is observed.

weekmask_egypt = "Sun Mon Tue Wed Thu"
# They also observe International Workers' Day so let's # add that for a couple of years
holidays=["2018-05-01",datetime.datetime(2019,5,1),
          np.datetime64("2020-05-01"),]
bday_egypt = pd.offsets.CustomBusinessDay(
    holidays = holidays,
    weekmask = weekmask_egypt
    )
dt = datetime.datetime(2021,4,30)
dt + 2 * bday_egypt # Timestamp('2021-05-03 00:00:00')
bday_egypt.apply(dt) # Timestamp('2021-05-02 00:00:00')

Let’s map to the weekday names:
dts = pd.date_range(dt, periods=5, freq=bday_egypt)
pd.Series(dts.weekday, dts).map(pd.Series("Mon Tue Wed Thu Fri Sat Sun".split()))

from pandas.tseries.holiday import USFederalHolidayCalendar
In [195]: from pandas.tseries.holiday import USFederalHolidayCalendar
In [196]: bday_us = pd.offsets.CustomBusinessDay(calendar=USFederalHolidayCalendar())
# Friday before MLK Day
In [197]: dt = datetime.datetime(2014, 1, 17)
# Tuesday after MLK Day (Monday is skipped because it's a holiday)
In [198]: dt + bday_us
Out[198]: Timestamp('2014-01-21 00:00:00')

Monthly offsets that respect a certain holiday calendar can be defined in the usual way.
In [199]: bmth_us = pd.offsets. ˓→CustomBusinessMonthBegin(calendar=USFederalHolidayCalendar())
# Skip new years
In [200]: dt = datetime.datetime(2013, 12, 17) In [201]: dt + bmth_us
Out[201]: Timestamp('2014-01-02 00:00:00')
# Define date index with custom offset
In [202]: pd.date_range(start="20100101", end="20120101", freq=bmth_us)

Note: The frequency string ‘C’ is used to indicate that a CustomBusinessDay DateOffset is used, it is important to note that since CustomBusinessDay is a parameterised type, instances of CustomBusinessDay may differ and this is not detectable from the ‘C’ frequency string. The user therefore needs to ensure that the ‘C’ frequency string is used consistently within the user’s application.

Business hour
The BusinessHour class provides a business hour representation on BusinessDay, allowing to use specific start and end times.
By default, BusinessHour uses 9:00 - 17:00 as business hours. Adding BusinessHour will increment Timestamp by hourly frequency. If target Timestamp is out of business hours, move to the next business hour then increment it. If the result exceeds the business hours end, the remaining hours are added to the next business day.

bh = pd.offsets.BusinessHour()
# 2014-08-01 is Friday
pd.Timestamp("2014-08-01 10:00").weekday() # 4
pd.Timestamp("2014-08-01 10:00") + bh # Timestamp('2014-08-01 11:00:00')

# Below example is the same as: pd.Timestamp('2014-08-01 09:00') + bh
pd.Timestamp("2014-08-01 08:00") + bh # Timestamp('2014-08-01 10:00:00')

# If the results is on the end time, move to the next business day
pd.Timestamp("2014-08-01 16:00") + bh # Timestamp('2014-08-04 09:00:00')

## Remainings are added to the next day
pd.Timestamp("2014-08-01 16:30") + bh # Timestamp('2014-08-04 09:30:00')

# Adding 2 business hours
pd.Timestamp("2014-08-01 10:00") + pd.offsets.BusinessHour(2)

# Subtracting 3 business hours
pd.Timestamp("2021-05-03 10:00") + pd.offsets.BusinessHour(-3) # Timestamp('2021-04-30 15:00:00')

You can also specify start and end time by keywords. The argument must be a str with an hour:minute representation or a datetime.time instance. Specifying seconds, microseconds and nanoseconds as business hour results in ValueError.
bh = pd.offsets.BusinessHour(start="11:00", end=datetime.time(20, 0))
pd.Timestamp("2014-08-01 13:00") + bh
pd.Timestamp("2014-08-01 09:00") + bh
pd.Timestamp("2014-08-01 18:00") + bh

Passing start time later than end represents midnight business hour. In this case, business hour exceeds midnight and overlap to the next day. Valid business hours are distinguished by whether it started from valid BusinessDay.
bh = pd.offsets.BusinessHour(start="17:00", end="09:00")
pd.Timestamp("2014-08-01 17:00") + bh
pd.Timestamp("2014-08-01 23:00") + bh

# Although 2014-08-02 is Saturday,
# it is valid because it starts from 08-01 (Friday).
pd.Timestamp("2014-08-02 04:00") + bh

# Although 2014-08-04 is Monday,
# it is out of business hours because it starts from 08-03 (Sunday).
pd.Timestamp("2014-08-04 04:00") + bh

Applying BusinessHour.rollforward and rollback to out of business hours results in the next business hour start or previous day’s end. Different from other offsets, BusinessHour.rollforward may output different results from apply by definition.
This is because one day’s business hour end is equal to next day’s business hour start. For example, under the default business hours (9:00 - 17:00), there is no gap (0 minutes) between 2014-08-01 17:00 and 2014-08-04 09: 00.

# This adjusts a Timestamp to business hour edge
pd.offsets.BusinessHour().rollback(pd.Timestamp("2014-08-02 15:00"))
pd.offsets.BusinessHour().rollforward(pd.Timestamp("2014-08-02 15:00"))
# It is the same as BusinessHour().apply(pd.Timestamp('2014-08-01 17:00')).
# And it is the same as BusinessHour().apply(pd.Timestamp('2014-08-04 09:00'))
pd.offsets.BusinessHour().apply(pd.Timestamp("2014-08-02 15:00"))
# BusinessDay results (for reference)
pd.offsets.BusinessHour().rollforward(pd.Timestamp("2014-08-02"))

# It is the same as BusinessDay().apply(pd.Timestamp('2014-08-01'))
# The result is the same as rollworward because BusinessDay never overlap.
pd.offsets.BusinessHour().apply(pd.Timestamp("2014-08-02"))

BusinessHour regards Saturday and Sunday as holidays. To use arbitrary holidays, you can use CustomBusinessHour offset, as explained in the following subsection.

Custom business hour
The CustomBusinessHour is a mixture of BusinessHour and CustomBusinessDay which allows you to specify arbitrary holidays. CustomBusinessHour works as the same as BusinessHour except that it skips specified custom holidays.
from pandas.tseries.holiday import USFederalHolidayCalendar
bhour_us = pd.offsets.CustomBusinessHour(calendar=USFederalHolidayCalendar())

# Friday before MLK Day
dt = datetime.datetime(2014, 1, 17, 15)

dt + bhour_us
Timestamp('2014-01-17 16:00:00')

# Tuesday after MLK Day (Monday is skipped because it's a holiday)
dt + bhour_us * 2

You can use keyword arguments supported by either BusinessHour and CustomBusinessDay.
bhour_mon = pd.offsets.CustomBusinessHour(start="10:00", weekmask="Tue Wed
# Monday is skipped because it's a holiday, business hour starts from 10:00Thu Fri")
dt + bhour_mon * 2

Offset aliases
A number of string aliases are given to useful common time series frequencies. We will refer to these aliases as offset aliases

Alias               Description
B       business day frequency
C       custom business day frequency
D       calendar day frequency
W       weekly frequency
M       month end frequency
SM      semi-month end frequency (15th and end of month)
BM      business month end frequency
CBM     custom business month end frequency
MS      month start frequency
SMS     semi-month start frequency (1st and 15th)
BMS     business month start frequency
CBMS    custom business month start frequency
Q       quarter end frequency
BQ      business quarter end frequency
QS      quarter start frequency
BQS     business quarter start frequency
A, Y    year end frequency
BA, BY  business year end frequency
AS, YS  year start frequency
BAS, BYS        business year start frequency
BH              business hour frequency
H               hourly frequency
T, min          minutely frequency
S               secondly frequency
L, ms           milliseconds
U, us           microseconds
N               nanoseconds

Combining aliases
------------------
As we have seen previously, the alias and the offset instance are fungible in most functions:
pd.date_range(start, periods=6, freq="B")
pd.date_range(start, periods=6, freq=pd.offsets.BDay())

You can combine together day and intraday offsets:
pd.date_range(start, periods=15, freq="2h20min")
pd.date_range(start, periods=10, freq="1D10U")

Anchored offsets
For some frequencies you can specify an anchoring suffix:
Alias   Description
W-SUN   weekly frequency (Sundays). Same as ‘W’
W-MON   weekly frequency (Mondays)
W-TUE   weekly frequency (Tuesdays)
W-WED   weekly frequency (Wednesdays)
W-THU   weekly frequency (Thursdays)
W-FRI   weekly frequency (Fridays)
W-SAT   weekly frequency (Saturdays)
(B)Q(S)- DEC        quarterly frequency, year ends in December. Same as ‘Q’
(B)Q(S)- JAN        quarterly frequency, year ends in January
(B)Q(S)- FEB        quarterly frequency, year ends in February
(B)Q(S)- MAR        quarterly frequency, year ends in March
(B)Q(S)- APR        quarterly frequency, year ends in April
(B)Q(S)- MAY        quarterly frequency, year ends in May
(B)Q(S)- JUN        quarterly frequency, year ends in June
(B)Q(S)- JUL        quarterly frequency, year ends in July
(B)Q(S)- AUG        quarterly frequency, year ends in August
(B)Q(S)- SEP        quarterly frequency, year ends in September
(B)Q(S)- OCT        quarterly frequency, year ends in October
(B)Q(S)- NOV        quarterly frequency, year ends in November
(B)A(S)- DEC        annual frequency, anchored end of December. Same as ‘A’

(B)A(S)- JAN        annual frequency, anchored end of January
(B)A(S)- FEB        annual frequency, anchored end of February
(B)A(S)- MAR        annual frequency, anchored end of March
(B)A(S)- APR        annual frequency, anchored end of April
(B)A(S)- MAY        annual frequency, anchored end of May
(B)A(S)- JUN        annual frequency, anchored end of June
(B)A(S)- JUL        annual frequency, anchored end of July
(B)A(S)- AUG        annual frequency, anchored end of August
(B)A(S)- SEP        annual frequency, anchored end of September
(B)A(S)- OCT        annual frequency, anchored end of October
(B)A(S)- NOV        annual frequency, anchored end of November

These can be used as arguments to date_range, bdate_range, constructors for DatetimeIndex, as well as various other timeseries-related functions in pandas.

Anchored offset semantics
--------------------------
For those offsets that are anchored to the start or end of specific frequency (MonthEnd, MonthBegin, WeekEnd, etc), the following rules apply to rolling forward and backwards.
When n is not 0, if the given date is not on an anchor point, it snapped to the next(previous) anchor point, and moved |n|-1 additional steps forwards or backwards.

pd.Timestamp("2014-1-2") + pd.offsets.MonthBegin(n=1) # Timestamp('2014-02-01 00:00:00')
pd.Timestamp("2014-1-2") + pd.offsets.MonthEnd(n=1) # Timestamp('2014-01-31 00:00:00')
pd.Timestamp("2014-1-2") - pd.offsets.MonthBegin(n=1) # Timestamp('2014-01-01 00:00:00')
pd.Timestamp("2014-1-2") - pd.offsets.MonthEnd(n=1) # Timestamp('2013-12-31 00:00:00')
pd.Timestamp("2014-1-2") + pd.offsets.MonthBegin(n=4) # Timestamp('2014-05-01 00:00:00')
pd.Timestamp("2014-1-2") - pd.offsets.MonthEnd(n=4) # Timestamp('2013-09-30 00:00:00')

If the given date is on an anchor point, it is moved |n| points forwards or backwards.
In [245]: pd.Timestamp("2014-01-01") + pd.offsets.MonthBegin(n=1) Out[245]: Timestamp('2014-02-01 00:00:00')
In [246]: pd.Timestamp("2014-01-31") + pd.offsets.MonthEnd(n=1) Out[246]: Timestamp('2014-02-28 00:00:00')
In [247]: pd.Timestamp("2014-01-01") - pd.offsets.MonthBegin(n=1) Out[247]: Timestamp('2013-12-01 00:00:00')
In [248]: pd.Timestamp("2014-01-31") - pd.offsets.MonthEnd(n=1) Out[248]: Timestamp('2013-12-31 00:00:00')
In [249]: pd.Timestamp("2014-01-01") + pd.offsets.MonthBegin(n=4) Out[249]: Timestamp('2014-05-01 00:00:00')
In [250]: pd.Timestamp("2014-01-31") - pd.offsets.MonthBegin(n=4) Out[250]: Timestamp('2013-10-01 00:00:00')

For the case when n=0, the date is not moved if on an anchor point, otherwise it is rolled forward to the next anchor point.
In [251]: pd.Timestamp("2014-01-02") + pd.offsets.MonthBegin(n=0) Out[251]: Timestamp('2014-02-01 00:00:00')
In [252]: pd.Timestamp("2014-01-02") + pd.offsets.MonthEnd(n=0) Out[252]: Timestamp('2014-01-31 00:00:00')
In [253]: pd.Timestamp("2014-01-01") + pd.offsets.MonthBegin(n=0) Out[253]: Timestamp('2014-01-01 00:00:00')
In [254]: pd.Timestamp("2014-01-31") + pd.offsets.MonthEnd(n=0) Out[254]: Timestamp('2014-01-31 00:00:00')

Holidays / holiday calendars
------------------------------
Holidays and calendars provide a simple way to define holiday rules to be used with CustomBusinessDay or in other analysis that requires a predefined set of holidays. The AbstractHolidayCalendar class provides all the necessary methods to return a list of holidays and only rules need to be defined in a specific holiday calendar class. Furthermore, the start_date and end_date class attributes determine over what date range holidays are generated. These should be overwritten on the AbstractHolidayCalendar class to have the range apply to all calendar subclasses. USFederalHolidayCalendar is the only calendar that exists and primarily serves as an example for developing other calendars.
For holidays that occur on fixed dates (e.g., US Memorial Day or July 4th) an observance rule determines when that holiday is observed if it falls on a weekend or some other non-observed day. Defined observance rules are:
Rule                        Description
nearest_workday             move Saturday to Friday and Sunday to Monday
sun- day_to_monday          move Sunday to following Monday
next_monday_or_tuesday      Saturday to Monday and Sunday/Monday to Tuesday
previous_friday             move Saturday and Sunday to previous Friday”
next_monday                 move Saturday and Sunday to following Monday
An example of how holidays and holiday calendars are defined:

In [255]: from pandas.tseries.holiday import (
.....: Holiday,
.....: USMemorialDay,
.....: AbstractHolidayCalendar,
.....: nearest_workday,
.....: MO, .....: ) .....:
In [256]: class ExampleCalendar(AbstractHolidayCalendar):
              rules = [
                  USMemorialDay,
                  Holiday("July 4th", month=7, day=4, observance=nearest_workday),
                  Holiday(
                      "Columbus Day",
                      month=10,
                      day=1,
                      offset=pd.DateOffset(weekday=MO(2)),
), ]
In [257]: cal = ExampleCalendar()
In [258]: cal.holidays(datetime.datetime(2012, 1, 1), datetime.datetime(2012, 12, 31)) Out[258]: DatetimeIndex(['2012-05-28', '2012-07-04', '2012-10-08'], dtype=
˓→'datetime64[ns]', freq=None)

hint weekday=MO(2) is same as 2 * Week(weekday=2)
Using this calendar, creating an index or doing offset arithmetic skips weekends and holidays (i.e., Memorial Day/July 4th). For example, the below defines a custom business day offset using the ExampleCalendar. Like any other offset, it can be used to create a DatetimeIndex or added to datetime or Timestamp objects.

pd.date_range(
.....: start="7/1/2012", end="7/10/2012", freq=pd.offsets.CDay(calendar=cal) .....: ).to_pydatetime()
.....:

In [260]: offset = pd.offsets.CustomBusinessDay(calendar=cal) In [261]: datetime.datetime(2012, 5, 25) + offset
Out[261]: Timestamp('2012-05-29 00:00:00')
In [262]: datetime.datetime(2012, 7, 3) + offset
Out[262]: Timestamp('2012-07-05 00:00:00')
In [263]: datetime.datetime(2012, 7, 3) + 2 * offset Out[263]: Timestamp('2012-07-06 00:00:00')

Ranges are defined by the start_date and end_date class attributes of AbstractHolidayCalendar. The defaults are shown below.

In [265]: AbstractHolidayCalendar.start_date Out[265]: Timestamp('1970-01-01 00:00:00')
In [266]: AbstractHolidayCalendar.end_date Out[266]: Timestamp('2200-12-31 00:00:00')

These dates can be overwritten by setting the attributes as datetime/Timestamp/string.
In [267]: AbstractHolidayCalendar.start_date = datetime.datetime(2012, 1, 1) In [268]: AbstractHolidayCalendar.end_date = datetime.datetime(2012, 12, 31)
In [269]: cal.holidays()
Out[269]: DatetimeIndex(['2012-05-28', '2012-07-04', '2012-10-08'], dtype=
˓→'datetime64[ns]', freq=None)

Every calendar class is accessible by name using the get_calendar function which returns a holiday class instance. Any imported calendar class will automatically be available by this function. Also, HolidayCalendarFactory provides an easy interface to create calendars that are combinations of calendars or calendars with additional rules.
In [270]: from pandas.tseries.holiday import get_calendar, HolidayCalendarFactory, ˓→USLaborDay
In [271]: cal = get_calendar("ExampleCalendar")
In [272]: cal.rules
Out[272]:
[Holiday: Memorial Day (month=5, day=31, offset=<DateOffset: weekday=MO(-1)>),
Holiday: July 4th (month=7, day=4, observance=<function nearest_workday at ˓→0x7fd285956ca0>),
 Holiday: Columbus Day (month=10, day=1, offset=<DateOffset: weekday=MO(+2)>)]
In [273]: new_cal = HolidayCalendarFactory("NewExampleCalendar", cal, USLaborDay)
In [274]: new_cal.rules
Out[274]:
[Holiday: Labor Day (month=9, day=1, offset=<DateOffset: weekday=MO(+1)>),
 Holiday: Memorial Day (month=5, day=31, offset=<DateOffset: weekday=MO(-1)>),
Holiday: July 4th (month=7, day=4, observance=<function nearest_workday at ˓→0x7fd285956ca0>),
 Holiday: Columbus Day (month=10, day=1, offset=<DateOffset: weekday=MO(+2)>)]

2.19.9 Time series-related instance methods
--------------------------------------------
Shifting / lagging
------------------
One may want to shift or lag the values in a time series back and forward in time. The method for this is shift(), which is available on all of the pandas objects.

rng = pd.date_range("2012-01-01", "2012-01-03")
ts = pd.Series(range(len(rng)), index=rng)
ts[:5]
ts.shift(1)
ts.shift(1).bfill()

The shift method accepts an freq argument which can accept a DateOffset class or other timedelta-like object or also an offset alias.
When freq is specified, shift method changes all the dates in the index rather than changing the alignment of the data and the index:
ts.shift(5, freq='D')
ts.shift(5, freq=pd.offsets.BDay())
ts.shift(5, freq="BM")

Note that  when freq is specified, the leading entry is no longer NaN because the data is not being realigned.
ts.shift(-1).ffill()

(ts.shift(-1)+ts).ffill()
(ts.shift(1, freq='D') + ts).ffill().bfill()

You can also fill the nans with default values
ts.shift(1, fill_value=10)

Frequency conversion
---------------------
df = pd.DataFrame({
    'date':pd.to_datetime('2021.05.01, 2021.05.01, 2021.05.3, 2021.05.4'.split(',')),
    'val':[10,20,12,15]
})

df.groupby('date').val.agg(['min', 'max', 'mean']).asfreq('3D')

The primary function for changing frequencies is the asfreq() method. For a DatetimeIndex, this is basically just a thin, but convenient wrapper around reindex() which generates a date_range and calls reindex.

hint weekday=MO(2) is same as 2 * Week(weekday=2)
dr = pd.date_range(start='2010.1.1', periods=3, freq=3*pd.offsets.BDay())
ts = pd.Series(np.random.randn(3), index=dr)
ts.asfreq(pd.offsets.BDay())
ts.asfreq(pd.DateOffset(1))
ts.asfreq(pd.offsets.BDay()).ffill()

asfreq provides a further convenience so you can specify an interpolation method for any gaps that may appear after the frequency conversion.
ts.asfreq(pd.DateOffset(), method='pad')

Filling forward / backward
Related to asfreq and reindex is fillna(),

Converting to Python datetimes
DatetimeIndex can be converted to an array of Python native datetime.datetime objects using the to_pydatetime method.
rng.to_pydatetime()

2.19.10 Resampling
------------------
pandas has a simple, powerful, and efficient functionality for performing resampling operations during frequency conversion (e.g., converting secondly data into 5-minutely data). This is extremely common in, but not limited to, financial applications.
resample() is a time-based groupby, followed by a reduction method on each of its groups
The resample() method can be used directly from DataFrameGroupBy objects.

Basics
-------
rng = pd.date_range("1/1/2012", periods=400, freq="S")
ts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng)
ts.resample('5min').sum()

The resample function is very flexible and allows you to specify many different parameters to control the frequency conversion and resampling operation.
Any function available via dispatching is available as a method of the returned object, including sum, mean, std, sem, max, min, median, first, last, ohlc:

rng = pd.date_range("1/1/2012", periods=400, freq="S")
ts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng)
ts.resample('5min').sum()
ts.resample('5 Min').mean()
ts.resample('5 min').ohlc() ## Open high low close
ts.resample('5 min').max()

For downsampling, closed can be set to ‘left’ or ‘right’ to specify which end of the interval is closed:
## Default left
rng = pd.date_range("1/1/2012", periods=7, freq="T")
ts = pd.Series(np.random.randint(0, 10, len(rng)), index=rng)
ts.resample('5 min').sum()
ts.resample('5 min', closed='right').sum()

Parameters like label are used to manipulate the resulting labels. label specifies whether the result is labeled with the beginning or the end of the interval.
## Default Left
ts.resample('5 min').sum()
ts.resample('5 min', label='right').sum()

Warning: The default values for label and closed is ‘left’ for all frequency offsets except for‘M’,‘A’,‘Q’, ‘BM’, ‘BA’, ‘BQ’, and ‘W’ which all have a default of ‘right’.
This might unintendedly lead to looking ahead, where the value for a later time is pulled back to a previous time as in the following example with the BusinessDay frequency:

s = pd.date_range("2021-05-08", "2021-05-12").to_series()
s.iloc[2] = pd.NaT
print(s)
s.dt.day_name()
# default: label='left', closed='left'
s.resample('B').last().dt.day_name()

Notice how the value for Sunday got pulled back to the previous Friday. To get the behavior where the value for Sunday is pushed to Monday, use instead
s.df[]('B', label='right', closed='right').last().dt.day_name()

The axis parameter can be set to 0 or 1 and allows you to resample the specified axis for a DataFrame.
kind can be set to ‘timestamp’ or ‘period’ to convert the resulting index to/from timestamp and time span represen-
tations. By default resample retains the input representation.
convention can be set to ‘start’ or ‘end’ when resampling period data (detail below). It specifies how low frequency
periods are converted to higher frequency periods.

s.resample('B', label='right', closed='right', kind='period', convention='end').last().dt.day_name()

Revision:
s.resample('5min', label='left/right', closed='left/right, kind='period/timestamp', convention='start/end')

Upsampling
===========
For upsampling, you can specify a way to upsample and the limit parameter to interpolate over the gaps that are created:

# from secondly to every 250 milliseconds
rng = pd.date_range("1/1/2012", periods=7, freq="T")
ts = pd.Series(np.random.randint(0, 10, len(rng)), index=rng)
print(ts)
ts[:2].resample('250ms').asfreq()
ts[:2].resample('250L').ffill()
ts[:2].resample('250L').ffill(limit=2)

Sparse resampling
==================
Sparse timeseries are the ones where you have a lot fewer points relative to the amount of time you are looking to resample. Naively upsampling a sparse series can potentially generate lots of intermediate values. When you don’t want to use a method to fill these values, e.g. fill_method is None, then intermediate values will be filled with NaN.
Since resample is a time-based groupby, the following is a method to efficiently resample only the groups that are not all NaN.

rng = pd.date_range('2021.01.01', periods=100, freq='D')+pd.Timedelta('1s')
ts = pd.Series(range(100), index=rng)
ts

If we want to resample to the full range of the series:
ts.resample('3T').sum()

We can instead only resample those groups where we have points as follows:
from functools import partial
from pandas.tseries.frequencies import to_offset

def round(t, freq):
    freq=to_offset(freq)
    return pd.Timestamp((t.value // freq.delta.value) * freq.delta.value)

ts.groupby(partial(round, freq='3T')).sum()

Aggregation
------------
 Similar to the aggregating API, groupby API, and the window API, a Resampler can be selectively resampled. Resampling a DataFrame, the default will be to act on all columns with the same function.

df = pd.DataFrame(
    np.random.randn(1000,3),
    index=pd.date_range('2021.01.01', periods=1000, freq='S'),
    columns=['A','B','C']
)
r = df.resample('3T')
r.mean()

We can select a specific column or columns using standard getitem.
r.A.mean()
r[['A','B']].mean()

You can pass a list or dict of functions to do aggregation with, outputting a DataFrame:
r.A.agg([np.sum, np.mean, np.std])

On a resampled DataFrame, you can pass a list of functions to apply to each column, which produces an aggregated result with a hierarchical index:
r.agg([np.sum, np.mean])

By passing a dict to aggregate you can apply a different aggregation to the columns of a DataFrame:
r.agg({'A':np.sum, 'B':lambda x:np.std(x, ddof=1)})

The function names can also be strings. In order for a string to be valid it must be implemented on the resampled object:
r.agg({'A':'sum', 'B':'mean'})

Furthermore, you can also specify multiple aggregation functions for each column separately.
r.agg({'A':['sum', 'std'], 'B':['mean', 'std']})

If a DataFrame does not have a datetimelike index, but instead you want to resample based on datetimelike column in the frame, it can passed to the on keyword.
df = pd.DataFrame({
    "date": pd.date_range("2015-01-01", freq="W", periods=5),
    "a": np.arange(5)},
    index=pd.MultiIndex.from_arrays([[1, 2, 3, 4, 5], pd.date_range("2015-01-01", freq="W", periods=5)],
        names=["v", "d"],)
)
df.resample('M',on='date').sum()

Similarly, if you instead want to  by a datetimelike level of MultiIndex, its name or location can be passed to the level keyword.
df.resample("M", level="d").sum()

Iterating through groups
-------------------------
With the Resampler object in hand, iterating through the grouped data is very natural and functions similarly to itertools.groupby():
small = pd.Series(range(6),
                 index=pd.to_datetime([
                     "2017-01-01T00:00:00",
                     "2017-01-01T00:30:00",
                     "2017-01-01T00:31:00",
                     "2017-01-01T01:00:00",
                     "2017-01-01T03:00:00",
                     "2017-01-01T03:05:00",
                 ]))
resampled = small.resample('H')
for name, group in resampled:
    print("Group: ", name)
    print("-"*27)
    print(group, end="\n\n")

Use origin or offset to adjust the start of the bins
-----------------------------------------------------
The bins of the grouping are adjusted based on the beginning of the day of the time series starting point. This works well with frequencies that are multiples of a day (like 30D) or that divide a day evenly (like 90s or 1min). This can create inconsistencies with some frequencies that do not meet this criteria. To change this behavior you can specify a fixed Timestamp with the argument origin.

start, end = "2000-10-01 23:30:00", "2000-10-02 00:30:00"
middle = "2000-10-02 00:00:00"
rng = pd.date_range(start, end, freq="7min")
ts = pd.Series(np.arange(len(rng)) * 3, index=rng)
ts

Here we can see that, when using origin with its default value ('start_day'), the result after '2000-10-02 00:00:00' are not identical depending on the start of time series:
ts.resample('17min', origin="start_day").sum()
ts[middle:end].resample('17min', origin="start_day").sum()

Here we can see that, when setting origin to 'epoch',the result after'2000-10-02 00:00:00'are identical depending on the start of time series:
ts.resample("17min", origin="epoch").sum()
ts[middle:end].resample("17min", origin="epoch").sum()

If needed you can use a custom timestamp for origin:
ts.resample("17min", origin="2001-01-01").sum()
ts[middle:end].resample("17min", origin=pd.Timestamp("2001-01-01")).sum()

If needed you can just adjust the bins with an offset Timedelta that would be added to the default origin. Those two examples are equivalent for this time series:
ts.resample("17min", origin="start").sum()
ts.resample("17min", offset="23h30min").sum()
Note the use of 'start' for origin on the last example. In that case, origin will be set to the first value of the timeseries.

2.19.11 Time span representation
---------------------------------
Regular intervals of time are represented by Period objects in pandas while sequences of Period objects are collected in a PeriodIndex, which can be created with the convenience function period_range.
Period
-------
A Period represents a span of time (e.g., a day, a month, a quarter, etc). You can specify the span via freq keyword using a frequency alias like below. Because freq represents a span of Period, it cannot be negative like “-3D”.
pd.Period("2012", freq="A-DEC")
pd.Period("2012-1-1", freq="D")
pd.Period("2012-1-1 19:00", freq="H")
pd.Period("2012-1-1 19:00", freq="5H")

Adding and subtracting integers from periods shifts the period by its own frequency. Arithmetic is not allowed between Period with different freq (span).
p = pd.Period("2012", freq="A-DEC")
p + 1
p - 3
p = pd.Period("2012-01", freq="2M")
p + 2
p - 1
p == pd.Period("2012-01", freq="3M")
IncompatibleFrequency: Input has different freq=3M from Period(freq=2M)

If Period freq is daily or higher (D, H, T, S, L, U, N), offsets and timedelta-like can be added if the result can have the same freq. Otherwise, ValueError will be raised.
p = pd.Period("2014-07-01 09:00", freq="H")
p + pd.offsets.Hour(2)
p + datetime.timedelta(minutes=120)
p + pd.Timedelta(minutes=120)
p + np.timedelta64(7200, "s")
p + pd.offsets.Minute(5) # ValueError: Input has different freq from Period(freq=H)
If Period has other frequencies, only the same offsets can be added. Otherwise, ValueError will be raised.
p = pd.Period("2014-07", freq="M")
p + pd.offsets.MonthEnd(3)
p + pd.offsets.MonthBegin(3) # ValueError: Input has different freq from Period(freq=M)

Taking the difference of Period instances with the same frequency will return the number of frequency units between them:
pd.Period("2012", freq="A-DEC") - pd.Period("2002", freq="A-DEC")

PeriodIndex and period_range
----------------------------
Regular sequences of Period objects can be collected in a PeriodIndex, which can be constructed using the period_range convenience function:
prng = pd.period_range("1/1/2011", "1/1/2012", freq="M")
The PeriodIndex constructor can also be used directly:
pd.PeriodIndex(["2011-1", "2011-2", "2011-3"], freq="M")

Passing multiplied frequency outputs a sequence of Period which has multiplied span.
pd.period_range(start="2014-01", freq="3M", periods=4)

If start or end are Period objects, they will be used as anchor endpoints for a PeriodIndex with frequency matching that of the PeriodIndex constructor.
pd.period_range(start=pd.Period('2017Q1', freq='Q'),
                end=pd.Period('2017Q2', freq='Q'),
                freq='M')

Just like DatetimeIndex, a PeriodIndex can also be used to index pandas objects:
ps = pd.Series(np.random.randn(len(prng)), prng)

PeriodIndex supports addition and subtraction with the same rule as Period.
idx = pd.period_range("2014-07-01 09:00", periods=5, freq="H")
idx + pd.offsets.Hour(2)
idx = pd.period_range("2014-07", periods=5, freq="M")
idx + pd.offsets.MonthEnd(3)
PeriodIndex has its own dtype named period,

Period dtypes
--------------
PeriodIndex has a custom period dtype. This is a pandas extension dtype similar to the timezone aware dtype (datetime64[ns, tz]).
pi = pd.period_range("2016-01-01", periods=3, freq="M")
pi.dtype # period[M]

The period dtype can be used in .astype(...). It allows one to change the freq of a PeriodIndex like .asfreq() and convert a DatetimeIndex to PeriodIndex like to_period():
# change monthly freq to daily freq
pi.astype("period[D]")

# convert to DatetimeIndex
pi.astype("datetime64[ns]")

# convert to PeriodIndex
dti = pd.date_range("2011-01-01", freq="M", periods=3)
dti.astype("period[M]")

PeriodIndex partial string indexing
------------------------------------
PeriodIndex now supports partial string slicing with non-monotonic indexes.
ps["2011-01"]
idx = pd.period_range('2010.12.01', periods=90, freq='D')
ps = pd.Series(np.random.randn(len(idx)), idx)
ps["2011-01"]
ps.index = ps.index.astype('Period[M]')
ps["2011-01"]
ps[datetime.datetime(2011, 12, 25):]
ps["10/31/2011":"12/31/2011"]

Passing a string representing a lower frequency than PeriodIndex returns partial sliced data.
ps["2011"]

dfp = pd.DataFrame(
    np.random.randn(600, 1),
    columns=["A"],
    index=pd.period_range("2013-01-01 9:00", periods=600, freq="T"),
)
dfp
dfp.loc["2013-01-01 10H"]
dfp["2013-01-01 10H":"2013-01-01 11H"]

As with DatetimeIndex, the endpoints will be included in the result. The example below slices data starting from 10:00 to 11:59.

Frequency conversion and resampling with PeriodIndex
-----------------------------------------------------
The frequency of Period and PeriodIndex can be converted via the asfreq method. Let’s start with the fiscal year 2011, ending in December:
p = pd.Period("2011", freq="A-DEC")

We can convert it to a monthly frequency. Using the how parameter, we can specify whether to return the starting or ending month:
p.asfreq("M", how="start")
p.asfreq("M", how="end")

The shorthands ‘s’ and ‘e’ are provided for convenience:
p.asfreq("M", "s")
p.asfreq("M", "e")

Converting to a “super-period” (e.g., annual frequency is a super-period of quarterly frequency) automatically returns the super-period that includes the input period:
p = pd.Period("2011-12", freq="M")
p.asfreq("A-NOV")

Note that since we converted to an annual frequency that ends the year in November, the monthly period of December 2011 is actually in the 2012 A-NOV period.
Period conversions with anchored frequencies are particularly useful for working with various quarterly data common to economics, business, and other fields. Many organizations define quarters relative to the month in which their fiscal year starts and ends. Thus, first quarter of 2011 could start in 2010 or a few months into 2011. Via anchored frequencies, pandas works for all quarterly frequencies Q-JAN through Q-DEC.
Q-DEC define regular calendar quarters:

p = pd.Period("2012Q1", freq="Q-DEC")
p.asfreq("D", "s")
p.asfreq("D", "e")

Q-MAR defines fiscal year end in March:
p = pd.Period("2011Q4", freq="Q-MAR")
p.asfreq("D", "s")
p.asfreq("D", "e")

2.19.12 Converting between representations
-------------------------------------------
Timestamped data can be converted to PeriodIndex-ed data using to_period and vice-versa using to_timestamp:
rng = pd.date_range("1/1/2012", periods=5, freq="M")
ts = pd.Series(np.random.randn(len(rng)), index=rng)
ps = ts.to_period()
ps.to_timestamp('D',how='e')
ps.to_timestamp('D',how='e').index.date
ps.to_timestamp("D", how="s")

Converting between period and timestamp enables some convenient arithmetic functions to be used. In the following example, we convert a quarterly frequency with year ending in November to 9am of the end of the month following the quarter end:
prng = pd.period_range("1990Q1", "2000Q4", freq="Q-NOV")
ts = pd.Series(np.random.randn(len(prng)), prng)
ts.index = (prng.asfreq("M", "e") + 1).asfreq("H", "s") + 9

2.19.13 Representing out-of-bounds spans
-----------------------------------------
If you have data that is outside of the Timestamp bounds, see Timestamp limitations, then you can use a PeriodIndex and/or Series of Periods to do computations.
span=pd.period_range("1215.01.01", '1381.01.01', freq='D')

To convert from an int64 based YYYYMMDD representation.
s=pd.Series([20121231, 20141130, 99991231])

def conv(x):
    return pd.Period(year=x//10000, month=x/100%100, day=x%100, freq='D')

s.apply(conv)

These can easily be converted to a PeriodIndex:
span = pd.PeriodIndex(s.apply(conv))

2.19.14 Time zone handling
--------------------------
pandas provides rich support for working with timestamps in different time zones using the pytz and dateutil libraries or datetime.timezone objects from the standard library.

Working with time zones
------------------------
By default, pandas objects are time zone unaware:
rng = pd.date_range('2012.6.3', periods=15, freq='D')
rng.tz is None # True

To localize these dates to a time zone (assign a particular time zone to a naive date), you can use the tz_localize method or the tz keyword argument in date_range(), Timestamp, or DatetimeIndex. You can either pass pytz or dateutil time zone objects or Olson time zone database strings. Olson time zone strings will return pytz time zone objects by default. To return dateutil time zone objects, append dateutil/ before the string.
• In pytz you can find a list of common (and less common) time zones using from pytz import common_timezones, all_timezones.
• dateutil uses the OS time zones so there isn’t a fixed list available. For common zones, the names are the same as pytz.

import dateutil

rng_pytz = pd.date_range('3.6.2021', periods=3,
                         freq='D', tz='Europe/London')
rng_pytz.tz # <DstTzInfo 'Europe/London' LMT-1 day, 23:59:00 STD>

rng_dtutil = pd.date_range('3/6/2012', periods=3, freq='D')
rng_dtutil = rng_dtutil.tz_localize('dateutil/Europe/London')
rng_dtutil.tz # tzfile('/usr/share/zoneinfo/Europe/London')

# dateutil - utc special case
rng_utc = pd.date_range( "3/6/2012 00:00",periods=3,freq="D",
                        tz=dateutil.tz.tzutc(),)
rng_utc.tz # tzutc()

New in version 0.25.0.
# datetime.timezone
rng_utc = pd.date_range( "3/6/2012 00:00",periods=3,freq="D",
                        tz=datetime.timezone.utc,)
rng_utc.tz # datetime.timezone.utc

Note that the UTC time zone is a special case in dateutil and should be constructed explicitly as an instance of dateutil.tz.tzutc. You can also construct other time zones objects explicitly first.
## pytz
import pytz
tz_pytz = pytz.timezone("Europe/London")
rng_pytz = pd.date_range("3/6/2012 00:00", periods=3, freq="D")
rng_pytz = rng_pytz.tz_localize(tz_pytz)
rng_pytz.tz == tz_pytz

## dateutil
tz_dateutil = dateutil.tz.gettz("Europe/London")
rng_dateutil = pd.date_range("3/6/2012 00:00", periods=3, freq="D",
                             tz=tz_dateutil)
rng_dateutil.tz == tz_dateutil

To convert a time zone aware pandas object from one time zone to another, you can use the tz_convert method.
rng_pytz.tz_convert("US/Eastern")

Note: When using pytz time zones, DatetimeIndex will construct a different time zone object than a Timestamp for the same time zone input. A DatetimeIndex can hold a collection of Timestamp objects that may have different UTC offsets and cannot be succinctly represented by one pytz time zone instance while one Timestamp represents one point in time with a specific UTC offset.
dti = pd.date_range("2019-01-01", periods=3, freq="D", tz="US/Pacific")
dti.tz

ts = pd.Timestamp("2019-01-01", tz="US/Pacific")
ts.tz

Warning: Be wary of conversions between libraries. For some timezones,pytz and dateutil have different definitions of the zone. This is more of a problem for unusual time zones than for ‘standard’ zones like US/ Eastern.
Warning: Beawarethatatimezonedefinitionacrossversionsoftimezonelibrariesmaynotbeconsideredequal. This may cause problems when working with stored data that is localized using one version and operated on with a different version.
Warning: For pytz time zones, it is incorrect to pass a time zone object directly into the datetime. datetime constructor (e.g., datetime.datetime(2011, 1, 1, tz=pytz.timezone('US/ Eastern')). Instead, the datetime needs to be localized using the localize method on the pytz time zone object.
Warning: Be aware that for times in the future, correct conversion between time zones (and UTC) cannot be guaranteed by any time zone library because a timezone’s offset from UTC may be changed by the respective government.
Warning: Ifyouareusingdatesbeyond2038-01-18,duetocurrentdeficienciesintheunderlyinglibrariescaused by the year 2038 problem, daylight saving time (DST) adjustments to timezone aware dates will not be applied. If and when the underlying libraries are fixed, the DST transitions will be applied.
For example, for two dates that are in British Summer Time (and so would normally be GMT+1), both the following
asserts evaluate as true:
In [447]: d_2037 = "2037-03-31T010101"
d_2038 = "2038-03-31T010101"
DST = "Europe/London"
assert pd.Timestamp(d_2037, tz=DST) != pd.Timestamp(d_2037, tz="GMT")
assert pd.Timestamp(d_2038, tz=DST) == pd.Timestamp(d_2038, tz="GMT")

Under the hood, all timestamps are stored in UTC. Values from a time zone aware DatetimeIndex or Timestamp will have their fields (day, hour, minute, etc.) localized to the time zone. However, timestamps with the same UTC value are still considered to be equal even if they are in different time zones:
rng_eastern = rng_utc.tz_convert("US/Eastern")
rng_berlin = rng_utc.tz_convert("Europe/Berlin"
rng_eastern[2]
rng_berlin[2]
rng_eastern[2] == rng_berlin[2]

Operations between Series in different time zones will yield UTC Series, aligning the data on the UTC times- tamps:
ts_utc = pd.Series(range(3), pd.date_range("20130101", periods=3, tz="UTC"))
eastern = ts_utc.tz_convert("US/Eastern")
berlin = ts_utc.tz_convert("Europe/Berlin")
result = eastern + berlin

To remove time zone information, use tz_localize(None) or tz_convert(None). tz_localize(None) will remove the time zone yielding the local time representation. tz_convert(None) will remove the time zone after converting to UTC time.
didx = pd.date_range(start="2014-08-01 09:00", freq="H", periods=3, tz="US/ ˓→Eastern")
didx.tz_localize(None)
didx.tz_convert(None)

# tz_convert(None) is identical to tz_convert('UTC').tz_localize(None)
didx.tz_convert("UTC").tz_localize(None)

rng_pytz.tz_localize(None)
rng_pytz.tz_convert(None)
rng_pytz.tz_convert("UTC").tz_localize(None)

Fold
-----
For ambiguous times, pandas supports explicitly specifying the keyword-only fold argument. Due to daylight sav- ing time, one wall clock time can occur twice when shifting from summer to winter time; fold describes whether the datetime-like corresponds to the first (0) or the second time (1) the wall clock hits the ambiguous time. Fold is supported only for constructing from naive datetime.datetime (see datetime documentation for details) or from Timestamp or for constructing from components (see below). Only dateutil timezones are supported (see dateutil documentation for dateutil methods that deal with ambiguous datetimes) as pytz timezones do not support fold (see pytz documentation for details on how pytz deals with ambiguous datetimes). To localize an am- biguous datetime with pytz, please use Timestamp.tz_localize(). In general, we recommend to rely on Timestamp.tz_localize() when localizing ambiguous datetimes if you need direct control over how they are handled.

pd.Timestamp(datetime.datetime(2019, 10, 27, 1, 30, 0, 0),tz="dateutil/Europe/London",fold=0,)
pd.Timestamp(datetime.datetime(2019, 10, 27, 1, 30, 0, 0),tz="dateutil/Europe/London",fold=0,)

Ambiguous times when localizing
pandas: powerful Python data analysis toolkit, Release 1.2.3
 tz_localize may not be able to determine the UTC offset of a timestamp because daylight savings time (DST) in a local time zone causes some times to occur twice within one day (“clocks fall back”). The following options are available:
• 'raise': Raises a pytz.AmbiguousTimeError (the default behavior)
• 'infer': Attempt to determine the correct offset base on the monotonicity of the timestamps
• 'NaT': Replaces ambiguous times with NaT
• bool: True represents a DST time, False represents non-DST time. An array-like of bool values is sup- ported for a sequence of times.
rng_hourly = pd.DatetimeIndex(["11/06/2011 00:00", "11/06/2011 01:00", "11/06/2011 01:00", "11/06/2011 02:00"])
This will fail as there are ambiguous times('11/06/2011 01:00')
rng_hourly.tz_localize('US/Eastern')AmbiguousTimeError: Cannot infer dst time from Timestamp('2011-11-06 01:00:00'), try using the 'ambiguous' argument
Handle these ambiguous times by specifying the following.
rng_hourly.tz_localize("US/Eastern", ambiguous="infer")
rng_hourly.tz_localize("US/Eastern", ambiguous="NaT")
rng_hourly.tz_localize("US/Eastern", ambiguous=[True, True, False, False])

Nonexistent times when localizing
---------------------------------
A DST transition may also shift the local time ahead by 1 hour creating nonexistent local times (“clocks spring forward”). The behavior of localizing a timeseries with nonexistent times can be controlled by the nonexistent argument. The following options are available:
• 'raise': Raises a pytz.NonExistentTimeError (the default behavior)
• 'NaT': Replaces nonexistent times with NaT
• 'shift_forward': Shifts nonexistent times forward to the closest real time
• 'shift_backward': Shifts nonexistent times backward to the closest real time • timedelta object: Shifts nonexistent times by the timedelta duration
dti = pd.date_range(start="2015-03-29 02:30:00", periods=3, freq="H")
Localization of nonexistent times will raise an error by default.
dti.tz_localize('Europe/Warsaw')
Transform nonexistent times to NaT or shift the times.
dti.tz_localize("Europe/Warsaw", nonexistent="shift_forward")
dti.tz_localize("Europe/Warsaw", nonexistent="shift_backward")
dti.tz_localize("Europe/Warsaw", nonexistent=pd.Timedelta(1, unit="H"))
dti.tz_localize("Europe/Warsaw", nonexistent="NaT")

Time zone series operations
---------------------------
A Series with time zone naive values is represented with a dtype of datetime64[ns].
s_naive = pd.Series(pd.date_range("20130101", periods=3))
A Series with a timezone aware values is represented with a dtype of datetime64[ns, tz]where tz is the time zone
s_aware = pd.Series(pd.date_range("20130101", periods=3, tz="US/Eastern"))

Both of these Series time zone information can be manipulated via the .dt accessor
s_naive.dt.tz_localize("UTC").dt.tz_convert("US/Eastern")

Time zone information can also be manipulated using the astype method. This method can localize and convert time zone naive timestamps or convert time zone aware timestamps.
# localize and convert a naive time zone
s_naive.astype("datetime64[ns, US/Eastern]")

# make an aware tz naive
s_aware.astype("datetime64[ns]")

# convert to a new time zone
s_aware.astype("datetime64[ns, CET]")

Note: Using Series.to_numpy() on a Series, returns a NumPy array of the data.NumPy does not currently support time zones (even though it is printing in the local time zone!), therefore an object array of Timestamps is returned for time zone aware data:

s_naive.to_numpy()
s_aware.to_numpy()

By converting to an object array of Timestamps, it preserves the time zone information. For example, when converting back to a Series:
pd.Series(s_aware.to_numpy())

However, if you want an actual NumPy datetime64[ns] array (with the values converted to UTC) instead of an array of objects, you can specify the dtype argument:
s_aware.to_numpy(dtype="datetime64[ns]")

2.20 Time deltas
----------------
Timedeltas are differences in times, expressed in difference units, e.g. days, hours, minutes, seconds. They can be both positive and negative.
Timedelta is a subclass of datetime.timedelta, and behaves in a similar manner, but allows compatibility with np.timedelta64 types as well as a host of custom representation, parsing, and attributes.

2.20.1 Parsing
---------------
You can construct a Timedelta scalar through various arguments, including ISO 8601 Duration strings.

import datetime
# strings
pd.Timedelta("1 day")
pd.Timedelta("1 days")
pd.Timedelta("1 days 00:00:00")
pd.Timedelta("1 days 2 hours")
pd.Timedelta("-1 days 2 min 3us")

# like datetime.timedelta
# note: these MUST be specified as keyword arguments
pd.Timedelta(days=1, seconds=1)

# integers with a unit
pd.Timedelta(1, unit="d")

# from a datetime.timedelta/np.timedelta64
pd.Timedelta(datetime.timedelta(days=1, seconds=1))
pd.Timedelta(np.timedelta64(1, "ms"))

# negative Timedeltas have this string repr
# to be more consistent with datetime.timedelta conventions
pd.Timedelta("-1us")

# a NaT
pd.Timedelta("nan")
pd.Timedelta("nat")

# ISO 8601 Duration strings
pd.Timedelta("P0DT0H1M0S")
pd.Timedelta("P0DT0H0M0.000000123S")

DateOffsets(Day, Hour, Minute, Second, Milli, Micro, Nano)canalsobeusedinconstruction.
pd.Timedelta(pd.offsets.Second(2))

Further, operations among the scalars yield another scalar Timedelta.
pd.Timedelta(pd.offsets.Day(2)) + pd.Timedelta(pd.offsets.Second(2)) + pd.Timedelta("00:00:00.000123")

to_timedelta
-------------
Using the top-level pd.to_timedelta, you can convert a scalar, array, list, or Series from a recognized timedelta format / value into a Timedelta type. It will construct Series if the input is a Series, a scalar if the input is scalar-like, otherwise it will output a TimedeltaIndex.
You can parse a single string to a Timedelta:
pd.to_timedelta("1 days 06:05:01.00003")
pd.to_timedelta("15.5us")

or a list/array of strings:
pd.to_timedelta(["1 days 06:05:01.00003", "15.5us", "nan"])

The unit keyword argument specifies the unit of the Timedelta:
pd.to_timedelta(np.arange(5), unit="s")

Timedelta limitations
----------------------
pandas represents Timedeltas in nanosecond resolution using 64 bit integers. As such, the 64 bit integer limits determine the Timedelta limits
pd.Timedelta.min # Timedelta('-106752 days +00:12:43.145224193')
pd.Timedelta.max # Timedelta('106751 days 23:47:16.854775807')

2.20.2 Operations
------------------
You can operate on Series/DataFrames and construct timedelta64[ns] Series through subtraction operations on datetime64[ns] Series, or Timestamps.
s = pd.Series(pd.date_range("2012-1-1", periods=3, freq="D"))
s = pd.Series(pd.date_range('2021-1-1', periods=3, freq='D'))
td = pd.Series([pd.Timedelta(days=i)for i in range(3)])
df = DataFrame({'A':s, 'B':td})
df["C"] = df["A"] + df["B"]
df.dtypes

s - s.max()
s - datetime.datetime(2011, 1, 1, 3, 5)
s + datetime.timedelta(minutes=5)
s + pd.offsets.Minute(5)
s + pd.offsets.Minute(5) + pd.offsets.Milli(5)

Operations with scalars from a timedelta64[ns] series:
y = s - s[0]

Series of timedeltas with NaT values are supported:
y=s-s.shift()

Elements can be set to NaT using np.nan analogously to datetimes:
y[1] = np.nan

Operands can also appear in a reversed order (a singular object operated with a Series):
s.max() - s
datetime.datetime(2011, 1, 1, 3, 5) - s
datetime.timedelta(minutes=5) + s

min, max and the corresponding idxmin, idxmax operations are supported on frames:
A = s - pd.Timestamp("20210101") - pd.Timedelta("00:05:05")
B = s - pd.Series(pd.date_range("2021-1-2", periods=3, freq="D"))
df = pd.DataFrame({"A": A, "B": B})
print(df)
df.min()
df.max()
df.idxmin()
df.idxmax()
df.min(axis=1)

min, max, idxmin, idxmax operations are supported on Series as well.A scalar result will be a Timedelta.
df.min().max()
df.min(axis=1).min()
df.min().idxmax()
df.min(axis=1).idxmin()

You can fillna on timedeltas, passing a timedelta to get a particular value.
 y.fillna(pd.Timedelta(0))
 y.fillna(pd.Timedelta(10, unit="s"))
 y.fillna(pd.Timedelta("-1 days, 00:00:05"))

You can also negate, multiply and use abs on Timedeltas:
td1 = pd.Timedelta("-1 days 2 hours 3 seconds")
-1 * td1
-td1
abs(td1)

2.20.3 Reductions
------------------
Numeric reduction operation for timedelta64[ns] will return Timedelta objects. As usual NaT are skipped during evaluation.
y2 = pd.Series(pd.to_timedelta(["-1 days +00:00:05", "nat",
                                "-1 days +00:00:05", "1 days"]))
y2.mean()
y2.median()
y2.quantile(.1)
y2.sum()

2.20.4 Frequency conversion
---------------------------
Timedelta Series, TimedeltaIndex, and Timedelta scalars can be converted to other ‘frequencies’ by dividing by another timedelta, or by astyping to a specific timedelta type. These operations yield Series and propagate NaT -> nan. Note that division by the NumPy scalar is true division, while astyping is equivalent of floor division.
december = pd.Series(pd.date_range("20121201", periods=4))
january = pd.Series(pd.date_range("20130101", periods=4))
td = january - december
td[2] += datetime.timedelta(minutes=5, seconds=3)
td[3] = np.nan
td / np.timedelta64(1, "D")
td.astype("timedelta64[D]")

# to seconds
td / np.timedelta64(1, "s")
td.astype("timedelta64[s]")

# to months (these are constant months)
td / np.timedelta64(1, "M")
td.astype("timedelta64[M]")

Dividing or multiplying a timedelta64[ns] Series by an integer or integer Series yields another timedelta64[ns] dtypes Series
td * -1
td * pd.Series([1, 2, 3, 4])

Rounded division (floor-division) of a timedelta64[ns] Series by a scalar Timedelta gives a series of integers.
td // pd.Timedelta(days=3, hours=4)
pd.Timedelta(days=3, hours=4) // td

The mod (%) and divmod operations are defined for Timedelta when operating with another timedelta-like or with a numeric argument.
pd.Timedelta(hours=37) % datetime.timedelta(hours=2)

# divmod against a timedelta-like returns a pair (int, Timedelta)
divmod(datetime.timedelta(hours=2), pd.Timedelta(minutes=11))

# divmod against a numeric returns a pair (Timedelta, Timedelta)
divmod(pd.Timedelta(hours=25), 86400000000000)

2.20.5 Attributes
-----------------
You can access various components of the Timedelta or TimedeltaIndex directly using the attributes days,seconds,microseconds,nanoseconds. These are identical to the values returned by datetime. timedelta, in that, for example, the .seconds attribute represents the number of seconds >= 0 and < 1 day. These are signed according to whether the Timedelta is signed.
These operations can also be directly accessed via the .dt property of the Series as well.
Note: Note that the attributes are NOT the displayed values of the Timedelta.Use .components to retrieve the
displayed values.
For a Series:
td.dt.days
td.dt.seconds

You can access the value of the fields for a scalar Timedelta directly.
tds = pd.Timedelta("31 days 5 min 3 sec")
tds.days
tds.seconds
(-tds).seconds

You can use the .components property to access a reduced form of the timedelta. This returns a DataFrame indexed similarly to the Series. These are the displayed values of the Timedelta.
td.dt.components
td.dt.components.seconds

You can convert a Timedelta to an ISO 8601 Duration string with the .isoformat method
pd.Timedelta(days=6, minutes=50, seconds=3, milliseconds=10, microseconds=10,nanoseconds=12).isoformat()

2.20.6 TimedeltaIndex
----------------------
To generate an index with time delta, you can use either the TimedeltaIndex or the timedelta_range() constructor.
Using TimedeltaIndex you can pass string-like, Timedelta, timedelta, or np.timedelta64 objects. Passing np.nan/pd.NaT/nat will represent missing values.
pd.TimedeltaIndex(["1 days","1 days, 00:00:05",np.timedelta64(2, "D"),datetime.timedelta(days=2, seconds=2),])

The string ‘infer’ can be passed in order to set the frequency of the index as the inferred frequency upon creation:
pd.TimedeltaIndex(["0 days", "10 days", "20 days"], freq="infer")

Generating ranges of time deltas
--------------------------------
Similar to date_range(), you can construct regular ranges of a TimedeltaIndex using timedelta_range(). The default frequency for timedelta_range is calendar day:
pd.timedelta_range(start="1 days", periods=5)

Various combinations of start, end, and periods can be used with timedelta_range:
pd.timedelta_range(start="1 days", end="5 days")
pd.timedelta_range(end="10 days", periods=4)

The freq parameter can passed a variety of frequency aliases:
pd.timedelta_range(start="1 days", end="2 days", freq="30T")
pd.timedelta_range(start="1 days", periods=5, freq="2D5H")

Specifying start, end, and periods will generate a range of evenly spaced timedeltas from start to end inclusively, with periods number of elements in the resulting TimedeltaIndex:
pd.timedelta_range("0 days", "4 days", periods=5)

Using the TimedeltaIndex
-------------------------
Similarly to other of the datetime-like indices, DatetimeIndex and PeriodIndex, you can use TimedeltaIndex as the index of pandas objects.
s = pd.Series(np.arange(100),index=pd.timedelta_range("1 days", periods=100, freq="h"),)
Selections work similarly, with coercion on string-likes and slices:
s["1 day":"2 day"]
s["1 day 01:00:00"]
s[pd.Timedelta("1 day 1h")]
Furthermore you can use partial string selection and the range will be inferre
s["1 day":"1 day 5 hours"]

Operations
-----------
Finally, the combination of TimedeltaIndex with DatetimeIndex allow certain combination operations that are NaT preserving:
tdi = pd.TimedeltaIndex(["1 days", pd.NaT, "2 days"])
tdi.to_list()
dti = pd.date_range("20130101", periods=3)
dti.to_list()
(dti + tdi).to_list()
(dti - tdi).to_list()

Conversions
------------
Similarly to frequency conversion on a Series above, you can convert these indices to yield another Index.
tdi / np.timedelta64(1, "s")
tdi.astype("timedelta64[s]")

Scalars type ops work as well. These can potentially return a different type of index.
# adding or timedelta and date -> datelike
tdi + pd.Timestamp("20130101")

# subtraction of a date and a timedelta -> datelike
# note that trying to subtract a date from a Timedelta will raise an exception
(pd.Timestamp("20130101") - tdi).to_list()

# timedelta + timedelta -> timedelta
tdi + pd.Timedelta("10 days")

# division can result in a Timedelta if the divisor is an integer
tdi / 2

# or a Float64Index if the divisor is a Timedelta
tdi / tdi[0]

2.20.7 Resampling
Similar to timeseries resampling, we can resample with a TimedeltaIndex.
s.resample("D").mean()



--------------------------------------------------------